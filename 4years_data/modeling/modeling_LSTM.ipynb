{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed9734d",
   "metadata": {},
   "source": [
    "# 1. 논문 재현\n",
    "- Stock prediction using combination of BERT sentiment analysis and Macro economy index\n",
    "    - news data : get_data_yahoo API\n",
    "    - historical data : 다우존스, 국제 금값, WTI, INR=X, JPY=X, CNY=X, CAD=X, EURUSD=X의 Adjusted close, close, low, high, open price를 각각 수집\n",
    "    - 기간 : 2018년 1월 1일 부터 2019년 12월 31일\n",
    "    - BERT, NLTK VADER, LSTM 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1b8d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa8e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b2a9c",
   "metadata": {},
   "source": [
    "## news data 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f131889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"C:/Users/default.DESKTOP-IT64657/Desktop/논문code/headline data/\"\n",
    "\n",
    "year_month = [\"2018_1\",\"2018_2\",\"2018_3\",\"2018_4\",\"2018_5\",\"2018_6\",\"2018_7\",\"2018_8\",\"2018_9\",\"2018_10\",\"2018_11\",\"2018_12\",\n",
    "             \"2019_1\",\"2019_2\",\"2019_3\",\"2019_4\",\"2019_5\",\"2019_6\",\"2019_7\",\"2019_8\",\"2019_9\",\"2019_10\",\"2019_11\",\"2019_12\",]\n",
    "\n",
    "j=0\n",
    "m = len(year_month)\n",
    "headline = [0]*m\n",
    "date = [0]*m\n",
    "for i in year_month:\n",
    "    with open(dir+str(i)+\".json\", 'r', encoding= \"UTF-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    n = len(json_data[\"response\"][\"docs\"])\n",
    "    headline[j] = [0]*n\n",
    "    date[j] = [0]*n\n",
    "    \n",
    "    for k in range(n):\n",
    "        headline[j][k] = json_data[\"response\"][\"docs\"][k][\"headline\"][\"main\"]\n",
    "        date[j][k] = json_data[\"response\"][\"docs\"][k][\"pub_date\"]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "db3005ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hesline과 date를 수집\n",
    "headline = np.hstack(headline)\n",
    "date = np.hstack(date)\n",
    "headline = pd.DataFrame({\"headline\":headline, \"date\":date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1b477243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date를 datetime으로 형식 변환\n",
    "headline.date = headline.date.apply(lambda x: pd.to_datetime(x, errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3fa4b",
   "metadata": {},
   "source": [
    "## news data 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "78106c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####-----------------------------------------------------------------------------\n",
    "# 토큰화\n",
    "headline_token = headline.headline.apply(lambda x:word_tokenize(x))\n",
    "\n",
    "\n",
    "\n",
    "#####-----------------------------------------------------------------------------\n",
    "# 구두점 제거\n",
    "#fullstop = re.compile(r'[,—\"“”‘’\\'-?:!;\\\\]')\n",
    "#headline_token = headline_token.apply(lambda x: fullstop.sub(\" \",x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####-----------------------------------------------------------------------------\n",
    "# 표제어 추출(WordNetLemmatizer 사용)\n",
    "def lemmatization_line(line):   ## 한 리스트의 한 문장씩 불러와서 lemmatization_line 적용\n",
    "    line = list(map(lemmatization_word, line))\n",
    "    return line\n",
    "\n",
    "def lemmatization_word(data):  ## string 형식, 한 문장의 한 단어씩 불러와서 표제어 추출\n",
    "    n=WordNetLemmatizer()\n",
    "    data = n.lemmatize(data)\n",
    "    return data\n",
    "\n",
    "headline_token = headline_token.apply(lambda x: lemmatization_line(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####-----------------------------------------------------------------------------\n",
    "# 표제어 추출(Poster 사용)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lemmatization_line(line):   ## 한 리스트의 한 문장씩 불러와서 lemmatization_line 적용\n",
    "    line = list(map(lemmatization_word, line))\n",
    "    return line\n",
    "\n",
    "def lemmatization_word(data):  ## string 형식, 한 문장의 한 단어씩 불러와서 표제어 추출\n",
    "    n=WordNetLemmatizer()\n",
    "    data = ps.stem(data)\n",
    "    return data\n",
    "\n",
    "headline_token = headline_token.apply(lambda x: lemmatization_line(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####-----------------------------------------------------------------------------\n",
    "# 대소문자 변환\n",
    "def upper_to_lower_line(line):  ## 한 리스트의 한 문장씩 불러와서 upper_to_lower_word 적용\n",
    "    line = list(map( upper_to_lower_word, line))\n",
    "    return line\n",
    "\n",
    "def upper_to_lower_word(word):  ## string 형식, 한 문장의 한 단어씩 불러와서 대문자 변환   The -> the, THE -> THE\n",
    "    word = re.sub(\"^[A-Z]{1}[a-z]*$\",word.lower(),word)  ## 정규표현식에 맞는 단어만 lower 적용\n",
    "    return word\n",
    "\n",
    "headline_token = headline_token.apply(lambda x: upper_to_lower_line(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####-----------------------------------------------------------------------------\n",
    "# 불용어 제거\n",
    "def del_stopword(line):              ## 한 리스트의 한 문장씩 불러와서 dir_stopword_produce 적용\n",
    "    dir_stop_words = stopwords.words('english')  ## 불용어 사전\n",
    "    \n",
    "    line_stopwords_intersection = list(set(line)& set(dir_stop_words))   ## 각 문장과 불용어 사전에 동시에 있는 단어 추출\n",
    "    \n",
    "    # 각 문장마다 불용어 사전과 교집합인 사전 생성\n",
    "    \n",
    "    # 각 문장마다 교집합 사전에 해당하지 않는 값만 추출\n",
    "    line = difference(line, line_stopwords_intersection)  \n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def difference(line, line_stopwords_intersection):      ## 각 문장, 각 문장과 불용어 사전의 교집합 입력\n",
    "    line = [i for i in line if i not in line_stopwords_intersection]  ## 불용어 사전에 해당하지 않는 단어만 추출\n",
    "    return line\n",
    "\n",
    "headline_token = headline_token.apply(lambda x: del_stopword(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "77596f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리한 token 하나의 문장으로 결합\n",
    "def word_to_sentence(lst):  ## 토큰화가 되어있는 tit, tit 데이터를 한 문장으로 만듦\n",
    "    lst = \" \".join(lst)     ## ex) the, and, me, bye => the and me bye\n",
    "    return lst\n",
    "\n",
    "headline_sentence= headline_token.apply(lambda x: word_to_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b3f0718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_sentence = pd.DataFrame(headline_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b12e3b",
   "metadata": {},
   "source": [
    "## 감정분석 : NLTK VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cf16f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-121-c8d6b64a7276>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headline_sentence.Score[i] = score[\"compound\"]\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "headline_sentence[\"Score\"]=0\n",
    "\n",
    "for i in range(len(headline_sentence)):\n",
    "    score = analyzer.polarity_scores(headline_sentence.headline[i])       # 감성점수 추출\n",
    "    headline_sentence.Score[i] = score[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7e2f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-122-31f861755797>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headline_sentence.class_3[i] = 0\n",
      "<ipython-input-122-31f861755797>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headline_sentence.class_3[i] = -1\n",
      "<ipython-input-122-31f861755797>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headline_sentence.class_3[i] = 1\n"
     ]
    }
   ],
   "source": [
    "headline_sentence[\"class_3\"] = 0\n",
    "for i in range(len(headline_sentence)):\n",
    "    if (headline_sentence.Score[i] <0.2)&(headline_sentence.Score[i]>-0.2):\n",
    "        headline_sentence.class_3[i] = 0\n",
    "    elif (headline_sentence.Score[i] >=0.2):\n",
    "        headline_sentence.class_3[i] = 1\n",
    "    elif (headline_sentence.Score[i] <=-0.2):\n",
    "        headline_sentence.class_3[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e5fcda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_sentence[\"date\"] = headline.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "13e5cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = pd.date_range(start='20180101', end='20200101')\n",
    "Date = pd.DataFrame({\"Date\" : Date.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "75b2bafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-215-5fa77a271ba6>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headline_sentence.prediction_day[i] = pd.Timestamp(datetime.datetime.strftime(standard_date, \"%Y-%m-%d\"),tz=\"UTC\")\n"
     ]
    }
   ],
   "source": [
    "## 다음날을 예측하도록 구성\n",
    "headline_sentence[\"prediction_day\"] = 0\n",
    "standard_date = pd.Timestamp(datetime.datetime.strftime(headline_sentence.date[0], \"%Y-%m-%d\"),tz=\"UTC\") + datetime.timedelta(hours=16)\n",
    "for i in range(len(headline_sentence)):\n",
    "    if standard_date > headline_sentence.date[i]:\n",
    "        headline_sentence.prediction_day[i] = pd.Timestamp(datetime.datetime.strftime(standard_date, \"%Y-%m-%d\"),tz=\"UTC\")\n",
    "    elif standard_date <= headline_sentence.date[i]:\n",
    "        standard_date = standard_date+ datetime.timedelta(days = 1)\n",
    "        headline_sentence.prediction_day[i] = pd.Timestamp(datetime.datetime.strftime(standard_date, \"%Y-%m-%d\"),tz=\"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c12b49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 날의 news score은 평균으로 계산\n",
    "score_mean = headline_sentence.groupby(\"prediction_day\")[\"class_3\"].mean()\n",
    "score_mean = score_mean.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "231af38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean[\"N\"] = headline_sentence.groupby(\"prediction_day\")[\"class_3\"].count().reset_index()[\"class_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292ccde",
   "metadata": {},
   "source": [
    "## Price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2b87d691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "price_data = yf.download([\"^DJI\"],start = '2017-12-31', end = \"2020-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2be08825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#price_data = price_data[\"Close\"].reset_index()\n",
    "price_data = price_data.reset_index()\n",
    "len(price_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e840fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "first_data = yf.download([\"^DJI\"],start = '2017-12-30', end = '2017-12-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b5488b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(time):\n",
    "    time = pd.Timestamp(time, tz = \"UTC\")\n",
    "    return time\n",
    "\n",
    "price_data.Date = price_data.Date.apply(lambda x: time_conversion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e1672746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-242-9ba825726141>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  score_mean.score[i]= score/n\n"
     ]
    }
   ],
   "source": [
    "standard_time = price_data.Date[0]\n",
    "j = 0\n",
    "score = 0\n",
    "n = 0\n",
    "score_mean[\"Score\"]=0\n",
    "for i in range(len(score_mean)-1):\n",
    "    standard_time = price_data.Date[j]\n",
    "    if score_mean.prediction_day[i]<standard_time:\n",
    "        score += score_mean.class_3[i]*score_mean.N[i]\n",
    "        n +=score_mean.N[i]\n",
    "    elif score_mean.prediction_day[i]==standard_time:\n",
    "        j+=1\n",
    "        score += score_mean.class_3[i]*score_mean.N[i]\n",
    "        n +=score_mean.N[i]\n",
    "        score_mean.Score[i]= score/n\n",
    "        score = 0\n",
    "        n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f0930d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-246-f3d0cc54194b>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_y.before_close[i+1] = x_y.Close[i]\n"
     ]
    }
   ],
   "source": [
    "# 현재 종가를 예측하기 위해 사용할 이전 종가 데이터 만들기\n",
    "x_y = pd.merge(price_data, score_mean, how =\"left\",left_on='Date', right_on = \"prediction_day\")[[\"Close\",\"Open\",\"High\",\"Low\",\"Adj Close\",\"Score\"]]\n",
    "x_y = x_y.dropna(axis=0)\n",
    "x_y[\"before_close\"] = 0\n",
    "x_y = x_y.reset_index(drop=True)\n",
    "for i in range(len(x_y)-1):\n",
    "    x_y.before_close[i+1] = x_y.Close[i]\n",
    "x_y.before_close[0] = first_data[\"Close\"][0]\n",
    "\n",
    "for i in range(len(x_y)):\n",
    "    x_y.High[len(x_y)-i] =x_y.High[len(x_y)-i-1]\n",
    "    x_y.Open[len(x_y)-i] =x_y.Open[len(x_y)-i-1]\n",
    "    x_y.Low[len(x_y)-i] =x_y.Low[len(x_y)-i-1]\n",
    "    x_y[\"Adj Close\"][len(x_y)-i] =x_y[\"Adj Close\"][len(x_y)-i-1]\n",
    "\n",
    "x_y.High[0] = first_data[\"High\"][0]\n",
    "x_y.Open[0] = first_data[\"Open\"][0]\n",
    "x_y.Low[0] = first_data[\"Low\"][0]\n",
    "x_y[\"Adj Close\"][0] = first_data[\"Adj Close\"][0]\n",
    "\n",
    "\n",
    "x_y = pd.DataFrame(MinMaxScaler().fit_transform(x_y))\n",
    "x_y.columns = [\"Close\",\"before_Open\",\"before_High\",\"before_Low\",\"before_Adj Close\",\"Score\",\"before_close\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f680e",
   "metadata": {},
   "source": [
    "## Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9da7e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(len(x_y)*0.7)\n",
    "\n",
    "train = x_y.iloc[0:train_index]\n",
    "test = x_y.iloc[train_index:len(x_y)]\n",
    "\n",
    "\n",
    "train_x = train[[\"before_Open\",\"before_High\",\"before_Low\",\"before_Adj Close\",\"Score\",\"before_close\"]]\n",
    "train_y = train[[\"Close\"]]\n",
    "\n",
    "test_x = test[[\"before_Open\",\"before_High\",\"before_Low\",\"before_Adj Close\",\"Score\",\"before_close\"]]\n",
    "test_y = test[[\"Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7e323991",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.to_numpy().reshape(len(train_x),1,6)\n",
    "train_y = train_y.to_numpy().reshape(len(train_y),1)\n",
    "test_x = test_x.to_numpy().reshape(len(test_x),1,6)\n",
    "test_y = test_y.to_numpy().reshape(len(test_y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8bf19de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "78f71885",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(shape=(len(train_x)-window_size+1,window_size,6))\n",
    "for i in range(len(train_x)-window_size+1):\n",
    "    x[i]=np.vstack((train_x[i:i+window_size]))\n",
    "    \n",
    "y = train_y[window_size-1:len(train_y)]\n",
    "\n",
    "x_t = np.zeros(shape=(len(test_x)-window_size+1,window_size,6))\n",
    "for i in range(len(test_x)-window_size+1):\n",
    "    x_t[i]=np.vstack((test_x[i:i+window_size]))\n",
    "    \n",
    "y_t = test_y[window_size-1:len(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "83042ad4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4298 - mse: 0.1847 - val_loss: 0.3684 - val_mse: 0.1357\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2921 - mse: 0.0853 - val_loss: 0.2092 - val_mse: 0.0438\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1677 - mse: 0.0281 - val_loss: 0.0699 - val_mse: 0.0049\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1260 - mse: 0.0159 - val_loss: 0.0712 - val_mse: 0.0051\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1686 - mse: 0.0284 - val_loss: 0.0791 - val_mse: 0.0063\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.1777 - mse: 0.0316 - val_loss: 0.0626 - val_mse: 0.0039\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.1621 - mse: 0.0263 - val_loss: 0.0556 - val_mse: 0.0031\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1284 - mse: 0.0165 - val_loss: 0.0883 - val_mse: 0.0078\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1120 - mse: 0.0125 - val_loss: 0.1297 - val_mse: 0.0168\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1194 - mse: 0.0143 - val_loss: 0.1586 - val_mse: 0.0252\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1251 - mse: 0.0157 - val_loss: 0.1695 - val_mse: 0.0287\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1270 - mse: 0.0161 - val_loss: 0.1656 - val_mse: 0.0274\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1353 - mse: 0.0183 - val_loss: 0.1496 - val_mse: 0.0224\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1287 - mse: 0.0166 - val_loss: 0.1271 - val_mse: 0.0162\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1089 - mse: 0.0119 - val_loss: 0.1025 - val_mse: 0.0105\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1072 - mse: 0.0115 - val_loss: 0.0806 - val_mse: 0.0065\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1105 - mse: 0.0122 - val_loss: 0.0664 - val_mse: 0.0044\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1163 - mse: 0.0135 - val_loss: 0.0597 - val_mse: 0.0036\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1261 - mse: 0.0159 - val_loss: 0.0585 - val_mse: 0.0034\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1197 - mse: 0.0143 - val_loss: 0.0619 - val_mse: 0.0038\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1143 - mse: 0.0131 - val_loss: 0.0708 - val_mse: 0.0050\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1007 - mse: 0.0101 - val_loss: 0.0831 - val_mse: 0.0069\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1105 - mse: 0.0122 - val_loss: 0.0962 - val_mse: 0.0093\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1029 - mse: 0.0106 - val_loss: 0.1069 - val_mse: 0.0114\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1088 - mse: 0.0118 - val_loss: 0.1131 - val_mse: 0.0128\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1062 - mse: 0.0113 - val_loss: 0.1126 - val_mse: 0.0127\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1063 - mse: 0.0113 - val_loss: 0.1049 - val_mse: 0.0110\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1054 - mse: 0.0111 - val_loss: 0.0924 - val_mse: 0.0085\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1047 - mse: 0.0110 - val_loss: 0.0789 - val_mse: 0.0062\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0966 - mse: 0.0093 - val_loss: 0.0663 - val_mse: 0.0044\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1006 - mse: 0.0101 - val_loss: 0.0587 - val_mse: 0.0034\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1011 - mse: 0.0102 - val_loss: 0.0566 - val_mse: 0.0032\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1054 - mse: 0.0111 - val_loss: 0.0582 - val_mse: 0.0034\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1062 - mse: 0.0113 - val_loss: 0.0645 - val_mse: 0.0042\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1025 - mse: 0.0105 - val_loss: 0.0757 - val_mse: 0.0057\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0972 - mse: 0.0094 - val_loss: 0.0865 - val_mse: 0.0075\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1027 - mse: 0.0106 - val_loss: 0.0943 - val_mse: 0.0089\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1022 - mse: 0.0104 - val_loss: 0.0949 - val_mse: 0.0090\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1028 - mse: 0.0106 - val_loss: 0.0866 - val_mse: 0.0075\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0977 - mse: 0.0096 - val_loss: 0.0724 - val_mse: 0.0052\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0958 - mse: 0.0092 - val_loss: 0.0601 - val_mse: 0.0036\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1009 - mse: 0.0102 - val_loss: 0.0536 - val_mse: 0.0029\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0997 - mse: 0.0099 - val_loss: 0.0521 - val_mse: 0.0027\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0980 - mse: 0.0096 - val_loss: 0.0546 - val_mse: 0.0030\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0987 - mse: 0.0097 - val_loss: 0.0619 - val_mse: 0.0038\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0965 - mse: 0.0093 - val_loss: 0.0712 - val_mse: 0.0051\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0915 - mse: 0.0084 - val_loss: 0.0788 - val_mse: 0.0062\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0957 - mse: 0.0092 - val_loss: 0.0797 - val_mse: 0.0064\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0961 - mse: 0.0092 - val_loss: 0.0725 - val_mse: 0.0052\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0940 - mse: 0.0088 - val_loss: 0.0613 - val_mse: 0.0038\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0960 - mse: 0.0092 - val_loss: 0.0535 - val_mse: 0.0029\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0942 - mse: 0.0089 - val_loss: 0.0508 - val_mse: 0.0026\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0984 - mse: 0.0097 - val_loss: 0.0517 - val_mse: 0.0027\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0910 - mse: 0.0083 - val_loss: 0.0581 - val_mse: 0.0034\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0893 - mse: 0.0080 - val_loss: 0.0685 - val_mse: 0.0047\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0894 - mse: 0.0080 - val_loss: 0.0753 - val_mse: 0.0057\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0976 - mse: 0.0095 - val_loss: 0.0750 - val_mse: 0.0056\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0949 - mse: 0.0090 - val_loss: 0.0666 - val_mse: 0.0044\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0898 - mse: 0.0081 - val_loss: 0.0553 - val_mse: 0.0031\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0890 - mse: 0.0079 - val_loss: 0.0493 - val_mse: 0.0024\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0915 - mse: 0.0084 - val_loss: 0.0488 - val_mse: 0.0024\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0932 - mse: 0.0087 - val_loss: 0.0544 - val_mse: 0.0030\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0883 - mse: 0.0078 - val_loss: 0.0633 - val_mse: 0.0040\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0910 - mse: 0.0083 - val_loss: 0.0715 - val_mse: 0.0051\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0885 - mse: 0.0078 - val_loss: 0.0686 - val_mse: 0.0047\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0898 - mse: 0.0081 - val_loss: 0.0588 - val_mse: 0.0035\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0929 - mse: 0.0086 - val_loss: 0.0516 - val_mse: 0.0027\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0902 - mse: 0.0081 - val_loss: 0.0496 - val_mse: 0.0025\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0908 - mse: 0.0082 - val_loss: 0.0517 - val_mse: 0.0027\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0880 - mse: 0.0077 - val_loss: 0.0569 - val_mse: 0.0032\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0922 - mse: 0.0085 - val_loss: 0.0628 - val_mse: 0.0039\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0911 - mse: 0.0083 - val_loss: 0.0626 - val_mse: 0.0039\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0867 - mse: 0.0075 - val_loss: 0.0583 - val_mse: 0.0034\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0870 - mse: 0.0076 - val_loss: 0.0506 - val_mse: 0.0026\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0835 - mse: 0.0070 - val_loss: 0.0469 - val_mse: 0.0022\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0872 - mse: 0.0076 - val_loss: 0.0471 - val_mse: 0.0022\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0878 - mse: 0.0077 - val_loss: 0.0521 - val_mse: 0.0027\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0882 - mse: 0.0078 - val_loss: 0.0587 - val_mse: 0.0034\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0879 - mse: 0.0077 - val_loss: 0.0620 - val_mse: 0.0038\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0871 - mse: 0.0076 - val_loss: 0.0553 - val_mse: 0.0031\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0891 - mse: 0.0079 - val_loss: 0.0485 - val_mse: 0.0023\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0845 - mse: 0.0071 - val_loss: 0.0457 - val_mse: 0.0021\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0827 - mse: 0.0068 - val_loss: 0.0464 - val_mse: 0.0022\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0878 - mse: 0.0077 - val_loss: 0.0501 - val_mse: 0.0025\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0850 - mse: 0.0072 - val_loss: 0.0557 - val_mse: 0.0031\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0895 - mse: 0.0080 - val_loss: 0.0577 - val_mse: 0.0033\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0867 - mse: 0.0075 - val_loss: 0.0537 - val_mse: 0.0029\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0876 - mse: 0.0077 - val_loss: 0.0486 - val_mse: 0.0024\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0883 - mse: 0.0078 - val_loss: 0.0460 - val_mse: 0.0021\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0842 - mse: 0.0071 - val_loss: 0.0457 - val_mse: 0.0021\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0860 - mse: 0.0074 - val_loss: 0.0479 - val_mse: 0.0023\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0856 - mse: 0.0073 - val_loss: 0.0513 - val_mse: 0.0026\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0872 - mse: 0.0076 - val_loss: 0.0545 - val_mse: 0.0030\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0855 - mse: 0.0073 - val_loss: 0.0559 - val_mse: 0.0031\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0830 - mse: 0.0069 - val_loss: 0.0510 - val_mse: 0.0026\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0827 - mse: 0.0068 - val_loss: 0.0459 - val_mse: 0.0021\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0873 - mse: 0.0076 - val_loss: 0.0430 - val_mse: 0.0018\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0800 - mse: 0.0064 - val_loss: 0.0441 - val_mse: 0.0019\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0892 - mse: 0.0080 - val_loss: 0.0486 - val_mse: 0.0024\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0786 - mse: 0.0062 - val_loss: 0.0523 - val_mse: 0.0027\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0837 - mse: 0.0070 - val_loss: 0.0532 - val_mse: 0.0028\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0844 - mse: 0.0071 - val_loss: 0.0514 - val_mse: 0.0026\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0823 - mse: 0.0068 - val_loss: 0.0458 - val_mse: 0.0021\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0810 - mse: 0.0066 - val_loss: 0.0423 - val_mse: 0.0018\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0881 - mse: 0.0078 - val_loss: 0.0425 - val_mse: 0.0018\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0803 - mse: 0.0064 - val_loss: 0.0447 - val_mse: 0.0020\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0810 - mse: 0.0066 - val_loss: 0.0482 - val_mse: 0.0023\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0806 - mse: 0.0065 - val_loss: 0.0530 - val_mse: 0.0028\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0835 - mse: 0.0070 - val_loss: 0.0519 - val_mse: 0.0027\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0838 - mse: 0.0070 - val_loss: 0.0465 - val_mse: 0.0022\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0818 - mse: 0.0067 - val_loss: 0.0423 - val_mse: 0.0018\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0814 - mse: 0.0066 - val_loss: 0.0434 - val_mse: 0.0019\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0836 - mse: 0.0070 - val_loss: 0.0467 - val_mse: 0.0022\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0833 - mse: 0.0069 - val_loss: 0.0490 - val_mse: 0.0024\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0812 - mse: 0.0066 - val_loss: 0.0487 - val_mse: 0.0024\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0802 - mse: 0.0064 - val_loss: 0.0470 - val_mse: 0.0022\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0835 - mse: 0.0070 - val_loss: 0.0434 - val_mse: 0.0019\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0807 - mse: 0.0065 - val_loss: 0.0417 - val_mse: 0.0017\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0834 - mse: 0.0069 - val_loss: 0.0434 - val_mse: 0.0019\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0808 - mse: 0.0065 - val_loss: 0.0459 - val_mse: 0.0021\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0835 - mse: 0.0070 - val_loss: 0.0504 - val_mse: 0.0025\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0798 - mse: 0.0064 - val_loss: 0.0498 - val_mse: 0.0025\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0811 - mse: 0.0066 - val_loss: 0.0439 - val_mse: 0.0019\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0809 - mse: 0.0065 - val_loss: 0.0410 - val_mse: 0.0017\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0787 - mse: 0.0062 - val_loss: 0.0405 - val_mse: 0.0016\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0800 - mse: 0.0064 - val_loss: 0.0441 - val_mse: 0.0019\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0806 - mse: 0.0065 - val_loss: 0.0476 - val_mse: 0.0023\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0782 - mse: 0.0061 - val_loss: 0.0453 - val_mse: 0.0021\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0829 - mse: 0.0069 - val_loss: 0.0405 - val_mse: 0.0016\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0798 - mse: 0.0064 - val_loss: 0.0404 - val_mse: 0.0016\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0787 - mse: 0.0062 - val_loss: 0.0404 - val_mse: 0.0016\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0767 - mse: 0.0059 - val_loss: 0.0434 - val_mse: 0.0019\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0793 - mse: 0.0063 - val_loss: 0.0462 - val_mse: 0.0021\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0810 - mse: 0.0066 - val_loss: 0.0418 - val_mse: 0.0017\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0783 - mse: 0.0061 - val_loss: 0.0401 - val_mse: 0.0016\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0820 - mse: 0.0067 - val_loss: 0.0400 - val_mse: 0.0016\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0810 - mse: 0.0066 - val_loss: 0.0403 - val_mse: 0.0016\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0795 - mse: 0.0063 - val_loss: 0.0431 - val_mse: 0.0019\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0787 - mse: 0.0062 - val_loss: 0.0457 - val_mse: 0.0021\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0772 - mse: 0.0060 - val_loss: 0.0434 - val_mse: 0.0019\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0770 - mse: 0.0059 - val_loss: 0.0411 - val_mse: 0.0017\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0775 - mse: 0.0060 - val_loss: 0.0413 - val_mse: 0.0017\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0772 - mse: 0.0060 - val_loss: 0.0408 - val_mse: 0.0017\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0778 - mse: 0.0061 - val_loss: 0.0416 - val_mse: 0.0017\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0752 - mse: 0.0057 - val_loss: 0.0401 - val_mse: 0.0016\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0780 - mse: 0.0061 - val_loss: 0.0411 - val_mse: 0.0017\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0798 - mse: 0.0064 - val_loss: 0.0429 - val_mse: 0.0018\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0771 - mse: 0.0059 - val_loss: 0.0417 - val_mse: 0.0017\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0793 - mse: 0.0063 - val_loss: 0.0394 - val_mse: 0.0016\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0816 - mse: 0.0067 - val_loss: 0.0405 - val_mse: 0.0016\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0783 - mse: 0.0061 - val_loss: 0.0395 - val_mse: 0.0016\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0815 - mse: 0.0066 - val_loss: 0.0409 - val_mse: 0.0017\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0762 - mse: 0.0058 - val_loss: 0.0439 - val_mse: 0.0019\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0748 - mse: 0.0056 - val_loss: 0.0398 - val_mse: 0.0016\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0749 - mse: 0.0056 - val_loss: 0.0398 - val_mse: 0.0016\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0746 - mse: 0.0056 - val_loss: 0.0393 - val_mse: 0.0015\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0719 - mse: 0.0052 - val_loss: 0.0404 - val_mse: 0.0016\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0770 - mse: 0.0059 - val_loss: 0.0472 - val_mse: 0.0022\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0749 - mse: 0.0056 - val_loss: 0.0409 - val_mse: 0.0017\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0803 - mse: 0.0065 - val_loss: 0.0388 - val_mse: 0.0015\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0790 - mse: 0.0062 - val_loss: 0.0393 - val_mse: 0.0015\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0782 - mse: 0.0061 - val_loss: 0.0407 - val_mse: 0.0017\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0793 - mse: 0.0063 - val_loss: 0.0499 - val_mse: 0.0025\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0770 - mse: 0.0059 - val_loss: 0.0445 - val_mse: 0.0020\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0749 - mse: 0.0056 - val_loss: 0.0385 - val_mse: 0.0015\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0782 - mse: 0.0061 - val_loss: 0.0393 - val_mse: 0.0015\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0789 - mse: 0.0062 - val_loss: 0.0393 - val_mse: 0.0015\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0765 - mse: 0.0059 - val_loss: 0.0427 - val_mse: 0.0018\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0797 - mse: 0.0064 - val_loss: 0.0397 - val_mse: 0.0016\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0740 - mse: 0.0055 - val_loss: 0.0383 - val_mse: 0.0015\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0721 - mse: 0.0052 - val_loss: 0.0383 - val_mse: 0.0015\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0729 - mse: 0.0053 - val_loss: 0.0385 - val_mse: 0.0015\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0712 - mse: 0.0051 - val_loss: 0.0388 - val_mse: 0.0015\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0757 - mse: 0.0057 - val_loss: 0.0382 - val_mse: 0.0015\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0729 - mse: 0.0053 - val_loss: 0.0382 - val_mse: 0.0015\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0758 - mse: 0.0057 - val_loss: 0.0380 - val_mse: 0.0014\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0729 - mse: 0.0053 - val_loss: 0.0382 - val_mse: 0.0015\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0713 - mse: 0.0051 - val_loss: 0.0379 - val_mse: 0.0014\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0744 - mse: 0.0055 - val_loss: 0.0395 - val_mse: 0.0016\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0715 - mse: 0.0051 - val_loss: 0.0377 - val_mse: 0.0014\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0717 - mse: 0.0051 - val_loss: 0.0420 - val_mse: 0.0018\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0790 - mse: 0.0062 - val_loss: 0.0380 - val_mse: 0.0014\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0748 - mse: 0.0056 - val_loss: 0.0438 - val_mse: 0.0019\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0720 - mse: 0.0052 - val_loss: 0.0402 - val_mse: 0.0016\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0739 - mse: 0.0055 - val_loss: 0.0383 - val_mse: 0.0015\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0710 - mse: 0.0050 - val_loss: 0.0378 - val_mse: 0.0014\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0680 - mse: 0.0046 - val_loss: 0.0401 - val_mse: 0.0016\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0730 - mse: 0.0053 - val_loss: 0.0433 - val_mse: 0.0019\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0711 - mse: 0.0051 - val_loss: 0.0383 - val_mse: 0.0015\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0720 - mse: 0.0052 - val_loss: 0.0381 - val_mse: 0.0015\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0741 - mse: 0.0055 - val_loss: 0.0376 - val_mse: 0.0014\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0780 - mse: 0.0061 - val_loss: 0.0377 - val_mse: 0.0014\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0671 - mse: 0.0045 - val_loss: 0.0376 - val_mse: 0.0014\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0764 - mse: 0.0058 - val_loss: 0.0386 - val_mse: 0.0015\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0717 - mse: 0.0051 - val_loss: 0.0416 - val_mse: 0.0017\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0692 - mse: 0.0048 - val_loss: 0.0383 - val_mse: 0.0015\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0700 - mse: 0.0049 - val_loss: 0.0378 - val_mse: 0.0014\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0710 - mse: 0.0050 - val_loss: 0.0405 - val_mse: 0.0016\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0720 - mse: 0.0052 - val_loss: 0.0431 - val_mse: 0.0019\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0729 - mse: 0.0053 - val_loss: 0.0374 - val_mse: 0.0014\n"
     ]
    }
   ],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape = (x.shape[1],x.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation=\"tanh\"))\n",
    "model.compile(optimizer='adam', loss = root_mean_squared_error, metrics=['mse'])\n",
    "history = model.fit(x, y, epochs=200, batch_size=512, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5b0709cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0750 - mse: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07495591044425964, 0.005888042971491814]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b37f3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a511ef0d60>]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_t)\n",
    "plt.plot(model.predict(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa38a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "model.save(\"lstm_model_1_0038.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4826db22",
   "metadata": {},
   "source": [
    "# 2. 논문과 동일(데이터만 다름)\n",
    "\n",
    "- news data : nbc 데이터 사용\n",
    "- historical data : senment score, 다우존스의 open, close, low, high, adj close 사용\n",
    "- 기간 : 2018년 1월 1일 ~ 2019년 12월 31일 까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d95869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 22:34:31.555297: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 22:34:31.654354: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-13 22:34:31.658244: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-13 22:34:31.658257: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-13 22:34:31.677636: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-13 22:34:32.121720: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 22:34:32.121767: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 22:34:32.121771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c060ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/home/whfhrs3260/csv_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85eeee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv(dir+\"/price_data_score.csv\")\n",
    "score.columns = [\"Date\", \"Score\", \"class_3\", \"class_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be03cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = score[[\"Date\",\"class_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fc8f7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "price_data = yf.download([\"^DJI\"],start = '2017-12-31', end = \"2020-01-01\")\n",
    "price_data = price_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec850350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "first_data = yf.download([\"^DJI\"],start = '2017-12-29', end = '2018-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ab84817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-01-02 00:00:00')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data.Date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d06e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(time):\n",
    "    time = pd.Timestamp(time)\n",
    "    return time\n",
    "\n",
    "score.Date = score.Date.apply(lambda x: time_conversion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44042a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>class_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0.092784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.065217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>0.337662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.309091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.235955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.156863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>-0.102362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>-0.016129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date   class_3\n",
       "249 2018-01-02  0.092784\n",
       "250 2018-01-03  0.028571\n",
       "251 2018-01-04  0.065217\n",
       "252 2018-01-05  0.000000\n",
       "253 2018-01-08  0.337662\n",
       "..         ...       ...\n",
       "746 2019-12-31  0.309091\n",
       "747 2020-01-02  0.235955\n",
       "748 2020-01-03  0.156863\n",
       "749 2020-01-06 -0.102362\n",
       "750 2020-01-07 -0.016129\n",
       "\n",
       "[502 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.iloc[249:751]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b8a816e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>24809.349609</td>\n",
       "      <td>24864.189453</td>\n",
       "      <td>24741.699219</td>\n",
       "      <td>24824.009766</td>\n",
       "      <td>24824.009766</td>\n",
       "      <td>341130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>24850.449219</td>\n",
       "      <td>24941.919922</td>\n",
       "      <td>24825.550781</td>\n",
       "      <td>24922.679688</td>\n",
       "      <td>24922.679688</td>\n",
       "      <td>456790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>24964.859375</td>\n",
       "      <td>25105.960938</td>\n",
       "      <td>24963.269531</td>\n",
       "      <td>25075.130859</td>\n",
       "      <td>25075.130859</td>\n",
       "      <td>403280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>25114.919922</td>\n",
       "      <td>25299.789062</td>\n",
       "      <td>25112.009766</td>\n",
       "      <td>25295.869141</td>\n",
       "      <td>25295.869141</td>\n",
       "      <td>358020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>25308.400391</td>\n",
       "      <td>25311.990234</td>\n",
       "      <td>25235.410156</td>\n",
       "      <td>25283.000000</td>\n",
       "      <td>25283.000000</td>\n",
       "      <td>341390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>28572.570312</td>\n",
       "      <td>28576.800781</td>\n",
       "      <td>28503.210938</td>\n",
       "      <td>28515.449219</td>\n",
       "      <td>28515.449219</td>\n",
       "      <td>86150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>28539.460938</td>\n",
       "      <td>28624.099609</td>\n",
       "      <td>28535.150391</td>\n",
       "      <td>28621.390625</td>\n",
       "      <td>28621.390625</td>\n",
       "      <td>155970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>28675.339844</td>\n",
       "      <td>28701.660156</td>\n",
       "      <td>28608.980469</td>\n",
       "      <td>28645.259766</td>\n",
       "      <td>28645.259766</td>\n",
       "      <td>182280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>28654.759766</td>\n",
       "      <td>28664.689453</td>\n",
       "      <td>28428.980469</td>\n",
       "      <td>28462.140625</td>\n",
       "      <td>28462.140625</td>\n",
       "      <td>181600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>28414.640625</td>\n",
       "      <td>28547.349609</td>\n",
       "      <td>28376.490234</td>\n",
       "      <td>28538.439453</td>\n",
       "      <td>28538.439453</td>\n",
       "      <td>193340000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date          Open          High           Low         Close  \\\n",
       "0   2018-01-02  24809.349609  24864.189453  24741.699219  24824.009766   \n",
       "1   2018-01-03  24850.449219  24941.919922  24825.550781  24922.679688   \n",
       "2   2018-01-04  24964.859375  25105.960938  24963.269531  25075.130859   \n",
       "3   2018-01-05  25114.919922  25299.789062  25112.009766  25295.869141   \n",
       "4   2018-01-08  25308.400391  25311.990234  25235.410156  25283.000000   \n",
       "..         ...           ...           ...           ...           ...   \n",
       "498 2019-12-24  28572.570312  28576.800781  28503.210938  28515.449219   \n",
       "499 2019-12-26  28539.460938  28624.099609  28535.150391  28621.390625   \n",
       "500 2019-12-27  28675.339844  28701.660156  28608.980469  28645.259766   \n",
       "501 2019-12-30  28654.759766  28664.689453  28428.980469  28462.140625   \n",
       "502 2019-12-31  28414.640625  28547.349609  28376.490234  28538.439453   \n",
       "\n",
       "        Adj Close     Volume  \n",
       "0    24824.009766  341130000  \n",
       "1    24922.679688  456790000  \n",
       "2    25075.130859  403280000  \n",
       "3    25295.869141  358020000  \n",
       "4    25283.000000  341390000  \n",
       "..            ...        ...  \n",
       "498  28515.449219   86150000  \n",
       "499  28621.390625  155970000  \n",
       "500  28645.259766  182280000  \n",
       "501  28462.140625  181600000  \n",
       "502  28538.439453  193340000  \n",
       "\n",
       "[503 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f42928d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142583/292413663.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data.before_close[i+1] = price_data.Close[i]\n"
     ]
    }
   ],
   "source": [
    "price_data = pd.merge(price_data, score, how =\"left\",left_on='Date', right_on = \"Date\")[[\"Close\",\"Open\",\"High\",\"Low\",\"Adj Close\",\"class_3\"]]\n",
    "price_data.class_3[price_data.class_3.isnull()]=0\n",
    "price_data[\"before_close\"] = 0\n",
    "price_data = price_data.reset_index(drop=True)\n",
    "for i in range(len(price_data)-1):\n",
    "    price_data.before_close[i+1] = price_data.Close[i]\n",
    "    \n",
    "price_data.before_close[0] = first_data[\"Close\"][0]\n",
    "\n",
    "for i in range(len(price_data)):\n",
    "    price_data.High[len(price_data)-i] =price_data.High[len(price_data)-i-1]\n",
    "    price_data.Open[len(price_data)-i] =price_data.Open[len(price_data)-i-1]\n",
    "    price_data.Low[len(price_data)-i] =price_data.Low[len(price_data)-i-1]\n",
    "    price_data[\"Adj Close\"][len(price_data)-i] =price_data[\"Adj Close\"][len(price_data)-i-1]\n",
    "\n",
    "price_data.High[0] = first_data[\"High\"][0]\n",
    "price_data.Open[0] = first_data[\"Open\"][0]\n",
    "price_data.Low[0] = first_data[\"Low\"][0]\n",
    "price_data[\"Adj Close\"][0] = first_data[\"Adj Close\"][0]\n",
    "\n",
    "\n",
    "price_data = pd.DataFrame(MinMaxScaler().fit_transform(price_data))\n",
    "price_data.columns = [\"Close\",\"before_Open\",\"before_High\",\"before_Low\",\"before_Adj Close\",\"class_3\",\"before_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dff44bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>before_Open</th>\n",
       "      <th>before_High</th>\n",
       "      <th>before_Low</th>\n",
       "      <th>before_Adj Close</th>\n",
       "      <th>class_3</th>\n",
       "      <th>before_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.442402</td>\n",
       "      <td>0.438849</td>\n",
       "      <td>0.397968</td>\n",
       "      <td>0.435977</td>\n",
       "      <td>0.427112</td>\n",
       "      <td>0.482442</td>\n",
       "      <td>0.427112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.432940</td>\n",
       "      <td>0.396794</td>\n",
       "      <td>0.439236</td>\n",
       "      <td>0.442402</td>\n",
       "      <td>0.417329</td>\n",
       "      <td>0.442402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479046</td>\n",
       "      <td>0.438969</td>\n",
       "      <td>0.409012</td>\n",
       "      <td>0.451395</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.454489</td>\n",
       "      <td>0.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511256</td>\n",
       "      <td>0.455751</td>\n",
       "      <td>0.434798</td>\n",
       "      <td>0.471364</td>\n",
       "      <td>0.479046</td>\n",
       "      <td>0.388356</td>\n",
       "      <td>0.479046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.509378</td>\n",
       "      <td>0.477761</td>\n",
       "      <td>0.465265</td>\n",
       "      <td>0.492932</td>\n",
       "      <td>0.511256</td>\n",
       "      <td>0.730760</td>\n",
       "      <td>0.511256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.981058</td>\n",
       "      <td>0.973076</td>\n",
       "      <td>0.981268</td>\n",
       "      <td>0.983006</td>\n",
       "      <td>0.986323</td>\n",
       "      <td>0.476534</td>\n",
       "      <td>0.986323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.996517</td>\n",
       "      <td>0.984926</td>\n",
       "      <td>0.980374</td>\n",
       "      <td>0.984663</td>\n",
       "      <td>0.981058</td>\n",
       "      <td>0.388356</td>\n",
       "      <td>0.981058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980069</td>\n",
       "      <td>0.987808</td>\n",
       "      <td>0.989294</td>\n",
       "      <td>0.996517</td>\n",
       "      <td>0.546096</td>\n",
       "      <td>0.996517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.973279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.984413</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.994189</td>\n",
       "      <td>0.973900</td>\n",
       "      <td>0.973279</td>\n",
       "      <td>0.701787</td>\n",
       "      <td>0.973279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close  before_Open  before_High  before_Low  before_Adj Close  \\\n",
       "0    0.442402     0.438849     0.397968    0.435977          0.427112   \n",
       "1    0.456800     0.432940     0.396794    0.439236          0.442402   \n",
       "2    0.479046     0.438969     0.409012    0.451395          0.456800   \n",
       "3    0.511256     0.455751     0.434798    0.471364          0.479046   \n",
       "4    0.509378     0.477761     0.465265    0.492932          0.511256   \n",
       "..        ...          ...          ...         ...               ...   \n",
       "498  0.981058     0.973076     0.981268    0.983006          0.986323   \n",
       "499  0.996517     0.984926     0.980374    0.984663          0.981058   \n",
       "500  1.000000     0.980069     0.987808    0.989294          0.996517   \n",
       "501  0.973279     1.000000     1.000000    1.000000          1.000000   \n",
       "502  0.984413     0.996981     0.994189    0.973900          0.973279   \n",
       "\n",
       "      class_3  before_close  \n",
       "0    0.482442      0.427112  \n",
       "1    0.417329      0.442402  \n",
       "2    0.454489      0.456800  \n",
       "3    0.388356      0.479046  \n",
       "4    0.730760      0.511256  \n",
       "..        ...           ...  \n",
       "498  0.476534      0.986323  \n",
       "499  0.388356      0.981058  \n",
       "500  0.546096      0.996517  \n",
       "501  0.388356      1.000000  \n",
       "502  0.701787      0.973279  \n",
       "\n",
       "[503 rows x 7 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "154ccc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(len(price_data)*0.7)\n",
    "\n",
    "train = price_data.iloc[0:train_index]\n",
    "test = price_data.iloc[train_index:len(price_data)]\n",
    "\n",
    "\n",
    "train_x = train[[\"before_Open\",\"before_High\",\"before_Low\",\"before_Adj Close\",\"class_3\",\"before_close\"]]\n",
    "train_y = train[[\"Close\"]]\n",
    "\n",
    "test_x = test[[\"before_Open\",\"before_High\",\"before_Low\",\"before_Adj Close\",\"class_3\",\"before_close\"]]\n",
    "test_y = test[[\"Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c8618dc-c279-436b-a85f-41a450dd2389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before_Open</th>\n",
       "      <th>before_High</th>\n",
       "      <th>before_Low</th>\n",
       "      <th>before_Adj Close</th>\n",
       "      <th>class_3</th>\n",
       "      <th>before_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.438849</td>\n",
       "      <td>0.397968</td>\n",
       "      <td>0.435977</td>\n",
       "      <td>0.427112</td>\n",
       "      <td>0.482442</td>\n",
       "      <td>0.427112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.432940</td>\n",
       "      <td>0.396794</td>\n",
       "      <td>0.439236</td>\n",
       "      <td>0.442402</td>\n",
       "      <td>0.417329</td>\n",
       "      <td>0.442402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.438969</td>\n",
       "      <td>0.409012</td>\n",
       "      <td>0.451395</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.454489</td>\n",
       "      <td>0.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.455751</td>\n",
       "      <td>0.434798</td>\n",
       "      <td>0.471364</td>\n",
       "      <td>0.479046</td>\n",
       "      <td>0.388356</td>\n",
       "      <td>0.479046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.477761</td>\n",
       "      <td>0.465265</td>\n",
       "      <td>0.492932</td>\n",
       "      <td>0.511256</td>\n",
       "      <td>0.730760</td>\n",
       "      <td>0.511256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.557025</td>\n",
       "      <td>0.536302</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.567294</td>\n",
       "      <td>0.539706</td>\n",
       "      <td>0.567294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.575658</td>\n",
       "      <td>0.559339</td>\n",
       "      <td>0.589735</td>\n",
       "      <td>0.596103</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>0.596103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.580956</td>\n",
       "      <td>0.556186</td>\n",
       "      <td>0.586183</td>\n",
       "      <td>0.581406</td>\n",
       "      <td>0.720829</td>\n",
       "      <td>0.581406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.557418</td>\n",
       "      <td>0.521570</td>\n",
       "      <td>0.524264</td>\n",
       "      <td>0.539653</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.539653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.541735</td>\n",
       "      <td>0.523585</td>\n",
       "      <td>0.548640</td>\n",
       "      <td>0.553547</td>\n",
       "      <td>0.388356</td>\n",
       "      <td>0.553547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     before_Open  before_High  before_Low  before_Adj Close   class_3  \\\n",
       "0       0.438849     0.397968    0.435977          0.427112  0.482442   \n",
       "1       0.432940     0.396794    0.439236          0.442402  0.417329   \n",
       "2       0.438969     0.409012    0.451395          0.456800  0.454489   \n",
       "3       0.455751     0.434798    0.471364          0.479046  0.388356   \n",
       "4       0.477761     0.465265    0.492932          0.511256  0.730760   \n",
       "..           ...          ...         ...               ...       ...   \n",
       "347     0.557025     0.536302    0.557971          0.567294  0.539706   \n",
       "348     0.575658     0.559339    0.589735          0.596103  0.483000   \n",
       "349     0.580956     0.556186    0.586183          0.581406  0.720829   \n",
       "350     0.557418     0.521570    0.524264          0.539653  0.111800   \n",
       "351     0.541735     0.523585    0.548640          0.553547  0.388356   \n",
       "\n",
       "     before_close  \n",
       "0        0.427112  \n",
       "1        0.442402  \n",
       "2        0.456800  \n",
       "3        0.479046  \n",
       "4        0.511256  \n",
       "..            ...  \n",
       "347      0.567294  \n",
       "348      0.596103  \n",
       "349      0.581406  \n",
       "350      0.539653  \n",
       "351      0.553547  \n",
       "\n",
       "[352 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7022b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_x \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(train_x),\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_y \u001b[38;5;241m=\u001b[39m train_y\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(train_y),\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_x \u001b[38;5;241m=\u001b[39m test_x\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(test_x),\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "train_x = train_x.to_numpy().reshape(len(train_x),1,6)\n",
    "train_y = train_y.to_numpy().reshape(len(train_y),1)\n",
    "test_x = test_x.to_numpy().reshape(len(test_x),1,6)\n",
    "test_y = test_y.to_numpy().reshape(len(test_y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e217849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e509485b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f24a6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(shape=(len(train_x)-window_size+1,window_size,6))\n",
    "for i in range(len(train_x)-window_size+1):\n",
    "    x[i]=np.vstack((train_x[i:i+window_size]))\n",
    "\n",
    "y = train_y[window_size-1:len(train_y)]\n",
    "\n",
    "x_t = np.zeros(shape=(len(test_x)-window_size+1,window_size,6))\n",
    "for i in range(len(test_x)-window_size+1):\n",
    "    x_t[i]=np.vstack((test_x[i:i+window_size]))\n",
    "    \n",
    "y_t = test_y[window_size-1:len(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "125c4eec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.3910 - mse: 0.1529 - val_loss: 0.2519 - val_mse: 0.0635\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1638 - mse: 0.0268 - val_loss: 0.0553 - val_mse: 0.0031\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1391 - mse: 0.0194 - val_loss: 0.0676 - val_mse: 0.0046\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1738 - mse: 0.0302 - val_loss: 0.0558 - val_mse: 0.0031\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1524 - mse: 0.0232 - val_loss: 0.0686 - val_mse: 0.0047\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1149 - mse: 0.0132 - val_loss: 0.1284 - val_mse: 0.0165\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1035 - mse: 0.0107 - val_loss: 0.1768 - val_mse: 0.0313\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1227 - mse: 0.0151 - val_loss: 0.1879 - val_mse: 0.0353\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1293 - mse: 0.0167 - val_loss: 0.1723 - val_mse: 0.0297\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1213 - mse: 0.0147 - val_loss: 0.1422 - val_mse: 0.0202\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1050 - mse: 0.0110 - val_loss: 0.1081 - val_mse: 0.0117\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0982 - mse: 0.0096 - val_loss: 0.0810 - val_mse: 0.0066\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0995 - mse: 0.0099 - val_loss: 0.0662 - val_mse: 0.0044\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1067 - mse: 0.0114 - val_loss: 0.0617 - val_mse: 0.0038\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1109 - mse: 0.0123 - val_loss: 0.0644 - val_mse: 0.0042\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1040 - mse: 0.0108 - val_loss: 0.0737 - val_mse: 0.0054\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0979 - mse: 0.0096 - val_loss: 0.0881 - val_mse: 0.0078\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0954 - mse: 0.0091 - val_loss: 0.1043 - val_mse: 0.0109\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0938 - mse: 0.0088 - val_loss: 0.1167 - val_mse: 0.0136\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0967 - mse: 0.0093 - val_loss: 0.1216 - val_mse: 0.0148\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1006 - mse: 0.0101 - val_loss: 0.1181 - val_mse: 0.0139\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0992 - mse: 0.0098 - val_loss: 0.1075 - val_mse: 0.0116\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0962 - mse: 0.0093 - val_loss: 0.0931 - val_mse: 0.0087\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0926 - mse: 0.0086 - val_loss: 0.0784 - val_mse: 0.0062\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0906 - mse: 0.0082 - val_loss: 0.0679 - val_mse: 0.0046\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0922 - mse: 0.0085 - val_loss: 0.0623 - val_mse: 0.0039\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0925 - mse: 0.0086 - val_loss: 0.0619 - val_mse: 0.0038\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0914 - mse: 0.0084 - val_loss: 0.0658 - val_mse: 0.0043\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0904 - mse: 0.0082 - val_loss: 0.0732 - val_mse: 0.0054\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0883 - mse: 0.0078 - val_loss: 0.0815 - val_mse: 0.0066\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0903 - mse: 0.0081 - val_loss: 0.0869 - val_mse: 0.0075\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0892 - mse: 0.0080 - val_loss: 0.0871 - val_mse: 0.0076\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0897 - mse: 0.0081 - val_loss: 0.0821 - val_mse: 0.0067\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0898 - mse: 0.0081 - val_loss: 0.0740 - val_mse: 0.0055\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0882 - mse: 0.0078 - val_loss: 0.0654 - val_mse: 0.0043\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0888 - mse: 0.0079 - val_loss: 0.0596 - val_mse: 0.0035\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0863 - mse: 0.0075 - val_loss: 0.0572 - val_mse: 0.0033\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0871 - mse: 0.0076 - val_loss: 0.0580 - val_mse: 0.0034\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0858 - mse: 0.0074 - val_loss: 0.0615 - val_mse: 0.0038\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0840 - mse: 0.0071 - val_loss: 0.0663 - val_mse: 0.0044\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0855 - mse: 0.0073 - val_loss: 0.0696 - val_mse: 0.0048\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0857 - mse: 0.0073 - val_loss: 0.0703 - val_mse: 0.0049\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0856 - mse: 0.0073 - val_loss: 0.0662 - val_mse: 0.0044\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0849 - mse: 0.0072 - val_loss: 0.0602 - val_mse: 0.0036\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0841 - mse: 0.0071 - val_loss: 0.0553 - val_mse: 0.0031\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0831 - mse: 0.0069 - val_loss: 0.0536 - val_mse: 0.0029\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0824 - mse: 0.0068 - val_loss: 0.0553 - val_mse: 0.0031\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0829 - mse: 0.0069 - val_loss: 0.0594 - val_mse: 0.0035\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0815 - mse: 0.0066 - val_loss: 0.0634 - val_mse: 0.0040\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0810 - mse: 0.0066 - val_loss: 0.0651 - val_mse: 0.0042\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0806 - mse: 0.0065 - val_loss: 0.0640 - val_mse: 0.0041\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0827 - mse: 0.0068 - val_loss: 0.0600 - val_mse: 0.0036\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0818 - mse: 0.0067 - val_loss: 0.0552 - val_mse: 0.0030\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0806 - mse: 0.0065 - val_loss: 0.0523 - val_mse: 0.0027\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0787 - mse: 0.0062 - val_loss: 0.0533 - val_mse: 0.0028\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0804 - mse: 0.0065 - val_loss: 0.0580 - val_mse: 0.0034\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0789 - mse: 0.0062 - val_loss: 0.0621 - val_mse: 0.0039\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0805 - mse: 0.0065 - val_loss: 0.0621 - val_mse: 0.0039\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0776 - mse: 0.0060 - val_loss: 0.0569 - val_mse: 0.0032\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0781 - mse: 0.0061 - val_loss: 0.0520 - val_mse: 0.0027\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0799 - mse: 0.0064 - val_loss: 0.0510 - val_mse: 0.0026\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0781 - mse: 0.0061 - val_loss: 0.0531 - val_mse: 0.0028\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0771 - mse: 0.0059 - val_loss: 0.0565 - val_mse: 0.0032\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0757 - mse: 0.0057 - val_loss: 0.0592 - val_mse: 0.0035\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0777 - mse: 0.0060 - val_loss: 0.0581 - val_mse: 0.0034\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0755 - mse: 0.0057 - val_loss: 0.0539 - val_mse: 0.0029\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0738 - mse: 0.0054 - val_loss: 0.0508 - val_mse: 0.0026\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0755 - mse: 0.0057 - val_loss: 0.0501 - val_mse: 0.0025\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0732 - mse: 0.0054 - val_loss: 0.0531 - val_mse: 0.0028\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0745 - mse: 0.0056 - val_loss: 0.0544 - val_mse: 0.0030\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0740 - mse: 0.0055 - val_loss: 0.0540 - val_mse: 0.0029\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0764 - mse: 0.0058 - val_loss: 0.0512 - val_mse: 0.0026\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0726 - mse: 0.0053 - val_loss: 0.0472 - val_mse: 0.0022\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0739 - mse: 0.0055 - val_loss: 0.0473 - val_mse: 0.0022\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0738 - mse: 0.0054 - val_loss: 0.0498 - val_mse: 0.0025\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0724 - mse: 0.0052 - val_loss: 0.0512 - val_mse: 0.0026\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0714 - mse: 0.0051 - val_loss: 0.0508 - val_mse: 0.0026\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0733 - mse: 0.0054 - val_loss: 0.0492 - val_mse: 0.0024\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0722 - mse: 0.0052 - val_loss: 0.0482 - val_mse: 0.0023\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0734 - mse: 0.0054 - val_loss: 0.0496 - val_mse: 0.0025\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0723 - mse: 0.0052 - val_loss: 0.0508 - val_mse: 0.0026\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0725 - mse: 0.0053 - val_loss: 0.0517 - val_mse: 0.0027\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0707 - mse: 0.0050 - val_loss: 0.0477 - val_mse: 0.0023\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0723 - mse: 0.0052 - val_loss: 0.0449 - val_mse: 0.0020\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0718 - mse: 0.0052 - val_loss: 0.0460 - val_mse: 0.0021\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0712 - mse: 0.0051 - val_loss: 0.0478 - val_mse: 0.0023\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0705 - mse: 0.0050 - val_loss: 0.0477 - val_mse: 0.0023\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0721 - mse: 0.0052 - val_loss: 0.0470 - val_mse: 0.0022\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0695 - mse: 0.0048 - val_loss: 0.0464 - val_mse: 0.0022\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0687 - mse: 0.0047 - val_loss: 0.0470 - val_mse: 0.0022\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0698 - mse: 0.0049 - val_loss: 0.0491 - val_mse: 0.0024\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0687 - mse: 0.0047 - val_loss: 0.0465 - val_mse: 0.0022\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0673 - mse: 0.0045 - val_loss: 0.0430 - val_mse: 0.0019\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0680 - mse: 0.0046 - val_loss: 0.0453 - val_mse: 0.0020\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0674 - mse: 0.0045 - val_loss: 0.0487 - val_mse: 0.0024\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0681 - mse: 0.0046 - val_loss: 0.0433 - val_mse: 0.0019\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0699 - mse: 0.0049 - val_loss: 0.0445 - val_mse: 0.0020\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0650 - mse: 0.0042 - val_loss: 0.0487 - val_mse: 0.0024\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0694 - mse: 0.0048 - val_loss: 0.0456 - val_mse: 0.0021\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0680 - mse: 0.0046 - val_loss: 0.0436 - val_mse: 0.0019\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0663 - mse: 0.0044 - val_loss: 0.0454 - val_mse: 0.0021\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0661 - mse: 0.0044 - val_loss: 0.0499 - val_mse: 0.0025\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0658 - mse: 0.0043 - val_loss: 0.0433 - val_mse: 0.0019\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0677 - mse: 0.0046 - val_loss: 0.0450 - val_mse: 0.0020\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0667 - mse: 0.0044 - val_loss: 0.0488 - val_mse: 0.0024\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0648 - mse: 0.0042 - val_loss: 0.0430 - val_mse: 0.0018\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0675 - mse: 0.0046 - val_loss: 0.0429 - val_mse: 0.0018\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0660 - mse: 0.0044 - val_loss: 0.0530 - val_mse: 0.0028\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0661 - mse: 0.0044 - val_loss: 0.0446 - val_mse: 0.0020\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0656 - mse: 0.0043 - val_loss: 0.0425 - val_mse: 0.0018\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0693 - mse: 0.0048 - val_loss: 0.0462 - val_mse: 0.0021\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0636 - mse: 0.0040 - val_loss: 0.0514 - val_mse: 0.0026\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0636 - mse: 0.0040 - val_loss: 0.0445 - val_mse: 0.0020\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0664 - mse: 0.0044 - val_loss: 0.0430 - val_mse: 0.0018\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0650 - mse: 0.0042 - val_loss: 0.0570 - val_mse: 0.0032\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0674 - mse: 0.0045 - val_loss: 0.0498 - val_mse: 0.0025\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0655 - mse: 0.0043 - val_loss: 0.0450 - val_mse: 0.0020\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0648 - mse: 0.0042 - val_loss: 0.0442 - val_mse: 0.0020\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0643 - mse: 0.0041 - val_loss: 0.0513 - val_mse: 0.0026\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0632 - mse: 0.0040 - val_loss: 0.0448 - val_mse: 0.0020\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0638 - mse: 0.0041 - val_loss: 0.0461 - val_mse: 0.0021\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0635 - mse: 0.0040 - val_loss: 0.0452 - val_mse: 0.0020\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0621 - mse: 0.0039 - val_loss: 0.0542 - val_mse: 0.0029\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0624 - mse: 0.0039 - val_loss: 0.0469 - val_mse: 0.0022\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0619 - mse: 0.0038 - val_loss: 0.0442 - val_mse: 0.0019\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0664 - mse: 0.0044 - val_loss: 0.0438 - val_mse: 0.0019\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0624 - mse: 0.0039 - val_loss: 0.0535 - val_mse: 0.0029\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0634 - mse: 0.0040 - val_loss: 0.0490 - val_mse: 0.0024\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0621 - mse: 0.0039 - val_loss: 0.0460 - val_mse: 0.0021\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0633 - mse: 0.0040 - val_loss: 0.0456 - val_mse: 0.0021\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0646 - mse: 0.0042 - val_loss: 0.0522 - val_mse: 0.0027\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0606 - mse: 0.0037 - val_loss: 0.0508 - val_mse: 0.0026\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0611 - mse: 0.0037 - val_loss: 0.0445 - val_mse: 0.0020\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0620 - mse: 0.0038 - val_loss: 0.0444 - val_mse: 0.0020\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0612 - mse: 0.0037 - val_loss: 0.0475 - val_mse: 0.0023\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0599 - mse: 0.0036 - val_loss: 0.0504 - val_mse: 0.0025\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0618 - mse: 0.0038 - val_loss: 0.0441 - val_mse: 0.0019\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0614 - mse: 0.0038 - val_loss: 0.0450 - val_mse: 0.0020\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0598 - mse: 0.0036 - val_loss: 0.0487 - val_mse: 0.0024\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0628 - mse: 0.0039 - val_loss: 0.0516 - val_mse: 0.0027\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0585 - mse: 0.0034 - val_loss: 0.0469 - val_mse: 0.0022\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0595 - mse: 0.0035 - val_loss: 0.0467 - val_mse: 0.0022\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0627 - mse: 0.0039 - val_loss: 0.0497 - val_mse: 0.0025\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0606 - mse: 0.0037 - val_loss: 0.0499 - val_mse: 0.0025\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0595 - mse: 0.0035 - val_loss: 0.0433 - val_mse: 0.0019\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0597 - mse: 0.0036 - val_loss: 0.0428 - val_mse: 0.0018\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0602 - mse: 0.0036 - val_loss: 0.0437 - val_mse: 0.0019\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0586 - mse: 0.0034 - val_loss: 0.0440 - val_mse: 0.0019\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0594 - mse: 0.0035 - val_loss: 0.0445 - val_mse: 0.0020\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0563 - mse: 0.0032 - val_loss: 0.0464 - val_mse: 0.0021\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0582 - mse: 0.0034 - val_loss: 0.0539 - val_mse: 0.0029\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0605 - mse: 0.0037 - val_loss: 0.0457 - val_mse: 0.0021\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0588 - mse: 0.0035 - val_loss: 0.0447 - val_mse: 0.0020\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0568 - mse: 0.0032 - val_loss: 0.0491 - val_mse: 0.0024\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0580 - mse: 0.0034 - val_loss: 0.0446 - val_mse: 0.0020\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0581 - mse: 0.0034 - val_loss: 0.0452 - val_mse: 0.0020\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0560 - mse: 0.0031 - val_loss: 0.0487 - val_mse: 0.0024\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0568 - mse: 0.0032 - val_loss: 0.0502 - val_mse: 0.0025\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0563 - mse: 0.0032 - val_loss: 0.0477 - val_mse: 0.0023\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0592 - mse: 0.0035 - val_loss: 0.0459 - val_mse: 0.0021\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0544 - mse: 0.0030 - val_loss: 0.0523 - val_mse: 0.0027\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0596 - mse: 0.0036 - val_loss: 0.0445 - val_mse: 0.0020\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0594 - mse: 0.0035 - val_loss: 0.0465 - val_mse: 0.0022\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0585 - mse: 0.0034 - val_loss: 0.0477 - val_mse: 0.0023\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0558 - mse: 0.0031 - val_loss: 0.0476 - val_mse: 0.0023\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0561 - mse: 0.0032 - val_loss: 0.0461 - val_mse: 0.0021\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0578 - mse: 0.0033 - val_loss: 0.0468 - val_mse: 0.0022\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0571 - mse: 0.0033 - val_loss: 0.0550 - val_mse: 0.0030\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0569 - mse: 0.0032 - val_loss: 0.0454 - val_mse: 0.0021\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0547 - mse: 0.0030 - val_loss: 0.0453 - val_mse: 0.0021\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0577 - mse: 0.0033 - val_loss: 0.0474 - val_mse: 0.0022\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0566 - mse: 0.0032 - val_loss: 0.0477 - val_mse: 0.0023\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0552 - mse: 0.0031 - val_loss: 0.0448 - val_mse: 0.0020\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0540 - mse: 0.0029 - val_loss: 0.0486 - val_mse: 0.0024\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0543 - mse: 0.0030 - val_loss: 0.0475 - val_mse: 0.0023\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0545 - mse: 0.0030 - val_loss: 0.0463 - val_mse: 0.0021\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0563 - mse: 0.0032 - val_loss: 0.0442 - val_mse: 0.0020\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0548 - mse: 0.0030 - val_loss: 0.0466 - val_mse: 0.0022\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0543 - mse: 0.0029 - val_loss: 0.0443 - val_mse: 0.0020\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0531 - mse: 0.0028 - val_loss: 0.0453 - val_mse: 0.0020\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0556 - mse: 0.0031 - val_loss: 0.0477 - val_mse: 0.0023\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0533 - mse: 0.0028 - val_loss: 0.0445 - val_mse: 0.0020\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0533 - mse: 0.0028 - val_loss: 0.0448 - val_mse: 0.0020\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0536 - mse: 0.0029 - val_loss: 0.0456 - val_mse: 0.0021\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0527 - mse: 0.0028 - val_loss: 0.0452 - val_mse: 0.0020\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0543 - mse: 0.0029 - val_loss: 0.0462 - val_mse: 0.0021\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0535 - mse: 0.0029 - val_loss: 0.0441 - val_mse: 0.0019\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0549 - mse: 0.0030 - val_loss: 0.0444 - val_mse: 0.0020\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0521 - mse: 0.0027 - val_loss: 0.0430 - val_mse: 0.0019\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0549 - mse: 0.0030 - val_loss: 0.0459 - val_mse: 0.0021\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0534 - mse: 0.0029 - val_loss: 0.0466 - val_mse: 0.0022\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0497 - mse: 0.0025 - val_loss: 0.0467 - val_mse: 0.0022\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0539 - mse: 0.0029 - val_loss: 0.0430 - val_mse: 0.0018\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0535 - mse: 0.0029 - val_loss: 0.0432 - val_mse: 0.0019\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0551 - mse: 0.0030 - val_loss: 0.0447 - val_mse: 0.0020\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0556 - mse: 0.0031 - val_loss: 0.0461 - val_mse: 0.0021\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0540 - mse: 0.0029 - val_loss: 0.0432 - val_mse: 0.0019\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0545 - mse: 0.0030 - val_loss: 0.0460 - val_mse: 0.0021\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0533 - mse: 0.0028 - val_loss: 0.0456 - val_mse: 0.0021\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0521 - mse: 0.0027 - val_loss: 0.0457 - val_mse: 0.0021\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (x.shape[1],x.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1,activation=\"tanh\"))\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "model.compile(optimizer='adam', loss = root_mean_squared_error, metrics=['mse'])\n",
    "history = model.fit(x, y, epochs=200, batch_size=512, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a06906a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0920 - mse: 0.0091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09197955578565598, 0.009138601832091808]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "346a046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d78bd591c0>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_t)\n",
    "plt.plot(model.predict(x_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f15ff",
   "metadata": {},
   "source": [
    "# 3. 논문과 동일(데이터와 데이터 기간이 다름)\n",
    "\n",
    "- news data : nbc 데이터 사용\n",
    "- historical data : senment score, 다우존스의 open, close, low, high, adj close 사용\n",
    "- 기간 : 2017년 1월 1일 ~ 2022년 4월 31일 까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb516d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"C:/Users/default.DESKTOP-IT64657/Desktop/논문code/csv_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdf00085",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv(dir+\"/price_data_score.csv\")\n",
    "score.columns = [\"Date\", \"Score\", \"class_3\", \"class_5\"]\n",
    "score = score[[\"Date\",\"class_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bae7037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "price_data = yf.download([\"^DJI\"],start = '2017-01-01', end = \"2022-05-01\")\n",
    "price_data = price_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cff3ea1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>19872.859375</td>\n",
       "      <td>19938.529297</td>\n",
       "      <td>19775.929688</td>\n",
       "      <td>19881.759766</td>\n",
       "      <td>19881.759766</td>\n",
       "      <td>339180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>19890.939453</td>\n",
       "      <td>19956.140625</td>\n",
       "      <td>19878.830078</td>\n",
       "      <td>19942.160156</td>\n",
       "      <td>19942.160156</td>\n",
       "      <td>280010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>19924.560547</td>\n",
       "      <td>19948.599609</td>\n",
       "      <td>19811.119141</td>\n",
       "      <td>19899.289062</td>\n",
       "      <td>19899.289062</td>\n",
       "      <td>269920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>19906.960938</td>\n",
       "      <td>19999.630859</td>\n",
       "      <td>19834.080078</td>\n",
       "      <td>19963.800781</td>\n",
       "      <td>19963.800781</td>\n",
       "      <td>277700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>19931.410156</td>\n",
       "      <td>19943.779297</td>\n",
       "      <td>19887.380859</td>\n",
       "      <td>19887.380859</td>\n",
       "      <td>19887.380859</td>\n",
       "      <td>287510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>33731.648438</td>\n",
       "      <td>34106.011719</td>\n",
       "      <td>33323.371094</td>\n",
       "      <td>34049.460938</td>\n",
       "      <td>34049.460938</td>\n",
       "      <td>416900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>33907.488281</td>\n",
       "      <td>33909.511719</td>\n",
       "      <td>33230.949219</td>\n",
       "      <td>33240.179688</td>\n",
       "      <td>33240.179688</td>\n",
       "      <td>400020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>33450.921875</td>\n",
       "      <td>33697.179688</td>\n",
       "      <td>33108.890625</td>\n",
       "      <td>33301.929688</td>\n",
       "      <td>33301.929688</td>\n",
       "      <td>447230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>33425.960938</td>\n",
       "      <td>34054.789062</td>\n",
       "      <td>33248.460938</td>\n",
       "      <td>33916.390625</td>\n",
       "      <td>33916.390625</td>\n",
       "      <td>440380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>33787.011719</td>\n",
       "      <td>33919.589844</td>\n",
       "      <td>32913.148438</td>\n",
       "      <td>32977.210938</td>\n",
       "      <td>32977.210938</td>\n",
       "      <td>501560000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1341 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date          Open          High           Low         Close  \\\n",
       "0    2017-01-03  19872.859375  19938.529297  19775.929688  19881.759766   \n",
       "1    2017-01-04  19890.939453  19956.140625  19878.830078  19942.160156   \n",
       "2    2017-01-05  19924.560547  19948.599609  19811.119141  19899.289062   \n",
       "3    2017-01-06  19906.960938  19999.630859  19834.080078  19963.800781   \n",
       "4    2017-01-09  19931.410156  19943.779297  19887.380859  19887.380859   \n",
       "...         ...           ...           ...           ...           ...   \n",
       "1336 2022-04-25  33731.648438  34106.011719  33323.371094  34049.460938   \n",
       "1337 2022-04-26  33907.488281  33909.511719  33230.949219  33240.179688   \n",
       "1338 2022-04-27  33450.921875  33697.179688  33108.890625  33301.929688   \n",
       "1339 2022-04-28  33425.960938  34054.789062  33248.460938  33916.390625   \n",
       "1340 2022-04-29  33787.011719  33919.589844  32913.148438  32977.210938   \n",
       "\n",
       "         Adj Close     Volume  \n",
       "0     19881.759766  339180000  \n",
       "1     19942.160156  280010000  \n",
       "2     19899.289062  269920000  \n",
       "3     19963.800781  277700000  \n",
       "4     19887.380859  287510000  \n",
       "...            ...        ...  \n",
       "1336  34049.460938  416900000  \n",
       "1337  33240.179688  400020000  \n",
       "1338  33301.929688  447230000  \n",
       "1339  33916.390625  440380000  \n",
       "1340  32977.210938  501560000  \n",
       "\n",
       "[1341 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a27bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "first_data = yf.download([\"^DJI\"],start = '2016-12-31', end = '2017-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab65328a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>19833.169922</td>\n",
       "      <td>19852.550781</td>\n",
       "      <td>19718.669922</td>\n",
       "      <td>19762.599609</td>\n",
       "      <td>19762.599609</td>\n",
       "      <td>271910000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  \\\n",
       "Date                                                                 \n",
       "2016-12-30  19833.169922  19852.550781  19718.669922  19762.599609   \n",
       "\n",
       "               Adj Close     Volume  \n",
       "Date                                 \n",
       "2016-12-30  19762.599609  271910000  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c66167ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(time):\n",
    "    time = pd.Timestamp(time)\n",
    "    return time\n",
    "\n",
    "score.Date = score.Date.apply(lambda x: time_conversion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32db2a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-b1ce5fe81da2>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data.before_close[i+1] = price_data.Close[i]\n"
     ]
    }
   ],
   "source": [
    "price_data = pd.merge(price_data, score, how =\"left\",left_on='Date', right_on = \"Date\")[[\"Close\",\"Open\",\"High\",\"Low\",\"Adj Close\",\"class_3\"]]\n",
    "price_data.class_3[price_data.class_3.isnull()]=0\n",
    "price_data[\"before_close\"] = 0\n",
    "price_data = price_data.reset_index(drop=True)\n",
    "for i in range(len(price_data)-1):\n",
    "    price_data.before_close[i+1] = price_data.Close[i]\n",
    "    \n",
    "price_data.before_close[0] = first_data[\"Close\"][0]\n",
    "\n",
    "for i in range(len(price_data)):\n",
    "    price_data.High[len(price_data)-i] =price_data.High[len(price_data)-i-1]\n",
    "    price_data.Open[len(price_data)-i] =price_data.Open[len(price_data)-i-1]\n",
    "    price_data.Low[len(price_data)-i] =price_data.Low[len(price_data)-i-1]\n",
    "    price_data[\"Adj Close\"][len(price_data)-i] =price_data[\"Adj Close\"][len(price_data)-i-1]\n",
    "\n",
    "price_data.High[0] = first_data[\"High\"][0]\n",
    "price_data.Open[0] = first_data[\"Open\"][0]\n",
    "price_data.Low[0] = first_data[\"Low\"][0]\n",
    "price_data[\"Adj Close\"][0] = first_data[\"Adj Close\"][0]\n",
    "\n",
    "price_data.columns = [\"Close\",\"before_Open\",\"before_High\",\"before_Low\",\"before_Adj Close\",\"class_3\",\"before_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "788a2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = price_data[[\"class_3\",\"before_close\"]]\n",
    "y = price_data[[\"Close\"]]\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "x = pd.DataFrame(scaler_x.fit_transform(x))\n",
    "y = pd.DataFrame(scaler_y.fit_transform(y))\n",
    "\n",
    "x.columns = [\"class_3\",\"before_close\"]\n",
    "y.columns = [\"Close\"]\n",
    "\n",
    "train_index = int(len(x)*0.7)\n",
    "\n",
    "train_x = x.iloc[0:train_index]\n",
    "test_x = x.iloc[train_index:len(x)]\n",
    "\n",
    "train_y = y.iloc[0:train_index]\n",
    "test_y = y.iloc[train_index:len(y)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "464480ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 30, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04620c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46e1b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.to_numpy().reshape(train_x.shape[0],1,train_x.shape[1])\n",
    "train_y = train_y.to_numpy().reshape(train_y.shape[0],train_y.shape[1])\n",
    "test_x = test_x.to_numpy().reshape(test_x.shape[0],1,test_x.shape[1])\n",
    "test_y = test_y.to_numpy().reshape(test_y.shape[0],test_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbe28df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5fdfdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 30, 30)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2e93c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(shape=(train_x.shape[0]-window_size+1,window_size,x.shape[1]))\n",
    "for i in range(train_x.shape[0]-window_size+1):\n",
    "    x[i]=np.vstack((train_x[i:i+window_size]))\n",
    "\n",
    "y = train_y[window_size-1:train_x.shape[0]]\n",
    "\n",
    "x_t = np.zeros(shape=(test_x.shape[0]-window_size+1,window_size,x.shape[2]))\n",
    "for i in range(test_x.shape[0]-window_size+1):\n",
    "    x_t[i]=np.vstack((test_x[i:i+window_size]))\n",
    "    \n",
    "y_t = test_y[window_size-1:test_x.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf64be22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.59868421, 0.44877677],\n",
       "        [0.61764706, 0.45164965],\n",
       "        [0.53515625, 0.47134028],\n",
       "        ...,\n",
       "        [0.60747664, 0.43441301],\n",
       "        [0.46590909, 0.4576697 ],\n",
       "        [0.61111111, 0.48815009]],\n",
       "\n",
       "       [[0.61764706, 0.45164965],\n",
       "        [0.53515625, 0.47134028],\n",
       "        [0.5       , 0.49386367],\n",
       "        ...,\n",
       "        [0.46590909, 0.4576697 ],\n",
       "        [0.61111111, 0.48815009],\n",
       "        [0.80508475, 0.50834103]],\n",
       "\n",
       "       [[0.53515625, 0.47134028],\n",
       "        [0.5       , 0.49386367],\n",
       "        [0.53676471, 0.48664693],\n",
       "        ...,\n",
       "        [0.61111111, 0.48815009],\n",
       "        [0.80508475, 0.50834103],\n",
       "        [0.62      , 0.53813716]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.425     , 0.82121272],\n",
       "        [0.44202899, 0.84970402],\n",
       "        [0.5       , 0.87264265],\n",
       "        ...,\n",
       "        [0.46721311, 0.83588004],\n",
       "        [0.55084746, 0.84895486],\n",
       "        [0.53731343, 0.8045077 ]],\n",
       "\n",
       "       [[0.44202899, 0.84970402],\n",
       "        [0.5       , 0.87264265],\n",
       "        [0.46078431, 0.88770044],\n",
       "        ...,\n",
       "        [0.55084746, 0.84895486],\n",
       "        [0.53731343, 0.8045077 ],\n",
       "        [0.35606061, 0.80789912]],\n",
       "\n",
       "       [[0.5       , 0.87264265],\n",
       "        [0.46078431, 0.88770044],\n",
       "        [0.49038462, 0.87660947],\n",
       "        ...,\n",
       "        [0.53731343, 0.8045077 ],\n",
       "        [0.35606061, 0.80789912],\n",
       "        [0.46825397, 0.8416464 ]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71a50950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-7e91eec31dc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot_mean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                 _r=1):\n\u001b[0;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m       _, _, filtered_flat_args = (\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (x.shape[1],x.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1,activation=\"tanh\"))\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "model.compile(optimizer='adam', loss = root_mean_squared_error, metrics=['mse'])\n",
    "history = model.fit(x, y, epochs=200, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa6fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "17b6077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0144 - mse: 3.2630e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01439722254872322, 0.00032629945781081915]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "9c308379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 13ms/step - loss: 0.0588 - mse: 0.0042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.058824121952056885, 0.004158635623753071]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6a63d",
   "metadata": {},
   "source": [
    "=> 기간이 길어지니 확실히 나음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "8b8f1beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d78c0dbc40>]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scaler_y.inverse_transform(model.predict(x_t)))\n",
    "plt.plot(scaler_y.inverse_transform(y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "5f79050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1378674.053126126"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "mean_squared_error(scaler_y.inverse_transform(model.predict(x_t)),scaler_y.inverse_transform(y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "af105116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('20221027_lstm_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75acd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f08c7",
   "metadata": {},
   "source": [
    "# 4. 논문과 동일(데이터와 데이터 기간와 변수가 다름)\n",
    "\n",
    "- news data : nbc 데이터 사용\n",
    "- historical data : senment score, 다우존스의 close 만 사용\n",
    "- 기간 : 2017년 1월 1일 ~ 2022년 04월 31일 까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54f593ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"C:/Users/default.DESKTOP-IT64657/Desktop/논문code/csv_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fbba9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv(dir+\"/price_data_score.csv\")\n",
    "score.columns = [\"Date\", \"Score\", \"class_3\", \"class_5\"]\n",
    "score = score[[\"Date\",\"class_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f9302a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "price_data = yf.download([\"^DJI\"],start = '2017-01-01', end = \"2022-05-01\")[\"Close\"]\n",
    "price_data = price_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "818441d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>19881.759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>19942.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>19899.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>19963.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>19887.380859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>34049.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>33240.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>33301.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>33916.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>32977.210938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Close\n",
       "0    2017-01-03  19881.759766\n",
       "1    2017-01-04  19942.160156\n",
       "2    2017-01-05  19899.289062\n",
       "3    2017-01-06  19963.800781\n",
       "4    2017-01-09  19887.380859\n",
       "...         ...           ...\n",
       "1336 2022-04-25  34049.460938\n",
       "1337 2022-04-26  33240.179688\n",
       "1338 2022-04-27  33301.929688\n",
       "1339 2022-04-28  33916.390625\n",
       "1340 2022-04-29  32977.210938\n",
       "\n",
       "[1341 rows x 2 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bab4786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "first_data = yf.download([\"^DJI\"],start = '2016-12-31', end = '2017-01-01')[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c7b126d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2016-12-30    19762.599609\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d7871e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(time):\n",
    "    time = pd.Timestamp(time)\n",
    "    return time\n",
    "\n",
    "score.Date = score.Date.apply(lambda x: time_conversion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83c74884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>19881.759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>19942.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>19899.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>19963.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>19887.380859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>34049.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>33240.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>33301.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>33916.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>32977.210938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Close\n",
       "0    2017-01-03  19881.759766\n",
       "1    2017-01-04  19942.160156\n",
       "2    2017-01-05  19899.289062\n",
       "3    2017-01-06  19963.800781\n",
       "4    2017-01-09  19887.380859\n",
       "...         ...           ...\n",
       "1336 2022-04-25  34049.460938\n",
       "1337 2022-04-26  33240.179688\n",
       "1338 2022-04-27  33301.929688\n",
       "1339 2022-04-28  33916.390625\n",
       "1340 2022-04-29  32977.210938\n",
       "\n",
       "[1341 rows x 2 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a9258192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-130-04be1271248a>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data.before_close[i+1] = price_data.Close[i]\n"
     ]
    }
   ],
   "source": [
    "price_data = pd.merge(price_data, score, how =\"left\",left_on='Date', right_on = \"Date\")[[\"Close\",\"class_3\"]]\n",
    "price_data.class_3[price_data.class_3.isnull()]=0\n",
    "price_data[\"before_close\"] = 0\n",
    "price_data = price_data.reset_index(drop=True)\n",
    "for i in range(len(price_data)-1):\n",
    "    price_data.before_close[i+1] = price_data.Close[i]\n",
    "\n",
    "\n",
    "price_data.before_close[0] = first_data[0]\n",
    "\n",
    "\n",
    "price_data = pd.DataFrame(MinMaxScaler().fit_transform(price_data))\n",
    "price_data.columns = [\"Close\",\"class_3\",\"before_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "63633dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(len(price_data)*0.7)\n",
    "\n",
    "train = price_data.iloc[0:train_index]\n",
    "test = price_data.iloc[train_index:len(price_data)]\n",
    "\n",
    "\n",
    "train_x = train[[\"class_3\",\"before_close\"]]\n",
    "train_y = train[[\"Close\"]]\n",
    "\n",
    "test_x = test[[\"class_3\",\"before_close\"]]\n",
    "test_y = test[[\"Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0412762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.to_numpy().reshape(len(train_x),1,2)\n",
    "train_y = train_y.to_numpy().reshape(len(train_y),1)\n",
    "test_x = test_x.to_numpy().reshape(len(test_x),1,2)\n",
    "test_y = test_y.to_numpy().reshape(len(test_y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "babeedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e9739592",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(shape=(len(train_x)-window_size+1,window_size,2))\n",
    "for i in range(len(train_x)-window_size+1):\n",
    "    x[i]=np.vstack((train_x[i:i+window_size]))\n",
    "\n",
    "y = train_y[window_size-1:len(train_y)]\n",
    "\n",
    "x_t = np.zeros(shape=(len(test_x)-window_size+1,window_size,2))\n",
    "for i in range(len(test_x)-window_size+1):\n",
    "    x_t[i]=np.vstack((test_x[i:i+window_size]))\n",
    "    \n",
    "y_t = test_y[window_size-1:len(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8763d65f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.3164 - mse: 0.1082 - val_loss: 0.1275 - val_mse: 0.0163\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.1306 - mse: 0.0174 - val_loss: 0.1322 - val_mse: 0.0175\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.1310 - mse: 0.0175 - val_loss: 0.1243 - val_mse: 0.0155\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0850 - mse: 0.0072 - val_loss: 0.1540 - val_mse: 0.0237\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 1s 288ms/step - loss: 0.0954 - mse: 0.0091 - val_loss: 0.1271 - val_mse: 0.0162\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0677 - mse: 0.0046 - val_loss: 0.1019 - val_mse: 0.0104\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0607 - mse: 0.0037 - val_loss: 0.1012 - val_mse: 0.0102\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0500 - mse: 0.0025 - val_loss: 0.1042 - val_mse: 0.0109\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0457 - mse: 0.0021 - val_loss: 0.1042 - val_mse: 0.0109\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0435 - mse: 0.0019 - val_loss: 0.1047 - val_mse: 0.0110\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0464 - mse: 0.0022 - val_loss: 0.1006 - val_mse: 0.0101\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0384 - mse: 0.0015 - val_loss: 0.1008 - val_mse: 0.0102\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 1s 295ms/step - loss: 0.0412 - mse: 0.0017 - val_loss: 0.0984 - val_mse: 0.0097\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0369 - mse: 0.0014 - val_loss: 0.0952 - val_mse: 0.0091\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0405 - mse: 0.0016 - val_loss: 0.0946 - val_mse: 0.0089\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0384 - mse: 0.0015 - val_loss: 0.0967 - val_mse: 0.0094\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0383 - mse: 0.0015 - val_loss: 0.0965 - val_mse: 0.0093\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0362 - mse: 0.0013 - val_loss: 0.0941 - val_mse: 0.0089\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0353 - mse: 0.0012 - val_loss: 0.0945 - val_mse: 0.0089\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0350 - mse: 0.0012 - val_loss: 0.0949 - val_mse: 0.0090\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0356 - mse: 0.0013 - val_loss: 0.0939 - val_mse: 0.0088\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0340 - mse: 0.0012 - val_loss: 0.0935 - val_mse: 0.0087\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0346 - mse: 0.0012 - val_loss: 0.0923 - val_mse: 0.0085\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0331 - mse: 0.0011 - val_loss: 0.0934 - val_mse: 0.0087\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0352 - mse: 0.0012 - val_loss: 0.0911 - val_mse: 0.0083\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0341 - mse: 0.0012 - val_loss: 0.0910 - val_mse: 0.0083\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 0.0345 - mse: 0.0012 - val_loss: 0.0907 - val_mse: 0.0082\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0333 - mse: 0.0011 - val_loss: 0.0911 - val_mse: 0.0083\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 0.0334 - mse: 0.0011 - val_loss: 0.0902 - val_mse: 0.0081\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 1s 313ms/step - loss: 0.0326 - mse: 0.0011 - val_loss: 0.0899 - val_mse: 0.0081\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0321 - mse: 0.0010 - val_loss: 0.0900 - val_mse: 0.0081\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0330 - mse: 0.0011 - val_loss: 0.0889 - val_mse: 0.0079\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0321 - mse: 0.0010 - val_loss: 0.0882 - val_mse: 0.0078\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0316 - mse: 9.9911e-04 - val_loss: 0.0881 - val_mse: 0.0078\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0318 - mse: 0.0010 - val_loss: 0.0874 - val_mse: 0.0076\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0317 - mse: 0.0010 - val_loss: 0.0873 - val_mse: 0.0076\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0308 - mse: 9.4929e-04 - val_loss: 0.0872 - val_mse: 0.0076\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0319 - mse: 0.0010 - val_loss: 0.0867 - val_mse: 0.0075\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0313 - mse: 9.8226e-04 - val_loss: 0.0862 - val_mse: 0.0074\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 1s 291ms/step - loss: 0.0312 - mse: 9.7945e-04 - val_loss: 0.0855 - val_mse: 0.0073\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0309 - mse: 9.5855e-04 - val_loss: 0.0848 - val_mse: 0.0072\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0306 - mse: 9.3958e-04 - val_loss: 0.0842 - val_mse: 0.0071\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0304 - mse: 9.2272e-04 - val_loss: 0.0837 - val_mse: 0.0070\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0304 - mse: 9.2477e-04 - val_loss: 0.0834 - val_mse: 0.0070\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0304 - mse: 9.2250e-04 - val_loss: 0.0830 - val_mse: 0.0069\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0300 - mse: 8.9917e-04 - val_loss: 0.0826 - val_mse: 0.0068\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0298 - mse: 8.8883e-04 - val_loss: 0.0821 - val_mse: 0.0067\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0292 - mse: 8.5340e-04 - val_loss: 0.0814 - val_mse: 0.0066\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0295 - mse: 8.7105e-04 - val_loss: 0.0806 - val_mse: 0.0065\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0297 - mse: 8.8202e-04 - val_loss: 0.0799 - val_mse: 0.0064\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.0295 - mse: 8.6793e-04 - val_loss: 0.0794 - val_mse: 0.0063\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0288 - mse: 8.3138e-04 - val_loss: 0.0790 - val_mse: 0.0062\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0288 - mse: 8.3219e-04 - val_loss: 0.0786 - val_mse: 0.0062\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0290 - mse: 8.4165e-04 - val_loss: 0.0780 - val_mse: 0.0061\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0287 - mse: 8.2155e-04 - val_loss: 0.0773 - val_mse: 0.0060\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0291 - mse: 8.4584e-04 - val_loss: 0.0766 - val_mse: 0.0059\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0292 - mse: 8.5582e-04 - val_loss: 0.0764 - val_mse: 0.0058\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0300 - mse: 9.0021e-04 - val_loss: 0.0755 - val_mse: 0.0057\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.0291 - mse: 8.4700e-04 - val_loss: 0.0752 - val_mse: 0.0057\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 0.0291 - mse: 8.4916e-04 - val_loss: 0.0748 - val_mse: 0.0056\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0289 - mse: 8.3658e-04 - val_loss: 0.0745 - val_mse: 0.0055\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0285 - mse: 8.2009e-04 - val_loss: 0.0754 - val_mse: 0.0057\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0302 - mse: 9.1286e-04 - val_loss: 0.0741 - val_mse: 0.0055\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0296 - mse: 8.7579e-04 - val_loss: 0.0734 - val_mse: 0.0054\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 1s 311ms/step - loss: 0.0287 - mse: 8.2139e-04 - val_loss: 0.0727 - val_mse: 0.0053\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 1s 311ms/step - loss: 0.0278 - mse: 7.7768e-04 - val_loss: 0.0728 - val_mse: 0.0053\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 1s 295ms/step - loss: 0.0290 - mse: 8.4161e-04 - val_loss: 0.0723 - val_mse: 0.0052\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 0.0276 - mse: 7.6371e-04 - val_loss: 0.0722 - val_mse: 0.0052\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 1s 313ms/step - loss: 0.0285 - mse: 8.1109e-04 - val_loss: 0.0719 - val_mse: 0.0052\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.0280 - mse: 7.8369e-04 - val_loss: 0.0712 - val_mse: 0.0051\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0278 - mse: 7.7668e-04 - val_loss: 0.0707 - val_mse: 0.0050\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0285 - mse: 8.1318e-04 - val_loss: 0.0702 - val_mse: 0.0049\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 1s 319ms/step - loss: 0.0289 - mse: 8.3881e-04 - val_loss: 0.0700 - val_mse: 0.0049\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0297 - mse: 8.8835e-04 - val_loss: 0.0700 - val_mse: 0.0049\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0278 - mse: 7.7253e-04 - val_loss: 0.0699 - val_mse: 0.0049\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.0272 - mse: 7.4055e-04 - val_loss: 0.0694 - val_mse: 0.0048\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0277 - mse: 7.6883e-04 - val_loss: 0.0688 - val_mse: 0.0047\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0267 - mse: 7.1505e-04 - val_loss: 0.0689 - val_mse: 0.0048\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0275 - mse: 7.5546e-04 - val_loss: 0.0686 - val_mse: 0.0047\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0273 - mse: 7.4510e-04 - val_loss: 0.0684 - val_mse: 0.0047\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 1s 316ms/step - loss: 0.0270 - mse: 7.3046e-04 - val_loss: 0.0681 - val_mse: 0.0046\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0266 - mse: 7.0883e-04 - val_loss: 0.0675 - val_mse: 0.0046\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0271 - mse: 7.3741e-04 - val_loss: 0.0671 - val_mse: 0.0045\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0269 - mse: 7.2238e-04 - val_loss: 0.0670 - val_mse: 0.0045\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 1s 326ms/step - loss: 0.0284 - mse: 8.0619e-04 - val_loss: 0.0667 - val_mse: 0.0044\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0268 - mse: 7.1740e-04 - val_loss: 0.0664 - val_mse: 0.0044\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 1s 316ms/step - loss: 0.0263 - mse: 6.9156e-04 - val_loss: 0.0656 - val_mse: 0.0043\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 1s 324ms/step - loss: 0.0264 - mse: 6.9583e-04 - val_loss: 0.0654 - val_mse: 0.0043\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.0269 - mse: 7.2620e-04 - val_loss: 0.0653 - val_mse: 0.0043\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0265 - mse: 7.0209e-04 - val_loss: 0.0655 - val_mse: 0.0043\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0264 - mse: 6.9959e-04 - val_loss: 0.0664 - val_mse: 0.0044\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.0270 - mse: 7.3160e-04 - val_loss: 0.0675 - val_mse: 0.0045\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0308 - mse: 9.5018e-04 - val_loss: 0.0681 - val_mse: 0.0046\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0330 - mse: 0.0011 - val_loss: 0.0661 - val_mse: 0.0044\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0317 - mse: 0.0010 - val_loss: 0.0644 - val_mse: 0.0042\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0286 - mse: 8.3678e-04 - val_loss: 0.0665 - val_mse: 0.0044\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 1s 314ms/step - loss: 0.0294 - mse: 8.6222e-04 - val_loss: 0.0685 - val_mse: 0.0047\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 1s 319ms/step - loss: 0.0315 - mse: 9.9364e-04 - val_loss: 0.0661 - val_mse: 0.0044\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0277 - mse: 7.7861e-04 - val_loss: 0.0655 - val_mse: 0.0043\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0270 - mse: 7.3331e-04 - val_loss: 0.0653 - val_mse: 0.0043\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0264 - mse: 6.9890e-04 - val_loss: 0.0652 - val_mse: 0.0043\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0273 - mse: 7.4754e-04 - val_loss: 0.0657 - val_mse: 0.0043\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0265 - mse: 7.0578e-04 - val_loss: 0.0652 - val_mse: 0.0043\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.0265 - mse: 7.0192e-04 - val_loss: 0.0651 - val_mse: 0.0042\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 1s 297ms/step - loss: 0.0266 - mse: 7.0996e-04 - val_loss: 0.0645 - val_mse: 0.0042\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 1s 318ms/step - loss: 0.0258 - mse: 6.6569e-04 - val_loss: 0.0646 - val_mse: 0.0042\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0266 - mse: 7.0959e-04 - val_loss: 0.0644 - val_mse: 0.0042\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0271 - mse: 7.3258e-04 - val_loss: 0.0633 - val_mse: 0.0040\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0260 - mse: 6.7689e-04 - val_loss: 0.0630 - val_mse: 0.0040\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0260 - mse: 6.7746e-04 - val_loss: 0.0634 - val_mse: 0.0040\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 1s 311ms/step - loss: 0.0262 - mse: 6.8746e-04 - val_loss: 0.0626 - val_mse: 0.0039\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0254 - mse: 6.4567e-04 - val_loss: 0.0625 - val_mse: 0.0039\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 1s 295ms/step - loss: 0.0247 - mse: 6.0830e-04 - val_loss: 0.0620 - val_mse: 0.0038\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0256 - mse: 6.5743e-04 - val_loss: 0.0617 - val_mse: 0.0038\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0245 - mse: 5.9970e-04 - val_loss: 0.0620 - val_mse: 0.0038\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0253 - mse: 6.4218e-04 - val_loss: 0.0611 - val_mse: 0.0037\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0245 - mse: 6.0061e-04 - val_loss: 0.0609 - val_mse: 0.0037\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0252 - mse: 6.3281e-04 - val_loss: 0.0607 - val_mse: 0.0037\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0250 - mse: 6.2947e-04 - val_loss: 0.0608 - val_mse: 0.0037\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0251 - mse: 6.2955e-04 - val_loss: 0.0612 - val_mse: 0.0037\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 0.0261 - mse: 6.8079e-04 - val_loss: 0.0600 - val_mse: 0.0036\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0259 - mse: 6.7061e-04 - val_loss: 0.0595 - val_mse: 0.0035\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0247 - mse: 6.1415e-04 - val_loss: 0.0601 - val_mse: 0.0036\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0247 - mse: 6.1476e-04 - val_loss: 0.0595 - val_mse: 0.0035\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0249 - mse: 6.2193e-04 - val_loss: 0.0600 - val_mse: 0.0036\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0251 - mse: 6.2790e-04 - val_loss: 0.0594 - val_mse: 0.0035\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0246 - mse: 6.0630e-04 - val_loss: 0.0592 - val_mse: 0.0035\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0245 - mse: 6.0115e-04 - val_loss: 0.0589 - val_mse: 0.0035\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0235 - mse: 5.5555e-04 - val_loss: 0.0589 - val_mse: 0.0035\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 1s 328ms/step - loss: 0.0243 - mse: 5.9287e-04 - val_loss: 0.0589 - val_mse: 0.0035\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.0246 - mse: 6.0557e-04 - val_loss: 0.0591 - val_mse: 0.0035\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0248 - mse: 6.1700e-04 - val_loss: 0.0586 - val_mse: 0.0034\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0242 - mse: 5.8467e-04 - val_loss: 0.0585 - val_mse: 0.0034\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0242 - mse: 5.8895e-04 - val_loss: 0.0586 - val_mse: 0.0034\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0245 - mse: 6.0249e-04 - val_loss: 0.0582 - val_mse: 0.0034\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 0.0241 - mse: 5.8075e-04 - val_loss: 0.0586 - val_mse: 0.0034\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 1s 302ms/step - loss: 0.0242 - mse: 5.8459e-04 - val_loss: 0.0582 - val_mse: 0.0034\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 1s 328ms/step - loss: 0.0240 - mse: 5.7744e-04 - val_loss: 0.0584 - val_mse: 0.0034\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 1s 324ms/step - loss: 0.0238 - mse: 5.6849e-04 - val_loss: 0.0579 - val_mse: 0.0034\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 0.0237 - mse: 5.6383e-04 - val_loss: 0.0581 - val_mse: 0.0034\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 1s 330ms/step - loss: 0.0235 - mse: 5.5283e-04 - val_loss: 0.0576 - val_mse: 0.0033\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.0243 - mse: 5.9168e-04 - val_loss: 0.0581 - val_mse: 0.0034\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 1s 316ms/step - loss: 0.0244 - mse: 5.9553e-04 - val_loss: 0.0575 - val_mse: 0.0033\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0242 - mse: 5.8815e-04 - val_loss: 0.0573 - val_mse: 0.0033\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0234 - mse: 5.4674e-04 - val_loss: 0.0571 - val_mse: 0.0033\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.0241 - mse: 5.8208e-04 - val_loss: 0.0569 - val_mse: 0.0032\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0233 - mse: 5.4157e-04 - val_loss: 0.0571 - val_mse: 0.0033\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 0.0232 - mse: 5.3882e-04 - val_loss: 0.0569 - val_mse: 0.0032\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0231 - mse: 5.3556e-04 - val_loss: 0.0571 - val_mse: 0.0033\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 1s 313ms/step - loss: 0.0236 - mse: 5.6388e-04 - val_loss: 0.0569 - val_mse: 0.0032\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.0238 - mse: 5.6821e-04 - val_loss: 0.0571 - val_mse: 0.0033\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0237 - mse: 5.5988e-04 - val_loss: 0.0565 - val_mse: 0.0032\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 1s 324ms/step - loss: 0.0229 - mse: 5.2505e-04 - val_loss: 0.0569 - val_mse: 0.0032\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0234 - mse: 5.4881e-04 - val_loss: 0.0563 - val_mse: 0.0032\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0236 - mse: 5.5728e-04 - val_loss: 0.0565 - val_mse: 0.0032\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 1s 295ms/step - loss: 0.0231 - mse: 5.3507e-04 - val_loss: 0.0565 - val_mse: 0.0032\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0238 - mse: 5.6646e-04 - val_loss: 0.0562 - val_mse: 0.0032\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0226 - mse: 5.1277e-04 - val_loss: 0.0561 - val_mse: 0.0031\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 0.0230 - mse: 5.3105e-04 - val_loss: 0.0561 - val_mse: 0.0031\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0228 - mse: 5.2093e-04 - val_loss: 0.0559 - val_mse: 0.0031\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0224 - mse: 5.0080e-04 - val_loss: 0.0560 - val_mse: 0.0031\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.0234 - mse: 5.4710e-04 - val_loss: 0.0562 - val_mse: 0.0032\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0225 - mse: 5.0940e-04 - val_loss: 0.0560 - val_mse: 0.0031\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0221 - mse: 4.8919e-04 - val_loss: 0.0557 - val_mse: 0.0031\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 1s 310ms/step - loss: 0.0231 - mse: 5.3462e-04 - val_loss: 0.0554 - val_mse: 0.0031\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 1s 301ms/step - loss: 0.0223 - mse: 4.9647e-04 - val_loss: 0.0554 - val_mse: 0.0031\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.0229 - mse: 5.2650e-04 - val_loss: 0.0563 - val_mse: 0.0032\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0236 - mse: 5.5921e-04 - val_loss: 0.0555 - val_mse: 0.0031\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0229 - mse: 5.2541e-04 - val_loss: 0.0552 - val_mse: 0.0030\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0228 - mse: 5.1806e-04 - val_loss: 0.0550 - val_mse: 0.0030\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 1s 307ms/step - loss: 0.0230 - mse: 5.3134e-04 - val_loss: 0.0553 - val_mse: 0.0031\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0221 - mse: 4.8876e-04 - val_loss: 0.0554 - val_mse: 0.0031\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0230 - mse: 5.3138e-04 - val_loss: 0.0550 - val_mse: 0.0030\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0220 - mse: 4.8496e-04 - val_loss: 0.0548 - val_mse: 0.0030\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 0.0219 - mse: 4.8185e-04 - val_loss: 0.0547 - val_mse: 0.0030\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.0219 - mse: 4.7940e-04 - val_loss: 0.0548 - val_mse: 0.0030\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.0226 - mse: 5.1357e-04 - val_loss: 0.0544 - val_mse: 0.0030\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 1s 305ms/step - loss: 0.0217 - mse: 4.7128e-04 - val_loss: 0.0545 - val_mse: 0.0030\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0218 - mse: 4.7508e-04 - val_loss: 0.0546 - val_mse: 0.0030\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0222 - mse: 4.9323e-04 - val_loss: 0.0550 - val_mse: 0.0030\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 1s 304ms/step - loss: 0.0223 - mse: 4.9842e-04 - val_loss: 0.0546 - val_mse: 0.0030\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0233 - mse: 5.4110e-04 - val_loss: 0.0547 - val_mse: 0.0030\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0224 - mse: 5.0055e-04 - val_loss: 0.0540 - val_mse: 0.0029\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0218 - mse: 4.7725e-04 - val_loss: 0.0540 - val_mse: 0.0029\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 1s 297ms/step - loss: 0.0214 - mse: 4.5906e-04 - val_loss: 0.0544 - val_mse: 0.0030\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0229 - mse: 5.2507e-04 - val_loss: 0.0538 - val_mse: 0.0029\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0215 - mse: 4.6172e-04 - val_loss: 0.0538 - val_mse: 0.0029\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0218 - mse: 4.7732e-04 - val_loss: 0.0535 - val_mse: 0.0029\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 1s 311ms/step - loss: 0.0212 - mse: 4.4897e-04 - val_loss: 0.0537 - val_mse: 0.0029\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 1s 297ms/step - loss: 0.0216 - mse: 4.6571e-04 - val_loss: 0.0537 - val_mse: 0.0029\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 1s 298ms/step - loss: 0.0224 - mse: 5.0050e-04 - val_loss: 0.0534 - val_mse: 0.0029\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 1s 313ms/step - loss: 0.0219 - mse: 4.7975e-04 - val_loss: 0.0532 - val_mse: 0.0028\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.0215 - mse: 4.6328e-04 - val_loss: 0.0537 - val_mse: 0.0029\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.0218 - mse: 4.7646e-04 - val_loss: 0.0531 - val_mse: 0.0028\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0216 - mse: 4.6684e-04 - val_loss: 0.0532 - val_mse: 0.0028\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0212 - mse: 4.5110e-04 - val_loss: 0.0532 - val_mse: 0.0028\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 0.0212 - mse: 4.5092e-04 - val_loss: 0.0532 - val_mse: 0.0028\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 1s 308ms/step - loss: 0.0216 - mse: 4.6689e-04 - val_loss: 0.0531 - val_mse: 0.0028\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 0.0211 - mse: 4.4814e-04 - val_loss: 0.0528 - val_mse: 0.0028\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 1s 318ms/step - loss: 0.0207 - mse: 4.2895e-04 - val_loss: 0.0526 - val_mse: 0.0028\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (x.shape[1],x.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1,activation=\"tanh\"))\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "model.compile(optimizer='adam', loss = root_mean_squared_error, metrics=['mse'])\n",
    "history = model.fit(x, y, epochs=200, batch_size=512, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f3cb60c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0962 - mse: 0.0103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09620599448680878, 0.010265326127409935]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68e7af",
   "metadata": {},
   "source": [
    "# 5. 논문과 동일(데이터와 데이터 기간와 변수가 다름, 금 + 유가+ 화페교환비)\n",
    "\n",
    "- news data : nbc 데이터 사용\n",
    "- historical data : senment score, 다우존스, 금, 유가, 화폐교환비(일본, 중국, 유럽, 캐나다, 인도)의 close 사용\n",
    "- 기간 : 2017년 1월 1일 ~ 2022년 04월 31일 까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d74824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"C:/Users/default.DESKTOP-IT64657/Desktop/논문code/csv_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9d0aff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv(dir+\"/price_data_score.csv\")\n",
    "score.columns = [\"Date\", \"Score\", \"class_3\", \"class_5\"]\n",
    "score = score[[\"Date\",\"class_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a7ea4843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  8 of 8 completed\n"
     ]
    }
   ],
   "source": [
    "price_data = yf.download([\"^DJI\", \"CL=F\",\"GC=F\",\"INR=X\",\"JPY=X\",\"CNY=X\",\"CAD=X\",\"EURUSD=X\"],start = '2017-01-01', end = \"2022-05-01\")[\"Close\"]\n",
    "price_data = price_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d5faff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  8 of 8 completed\n"
     ]
    }
   ],
   "source": [
    "first_data = yf.download([\"^DJI\", \"CL=F\",\"GC=F\",\"INR=X\",\"JPY=X\",\"CNY=X\",\"CAD=X\",\"EURUSD=X\"],start = '2016-12-31', end = '2017-01-03')[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "5e568670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(time):\n",
    "    time = pd.Timestamp(time)\n",
    "    return time\n",
    "\n",
    "score.Date = score.Date.apply(lambda x: time_conversion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d9c6ed99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>CAD=X</th>\n",
       "      <th>CL=F</th>\n",
       "      <th>CNY=X</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>GC=F</th>\n",
       "      <th>INR=X</th>\n",
       "      <th>JPY=X</th>\n",
       "      <th>^DJI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1.34340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9438</td>\n",
       "      <td>1.052698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.944801</td>\n",
       "      <td>116.794998</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1.34414</td>\n",
       "      <td>52.330002</td>\n",
       "      <td>6.9440</td>\n",
       "      <td>1.046003</td>\n",
       "      <td>1160.400024</td>\n",
       "      <td>68.133904</td>\n",
       "      <td>117.495003</td>\n",
       "      <td>19881.759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1.34218</td>\n",
       "      <td>53.259998</td>\n",
       "      <td>6.9598</td>\n",
       "      <td>1.041992</td>\n",
       "      <td>1163.800049</td>\n",
       "      <td>68.269798</td>\n",
       "      <td>117.658997</td>\n",
       "      <td>19942.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1.32990</td>\n",
       "      <td>53.759998</td>\n",
       "      <td>6.9251</td>\n",
       "      <td>1.050089</td>\n",
       "      <td>1179.699951</td>\n",
       "      <td>67.885498</td>\n",
       "      <td>117.112999</td>\n",
       "      <td>19899.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>1.32294</td>\n",
       "      <td>53.990002</td>\n",
       "      <td>6.8879</td>\n",
       "      <td>1.060592</td>\n",
       "      <td>1171.900024</td>\n",
       "      <td>67.733002</td>\n",
       "      <td>115.264999</td>\n",
       "      <td>19963.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>1.27210</td>\n",
       "      <td>98.540001</td>\n",
       "      <td>6.5003</td>\n",
       "      <td>1.081105</td>\n",
       "      <td>1893.199951</td>\n",
       "      <td>76.463501</td>\n",
       "      <td>128.604996</td>\n",
       "      <td>34049.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>1.27356</td>\n",
       "      <td>101.699997</td>\n",
       "      <td>6.5579</td>\n",
       "      <td>1.071421</td>\n",
       "      <td>1901.400024</td>\n",
       "      <td>76.527802</td>\n",
       "      <td>127.744003</td>\n",
       "      <td>33240.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>2022-04-27</td>\n",
       "      <td>1.28061</td>\n",
       "      <td>102.019997</td>\n",
       "      <td>6.5563</td>\n",
       "      <td>1.064362</td>\n",
       "      <td>1885.900024</td>\n",
       "      <td>76.794098</td>\n",
       "      <td>127.265999</td>\n",
       "      <td>33301.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>1.28140</td>\n",
       "      <td>105.360001</td>\n",
       "      <td>6.5595</td>\n",
       "      <td>1.055509</td>\n",
       "      <td>1888.699951</td>\n",
       "      <td>76.472198</td>\n",
       "      <td>128.393997</td>\n",
       "      <td>33916.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>2022-04-29</td>\n",
       "      <td>1.28046</td>\n",
       "      <td>104.690002</td>\n",
       "      <td>6.6253</td>\n",
       "      <td>1.050420</td>\n",
       "      <td>1909.300049</td>\n",
       "      <td>76.588997</td>\n",
       "      <td>130.811005</td>\n",
       "      <td>32977.210938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    CAD=X        CL=F   CNY=X  EURUSD=X         GC=F  \\\n",
       "0    2017-01-02  1.34340         NaN  6.9438  1.052698          NaN   \n",
       "1    2017-01-03  1.34414   52.330002  6.9440  1.046003  1160.400024   \n",
       "2    2017-01-04  1.34218   53.259998  6.9598  1.041992  1163.800049   \n",
       "3    2017-01-05  1.32990   53.759998  6.9251  1.050089  1179.699951   \n",
       "4    2017-01-06  1.32294   53.990002  6.8879  1.060592  1171.900024   \n",
       "...         ...      ...         ...     ...       ...          ...   \n",
       "1385 2022-04-25  1.27210   98.540001  6.5003  1.081105  1893.199951   \n",
       "1386 2022-04-26  1.27356  101.699997  6.5579  1.071421  1901.400024   \n",
       "1387 2022-04-27  1.28061  102.019997  6.5563  1.064362  1885.900024   \n",
       "1388 2022-04-28  1.28140  105.360001  6.5595  1.055509  1888.699951   \n",
       "1389 2022-04-29  1.28046  104.690002  6.6253  1.050420  1909.300049   \n",
       "\n",
       "          INR=X       JPY=X          ^DJI  \n",
       "0     67.944801  116.794998           NaN  \n",
       "1     68.133904  117.495003  19881.759766  \n",
       "2     68.269798  117.658997  19942.160156  \n",
       "3     67.885498  117.112999  19899.289062  \n",
       "4     67.733002  115.264999  19963.800781  \n",
       "...         ...         ...           ...  \n",
       "1385  76.463501  128.604996  34049.460938  \n",
       "1386  76.527802  127.744003  33240.179688  \n",
       "1387  76.794098  127.265999  33301.929688  \n",
       "1388  76.472198  128.393997  33916.390625  \n",
       "1389  76.588997  130.811005  32977.210938  \n",
       "\n",
       "[1390 rows x 9 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e929bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-266-0de8027c808f>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data.before_close[i+1] = price_data[\"^DJI\"][i]\n"
     ]
    }
   ],
   "source": [
    "def first(close):\n",
    "    if np.isnan(close[1]):\n",
    "        value = close[0]\n",
    "    else: \n",
    "        value = close[1]\n",
    "    \n",
    "    return value\n",
    "\n",
    "price_data = pd.merge(price_data, score, how =\"left\",left_on='Date', right_on = \"Date\")[[\"^DJI\", \"CL=F\",\"GC=F\",\"INR=X\",\"JPY=X\",\"CNY=X\",\"CAD=X\",\"EURUSD=X\",\"class_3\"]]\n",
    "price_data = price_data.dropna(axis=0)\n",
    "price_data[\"before_close\"] = 0\n",
    "price_data = price_data.reset_index(drop=True)\n",
    "for i in range(len(price_data)-1):\n",
    "    price_data.before_close[i+1] = price_data[\"^DJI\"][i]\n",
    "\n",
    "price_data.before_close[0] = first(first_data[\"^DJI\"])\n",
    "\n",
    "\n",
    "for i in range(len(price_data)):\n",
    "    price_data[\"JPY=X\"][len(price_data)-i] =price_data[\"JPY=X\"][len(price_data)-i-1]\n",
    "    price_data[\"CAD=X\"][len(price_data)-i] =price_data[\"CAD=X\"][len(price_data)-i-1]\n",
    "    price_data[\"CL=F\"][len(price_data)-i] =price_data[\"CL=F\"][len(price_data)-i-1]\n",
    "    price_data[\"CNY=X\"][len(price_data)-i] =price_data[\"CNY=X\"][len(price_data)-i-1]\n",
    "    price_data[\"EURUSD=X\"][len(price_data)-i] =price_data[\"EURUSD=X\"][len(price_data)-i-1]\n",
    "    price_data[\"GC=F\"][len(price_data)-i] =price_data[\"GC=F\"][len(price_data)-i-1]\n",
    "    price_data[\"INR=X\"][len(price_data)-i] =price_data[\"INR=X\"][len(price_data)-i-1]\n",
    "    \n",
    "price_data[\"JPY=X\"][0] = first(first_data[\"JPY=X\"])\n",
    "price_data[\"CAD=X\"][0] = first(first_data[\"CAD=X\"])\n",
    "price_data[\"CL=F\"][0] = first(first_data[\"CL=F\"])\n",
    "price_data[\"CNY=X\"][0] = first(first_data[\"CNY=X\"])\n",
    "price_data[\"EURUSD=X\"][0] = first(first_data[\"EURUSD=X\"])\n",
    "price_data[\"GC=F\"][0] = first(first_data[\"GC=F\"])\n",
    "price_data[\"INR=X\"][0] = first(first_data[\"INR=X\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "price_data = pd.DataFrame(MinMaxScaler().fit_transform(price_data))\n",
    "price_data.columns = [\"Close\",\"Oil\",\"Gold\",\"india\",\"japan\",\"china\",\"canada\",\"europe\",\"class_3\",\"before_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d44efe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Oil</th>\n",
       "      <th>Gold</th>\n",
       "      <th>india</th>\n",
       "      <th>japan</th>\n",
       "      <th>china</th>\n",
       "      <th>canada</th>\n",
       "      <th>europe</th>\n",
       "      <th>class_3</th>\n",
       "      <th>before_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008751</td>\n",
       "      <td>0.566231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327144</td>\n",
       "      <td>0.529619</td>\n",
       "      <td>0.742750</td>\n",
       "      <td>0.579975</td>\n",
       "      <td>0.051221</td>\n",
       "      <td>0.507353</td>\n",
       "      <td>0.001769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012290</td>\n",
       "      <td>0.557615</td>\n",
       "      <td>0.011536</td>\n",
       "      <td>0.340364</td>\n",
       "      <td>0.555881</td>\n",
       "      <td>0.742970</td>\n",
       "      <td>0.583050</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.008751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009778</td>\n",
       "      <td>0.563379</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>0.349864</td>\n",
       "      <td>0.562033</td>\n",
       "      <td>0.760325</td>\n",
       "      <td>0.574907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.012290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013558</td>\n",
       "      <td>0.566479</td>\n",
       "      <td>0.032945</td>\n",
       "      <td>0.322999</td>\n",
       "      <td>0.541550</td>\n",
       "      <td>0.722210</td>\n",
       "      <td>0.523889</td>\n",
       "      <td>0.038740</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>0.009778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.567904</td>\n",
       "      <td>0.024293</td>\n",
       "      <td>0.312338</td>\n",
       "      <td>0.472219</td>\n",
       "      <td>0.681349</td>\n",
       "      <td>0.494973</td>\n",
       "      <td>0.088988</td>\n",
       "      <td>0.574766</td>\n",
       "      <td>0.013558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>0.838862</td>\n",
       "      <td>0.865927</td>\n",
       "      <td>0.866334</td>\n",
       "      <td>0.910185</td>\n",
       "      <td>0.965298</td>\n",
       "      <td>0.199582</td>\n",
       "      <td>0.228957</td>\n",
       "      <td>0.198677</td>\n",
       "      <td>0.467213</td>\n",
       "      <td>0.824913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>0.791444</td>\n",
       "      <td>0.844046</td>\n",
       "      <td>0.824404</td>\n",
       "      <td>0.922650</td>\n",
       "      <td>0.972688</td>\n",
       "      <td>0.255602</td>\n",
       "      <td>0.283756</td>\n",
       "      <td>0.187132</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.838862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>0.795063</td>\n",
       "      <td>0.863634</td>\n",
       "      <td>0.833500</td>\n",
       "      <td>0.927144</td>\n",
       "      <td>0.940387</td>\n",
       "      <td>0.318871</td>\n",
       "      <td>0.289822</td>\n",
       "      <td>0.140801</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.791444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0.831065</td>\n",
       "      <td>0.865617</td>\n",
       "      <td>0.816306</td>\n",
       "      <td>0.945760</td>\n",
       "      <td>0.922454</td>\n",
       "      <td>0.317114</td>\n",
       "      <td>0.319111</td>\n",
       "      <td>0.107027</td>\n",
       "      <td>0.356061</td>\n",
       "      <td>0.795063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>0.776037</td>\n",
       "      <td>0.886320</td>\n",
       "      <td>0.819412</td>\n",
       "      <td>0.923258</td>\n",
       "      <td>0.964772</td>\n",
       "      <td>0.320628</td>\n",
       "      <td>0.322393</td>\n",
       "      <td>0.064672</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.831065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close       Oil      Gold     india     japan     china    canada  \\\n",
       "0     0.008751  0.566231  0.000000  0.327144  0.529619  0.742750  0.579975   \n",
       "1     0.012290  0.557615  0.011536  0.340364  0.555881  0.742970  0.583050   \n",
       "2     0.009778  0.563379  0.015308  0.349864  0.562033  0.760325  0.574907   \n",
       "3     0.013558  0.566479  0.032945  0.322999  0.541550  0.722210  0.523889   \n",
       "4     0.009081  0.567904  0.024293  0.312338  0.472219  0.681349  0.494973   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1304  0.838862  0.865927  0.866334  0.910185  0.965298  0.199582  0.228957   \n",
       "1305  0.791444  0.844046  0.824404  0.922650  0.972688  0.255602  0.283756   \n",
       "1306  0.795063  0.863634  0.833500  0.927144  0.940387  0.318871  0.289822   \n",
       "1307  0.831065  0.865617  0.816306  0.945760  0.922454  0.317114  0.319111   \n",
       "1308  0.776037  0.886320  0.819412  0.923258  0.964772  0.320628  0.322393   \n",
       "\n",
       "        europe   class_3  before_close  \n",
       "0     0.051221  0.507353      0.001769  \n",
       "1     0.019190  0.535088      0.008751  \n",
       "2     0.000000  0.692308      0.012290  \n",
       "3     0.038740  0.574074      0.009778  \n",
       "4     0.088988  0.574766      0.013558  \n",
       "...        ...       ...           ...  \n",
       "1304  0.198677  0.467213      0.824913  \n",
       "1305  0.187132  0.550847      0.838862  \n",
       "1306  0.140801  0.537313      0.791444  \n",
       "1307  0.107027  0.356061      0.795063  \n",
       "1308  0.064672  0.468254      0.831065  \n",
       "\n",
       "[1309 rows x 10 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "40855023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(len(price_data)*0.7)\n",
    "\n",
    "train = price_data.iloc[0:train_index]\n",
    "test = price_data.iloc[train_index:len(price_data)]\n",
    "\n",
    "\n",
    "train_x = train[[\"Oil\",\"Gold\",\"india\",\"japan\",\"china\",\"canada\",\"europe\",\"class_3\",\"before_close\"]]\n",
    "train_y = train[[\"Close\"]]\n",
    "\n",
    "test_x = test[[\"Oil\",\"Gold\",\"india\",\"japan\",\"china\",\"canada\",\"europe\",\"class_3\",\"before_close\"]]\n",
    "test_y = test[[\"Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "612e27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.to_numpy().reshape(len(train_x),1,9)\n",
    "train_y = train_y.to_numpy().reshape(len(train_y),1)\n",
    "test_x = test_x.to_numpy().reshape(len(test_x),1,9)\n",
    "test_y = test_y.to_numpy().reshape(len(test_y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e39f923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "558c5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(shape=(len(train_x)-window_size+1,window_size,9))\n",
    "for i in range(len(train_x)-window_size+1):\n",
    "    x[i]=np.vstack((train_x[i:i+window_size]))\n",
    "\n",
    "y = train_y[window_size-1:len(train_y)]\n",
    "\n",
    "x_t = np.zeros(shape=(len(test_x)-window_size+1,window_size,9))\n",
    "for i in range(len(test_x)-window_size+1):\n",
    "    x_t[i]=np.vstack((test_x[i:i+window_size]))\n",
    "    \n",
    "y_t = test_y[window_size-1:len(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c9f73cbe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 3s 545ms/step - loss: 0.3628 - mse: 0.1325 - val_loss: 0.3612 - val_mse: 0.1305\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.2405 - mse: 0.0588 - val_loss: 0.2252 - val_mse: 0.0507\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1522 - mse: 0.0233 - val_loss: 0.1439 - val_mse: 0.0207\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1540 - mse: 0.0238 - val_loss: 0.1380 - val_mse: 0.0190\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1582 - mse: 0.0250 - val_loss: 0.1384 - val_mse: 0.0192\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1545 - mse: 0.0239 - val_loss: 0.1357 - val_mse: 0.0184\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1292 - mse: 0.0167 - val_loss: 0.1422 - val_mse: 0.0202\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1119 - mse: 0.0125 - val_loss: 0.1571 - val_mse: 0.0247\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1121 - mse: 0.0126 - val_loss: 0.1653 - val_mse: 0.0273\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1165 - mse: 0.0136 - val_loss: 0.1590 - val_mse: 0.0253\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1111 - mse: 0.0123 - val_loss: 0.1455 - val_mse: 0.0212\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0981 - mse: 0.0096 - val_loss: 0.1371 - val_mse: 0.0188\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0949 - mse: 0.0090 - val_loss: 0.1392 - val_mse: 0.0194\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0937 - mse: 0.0088 - val_loss: 0.1438 - val_mse: 0.0207\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0935 - mse: 0.0087 - val_loss: 0.1440 - val_mse: 0.0207\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0843 - mse: 0.0071 - val_loss: 0.1405 - val_mse: 0.0197\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0807 - mse: 0.0065 - val_loss: 0.1378 - val_mse: 0.0190\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0810 - mse: 0.0066 - val_loss: 0.1381 - val_mse: 0.0191\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0823 - mse: 0.0068 - val_loss: 0.1433 - val_mse: 0.0205\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0767 - mse: 0.0059 - val_loss: 0.1533 - val_mse: 0.0235\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0746 - mse: 0.0056 - val_loss: 0.1606 - val_mse: 0.0258\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0740 - mse: 0.0055 - val_loss: 0.1590 - val_mse: 0.0253\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0745 - mse: 0.0056 - val_loss: 0.1535 - val_mse: 0.0236\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0725 - mse: 0.0053 - val_loss: 0.1525 - val_mse: 0.0233\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0725 - mse: 0.0053 - val_loss: 0.1575 - val_mse: 0.0248\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0670 - mse: 0.0045 - val_loss: 0.1641 - val_mse: 0.0269\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0693 - mse: 0.0048 - val_loss: 0.1629 - val_mse: 0.0266\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0679 - mse: 0.0046 - val_loss: 0.1535 - val_mse: 0.0236\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0665 - mse: 0.0044 - val_loss: 0.1486 - val_mse: 0.0221\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0680 - mse: 0.0046 - val_loss: 0.1499 - val_mse: 0.0225\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0672 - mse: 0.0045 - val_loss: 0.1530 - val_mse: 0.0234\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0664 - mse: 0.0044 - val_loss: 0.1517 - val_mse: 0.0230\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0652 - mse: 0.0043 - val_loss: 0.1458 - val_mse: 0.0213\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0637 - mse: 0.0041 - val_loss: 0.1405 - val_mse: 0.0197\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0639 - mse: 0.0041 - val_loss: 0.1405 - val_mse: 0.0198\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0626 - mse: 0.0039 - val_loss: 0.1442 - val_mse: 0.0208\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0606 - mse: 0.0037 - val_loss: 0.1435 - val_mse: 0.0206\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0628 - mse: 0.0039 - val_loss: 0.1400 - val_mse: 0.0196\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0617 - mse: 0.0038 - val_loss: 0.1404 - val_mse: 0.0197\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0609 - mse: 0.0037 - val_loss: 0.1409 - val_mse: 0.0199\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0595 - mse: 0.0035 - val_loss: 0.1392 - val_mse: 0.0194\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0596 - mse: 0.0035 - val_loss: 0.1370 - val_mse: 0.0188\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0616 - mse: 0.0038 - val_loss: 0.1383 - val_mse: 0.0191\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0585 - mse: 0.0034 - val_loss: 0.1392 - val_mse: 0.0194\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0576 - mse: 0.0033 - val_loss: 0.1341 - val_mse: 0.0180\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0587 - mse: 0.0035 - val_loss: 0.1279 - val_mse: 0.0164\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0561 - mse: 0.0032 - val_loss: 0.1240 - val_mse: 0.0154\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0563 - mse: 0.0032 - val_loss: 0.1277 - val_mse: 0.0163\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0583 - mse: 0.0034 - val_loss: 0.1318 - val_mse: 0.0174\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0563 - mse: 0.0032 - val_loss: 0.1271 - val_mse: 0.0162\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0549 - mse: 0.0030 - val_loss: 0.1244 - val_mse: 0.0155\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0562 - mse: 0.0032 - val_loss: 0.1281 - val_mse: 0.0164\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0544 - mse: 0.0030 - val_loss: 0.1222 - val_mse: 0.0149\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0530 - mse: 0.0028 - val_loss: 0.1159 - val_mse: 0.0134\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0527 - mse: 0.0028 - val_loss: 0.1117 - val_mse: 0.0125\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0533 - mse: 0.0028 - val_loss: 0.1125 - val_mse: 0.0126\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0544 - mse: 0.0030 - val_loss: 0.1053 - val_mse: 0.0111\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0525 - mse: 0.0028 - val_loss: 0.1096 - val_mse: 0.0120\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0514 - mse: 0.0026 - val_loss: 0.1157 - val_mse: 0.0134\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0552 - mse: 0.0031 - val_loss: 0.1043 - val_mse: 0.0109\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0513 - mse: 0.0026 - val_loss: 0.1010 - val_mse: 0.0102\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0515 - mse: 0.0026 - val_loss: 0.1075 - val_mse: 0.0116\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0531 - mse: 0.0028 - val_loss: 0.0981 - val_mse: 0.0096\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0524 - mse: 0.0027 - val_loss: 0.0982 - val_mse: 0.0097\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0506 - mse: 0.0026 - val_loss: 0.1128 - val_mse: 0.0127\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0573 - mse: 0.0033 - val_loss: 0.1020 - val_mse: 0.0104\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0466 - mse: 0.0022 - val_loss: 0.0985 - val_mse: 0.0097\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0496 - mse: 0.0025 - val_loss: 0.1051 - val_mse: 0.0110\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0492 - mse: 0.0024 - val_loss: 0.1007 - val_mse: 0.0101\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0498 - mse: 0.0025 - val_loss: 0.0947 - val_mse: 0.0090\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0508 - mse: 0.0026 - val_loss: 0.1002 - val_mse: 0.0100\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0505 - mse: 0.0025 - val_loss: 0.1015 - val_mse: 0.0103\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0508 - mse: 0.0026 - val_loss: 0.0957 - val_mse: 0.0092\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0504 - mse: 0.0025 - val_loss: 0.0986 - val_mse: 0.0097\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0515 - mse: 0.0026 - val_loss: 0.1049 - val_mse: 0.0110\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0476 - mse: 0.0023 - val_loss: 0.0966 - val_mse: 0.0093\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0496 - mse: 0.0025 - val_loss: 0.0956 - val_mse: 0.0091\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0454 - mse: 0.0021 - val_loss: 0.1011 - val_mse: 0.0102\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0496 - mse: 0.0025 - val_loss: 0.0972 - val_mse: 0.0094\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0481 - mse: 0.0023 - val_loss: 0.0943 - val_mse: 0.0089\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0473 - mse: 0.0022 - val_loss: 0.0986 - val_mse: 0.0097\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0467 - mse: 0.0022 - val_loss: 0.0999 - val_mse: 0.0100\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0461 - mse: 0.0021 - val_loss: 0.0949 - val_mse: 0.0090\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0491 - mse: 0.0024 - val_loss: 0.0958 - val_mse: 0.0092\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0465 - mse: 0.0022 - val_loss: 0.0975 - val_mse: 0.0095\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0486 - mse: 0.0024 - val_loss: 0.0945 - val_mse: 0.0089\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0461 - mse: 0.0021 - val_loss: 0.0941 - val_mse: 0.0088\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0462 - mse: 0.0021 - val_loss: 0.0941 - val_mse: 0.0089\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0471 - mse: 0.0022 - val_loss: 0.0943 - val_mse: 0.0089\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0487 - mse: 0.0024 - val_loss: 0.0936 - val_mse: 0.0088\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0459 - mse: 0.0021 - val_loss: 0.0942 - val_mse: 0.0089\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0443 - mse: 0.0020 - val_loss: 0.0960 - val_mse: 0.0092\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0457 - mse: 0.0021 - val_loss: 0.0959 - val_mse: 0.0092\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0454 - mse: 0.0021 - val_loss: 0.0953 - val_mse: 0.0091\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0453 - mse: 0.0021 - val_loss: 0.0943 - val_mse: 0.0089\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0455 - mse: 0.0021 - val_loss: 0.0942 - val_mse: 0.0089\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0477 - mse: 0.0023 - val_loss: 0.0937 - val_mse: 0.0088\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0465 - mse: 0.0022 - val_loss: 0.0936 - val_mse: 0.0088\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0457 - mse: 0.0021 - val_loss: 0.0932 - val_mse: 0.0087\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0433 - mse: 0.0019 - val_loss: 0.0926 - val_mse: 0.0086\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0445 - mse: 0.0020 - val_loss: 0.0930 - val_mse: 0.0087\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0443 - mse: 0.0020 - val_loss: 0.0929 - val_mse: 0.0086\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0435 - mse: 0.0019 - val_loss: 0.0924 - val_mse: 0.0085\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0450 - mse: 0.0020 - val_loss: 0.0931 - val_mse: 0.0087\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0433 - mse: 0.0019 - val_loss: 0.0955 - val_mse: 0.0091\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0471 - mse: 0.0022 - val_loss: 0.0942 - val_mse: 0.0089\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0429 - mse: 0.0018 - val_loss: 0.0944 - val_mse: 0.0089\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0416 - mse: 0.0017 - val_loss: 0.0937 - val_mse: 0.0088\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0438 - mse: 0.0019 - val_loss: 0.0920 - val_mse: 0.0085\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0431 - mse: 0.0019 - val_loss: 0.0917 - val_mse: 0.0084\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0431 - mse: 0.0019 - val_loss: 0.0919 - val_mse: 0.0084\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0432 - mse: 0.0019 - val_loss: 0.0918 - val_mse: 0.0084\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0430 - mse: 0.0019 - val_loss: 0.0923 - val_mse: 0.0085\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0423 - mse: 0.0018 - val_loss: 0.0918 - val_mse: 0.0084\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0448 - mse: 0.0020 - val_loss: 0.0923 - val_mse: 0.0085\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0456 - mse: 0.0021 - val_loss: 0.0922 - val_mse: 0.0085\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0437 - mse: 0.0019 - val_loss: 0.0915 - val_mse: 0.0084\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0439 - mse: 0.0019 - val_loss: 0.0918 - val_mse: 0.0084\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0421 - mse: 0.0018 - val_loss: 0.0916 - val_mse: 0.0084\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0414 - mse: 0.0017 - val_loss: 0.0919 - val_mse: 0.0084\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0435 - mse: 0.0019 - val_loss: 0.0942 - val_mse: 0.0089\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0431 - mse: 0.0019 - val_loss: 0.0922 - val_mse: 0.0085\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0420 - mse: 0.0018 - val_loss: 0.0910 - val_mse: 0.0083\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0441 - mse: 0.0019 - val_loss: 0.0907 - val_mse: 0.0082\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0402 - mse: 0.0016 - val_loss: 0.0899 - val_mse: 0.0081\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0421 - mse: 0.0018 - val_loss: 0.0895 - val_mse: 0.0080\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0418 - mse: 0.0017 - val_loss: 0.0897 - val_mse: 0.0080\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0412 - mse: 0.0017 - val_loss: 0.0900 - val_mse: 0.0081\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0418 - mse: 0.0017 - val_loss: 0.0908 - val_mse: 0.0082\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0411 - mse: 0.0017 - val_loss: 0.0922 - val_mse: 0.0085\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0396 - mse: 0.0016 - val_loss: 0.0909 - val_mse: 0.0083\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0414 - mse: 0.0017 - val_loss: 0.0902 - val_mse: 0.0081\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0413 - mse: 0.0017 - val_loss: 0.0898 - val_mse: 0.0081\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0412 - mse: 0.0017 - val_loss: 0.0886 - val_mse: 0.0078\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0397 - mse: 0.0016 - val_loss: 0.0885 - val_mse: 0.0078\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0394 - mse: 0.0016 - val_loss: 0.0894 - val_mse: 0.0080\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0399 - mse: 0.0016 - val_loss: 0.0894 - val_mse: 0.0080\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0415 - mse: 0.0017 - val_loss: 0.0891 - val_mse: 0.0079\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0401 - mse: 0.0016 - val_loss: 0.0903 - val_mse: 0.0082\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0397 - mse: 0.0016 - val_loss: 0.0886 - val_mse: 0.0078\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0388 - mse: 0.0015 - val_loss: 0.0885 - val_mse: 0.0078\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0406 - mse: 0.0016 - val_loss: 0.0901 - val_mse: 0.0081\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0412 - mse: 0.0017 - val_loss: 0.0894 - val_mse: 0.0080\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0411 - mse: 0.0017 - val_loss: 0.0886 - val_mse: 0.0078\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0405 - mse: 0.0016 - val_loss: 0.0880 - val_mse: 0.0077\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0388 - mse: 0.0015 - val_loss: 0.0890 - val_mse: 0.0079\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0380 - mse: 0.0015 - val_loss: 0.0873 - val_mse: 0.0076\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0426 - mse: 0.0018 - val_loss: 0.0865 - val_mse: 0.0075\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0404 - mse: 0.0016 - val_loss: 0.0888 - val_mse: 0.0079\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0417 - mse: 0.0017 - val_loss: 0.0872 - val_mse: 0.0076\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0398 - mse: 0.0016 - val_loss: 0.0871 - val_mse: 0.0076\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0390 - mse: 0.0015 - val_loss: 0.0883 - val_mse: 0.0078\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0377 - mse: 0.0014 - val_loss: 0.0876 - val_mse: 0.0077\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0406 - mse: 0.0017 - val_loss: 0.0873 - val_mse: 0.0076\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0405 - mse: 0.0016 - val_loss: 0.0879 - val_mse: 0.0077\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0379 - mse: 0.0014 - val_loss: 0.0893 - val_mse: 0.0080\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0382 - mse: 0.0015 - val_loss: 0.0866 - val_mse: 0.0075\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0388 - mse: 0.0015 - val_loss: 0.0860 - val_mse: 0.0074\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0383 - mse: 0.0015 - val_loss: 0.0867 - val_mse: 0.0075\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0383 - mse: 0.0015 - val_loss: 0.0869 - val_mse: 0.0075\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0383 - mse: 0.0015 - val_loss: 0.0866 - val_mse: 0.0075\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0391 - mse: 0.0015 - val_loss: 0.0866 - val_mse: 0.0075\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0366 - mse: 0.0013 - val_loss: 0.0875 - val_mse: 0.0076\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0373 - mse: 0.0014 - val_loss: 0.0858 - val_mse: 0.0074\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0360 - mse: 0.0013 - val_loss: 0.0854 - val_mse: 0.0073\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0384 - mse: 0.0015 - val_loss: 0.0875 - val_mse: 0.0077\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0373 - mse: 0.0014 - val_loss: 0.0855 - val_mse: 0.0073\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0386 - mse: 0.0015 - val_loss: 0.0862 - val_mse: 0.0074\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0386 - mse: 0.0015 - val_loss: 0.0865 - val_mse: 0.0075\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0388 - mse: 0.0015 - val_loss: 0.0857 - val_mse: 0.0073\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0389 - mse: 0.0015 - val_loss: 0.0861 - val_mse: 0.0074\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0386 - mse: 0.0015 - val_loss: 0.0855 - val_mse: 0.0073\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0379 - mse: 0.0014 - val_loss: 0.0857 - val_mse: 0.0073\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0363 - mse: 0.0013 - val_loss: 0.0853 - val_mse: 0.0073\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0390 - mse: 0.0015 - val_loss: 0.0847 - val_mse: 0.0072\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0386 - mse: 0.0015 - val_loss: 0.0847 - val_mse: 0.0072\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0373 - mse: 0.0014 - val_loss: 0.0846 - val_mse: 0.0072\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0369 - mse: 0.0014 - val_loss: 0.0848 - val_mse: 0.0072\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0380 - mse: 0.0014 - val_loss: 0.0850 - val_mse: 0.0072\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0365 - mse: 0.0013 - val_loss: 0.0845 - val_mse: 0.0071\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0372 - mse: 0.0014 - val_loss: 0.0842 - val_mse: 0.0071\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0379 - mse: 0.0014 - val_loss: 0.0842 - val_mse: 0.0071\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0368 - mse: 0.0014 - val_loss: 0.0850 - val_mse: 0.0072\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0363 - mse: 0.0013 - val_loss: 0.0854 - val_mse: 0.0073\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0369 - mse: 0.0014 - val_loss: 0.0858 - val_mse: 0.0074\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0383 - mse: 0.0015 - val_loss: 0.0863 - val_mse: 0.0074\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0383 - mse: 0.0015 - val_loss: 0.0853 - val_mse: 0.0073\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0378 - mse: 0.0014 - val_loss: 0.0847 - val_mse: 0.0072\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0367 - mse: 0.0013 - val_loss: 0.0844 - val_mse: 0.0071\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0381 - mse: 0.0015 - val_loss: 0.0845 - val_mse: 0.0071\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0359 - mse: 0.0013 - val_loss: 0.0855 - val_mse: 0.0073\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0368 - mse: 0.0014 - val_loss: 0.0856 - val_mse: 0.0073\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0374 - mse: 0.0014 - val_loss: 0.0857 - val_mse: 0.0073\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0353 - mse: 0.0012 - val_loss: 0.0855 - val_mse: 0.0073\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0349 - mse: 0.0012 - val_loss: 0.0844 - val_mse: 0.0071\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0368 - mse: 0.0014 - val_loss: 0.0859 - val_mse: 0.0074\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0374 - mse: 0.0014 - val_loss: 0.0839 - val_mse: 0.0070\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0375 - mse: 0.0014 - val_loss: 0.0843 - val_mse: 0.0071\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0347 - mse: 0.0012 - val_loss: 0.0861 - val_mse: 0.0074\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0357 - mse: 0.0013 - val_loss: 0.0849 - val_mse: 0.0072\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape = (x.shape[1],x.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation=\"tanh\"))\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "model.compile(optimizer='adam', loss = root_mean_squared_error, metrics=['mse'])\n",
    "history = model.fit(x, y, epochs=200, batch_size=512, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "086e0f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2507 - mse: 0.0653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25071606040000916, 0.06532151252031326]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026339f4",
   "metadata": {},
   "source": [
    "# 6. 논문과 동일(데이터와 데이터 기간와 변수가 다름, 금 + 유가+ 화페교환비)\n",
    "\n",
    "- news data : nbc 데이터 사용\n",
    "- historical data : senment score, 다우존스, 금, 유가, 화폐교환비(일본, 중국, 유럽, 캐나다, 인도)의 close, open, high, low, adj close 사용\n",
    "- 기간 : 2017년 1월 1일 ~ 2022년 04월 31일 까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "35ec56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"C:/Users/default.DESKTOP-IT64657/Desktop/논문code/csv_data\"\n",
    "\n",
    "score = pd.read_csv(dir+\"/price_data_score.csv\")\n",
    "score.columns = [\"Date\", \"Score\", \"class_3\", \"class_5\"]\n",
    "score = score[[\"Date\",\"class_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "bb0026a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  8 of 8 completed\n"
     ]
    }
   ],
   "source": [
    "price_data = yf.download([\"^DJI\", \"CL=F\",\"GC=F\",\"INR=X\",\"JPY=X\",\"CNY=X\",\"CAD=X\",\"EURUSD=X\"],start = '2017-01-01', end = \"2022-05-01\")\n",
    "price_data.columns = price_data.columns.get_level_values(0)+\"_\"+price_data.columns.get_level_values(1)\n",
    "price_data = price_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "fde41de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  8 of 8 completed\n"
     ]
    }
   ],
   "source": [
    "first_data = yf.download([\"^DJI\", \"CL=F\",\"GC=F\",\"INR=X\",\"JPY=X\",\"CNY=X\",\"CAD=X\",\"EURUSD=X\"],start = '2016-12-31', end = '2017-01-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "70c3e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_data.columns = first_data.columns.get_level_values(0)+\"_\"+first_data.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "6196938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(time):\n",
    "    time = pd.Timestamp(time)\n",
    "    return time\n",
    "\n",
    "score.Date = score.Date.apply(lambda x: time_conversion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "671b589e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close_CAD=X</th>\n",
       "      <th>Adj Close_CL=F</th>\n",
       "      <th>Adj Close_CNY=X</th>\n",
       "      <th>Adj Close_EURUSD=X</th>\n",
       "      <th>Adj Close_GC=F</th>\n",
       "      <th>Adj Close_INR=X</th>\n",
       "      <th>Adj Close_JPY=X</th>\n",
       "      <th>Adj Close_^DJI</th>\n",
       "      <th>Close_CAD=X</th>\n",
       "      <th>Close_CL=F</th>\n",
       "      <th>...</th>\n",
       "      <th>Open_JPY=X</th>\n",
       "      <th>Open_^DJI</th>\n",
       "      <th>Volume_CAD=X</th>\n",
       "      <th>Volume_CL=F</th>\n",
       "      <th>Volume_CNY=X</th>\n",
       "      <th>Volume_EURUSD=X</th>\n",
       "      <th>Volume_GC=F</th>\n",
       "      <th>Volume_INR=X</th>\n",
       "      <th>Volume_JPY=X</th>\n",
       "      <th>Volume_^DJI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>1.34781</td>\n",
       "      <td>53.720001</td>\n",
       "      <td>6.9453</td>\n",
       "      <td>1.057530</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>67.932999</td>\n",
       "      <td>116.266998</td>\n",
       "      <td>19762.599609</td>\n",
       "      <td>1.34781</td>\n",
       "      <td>53.720001</td>\n",
       "      <td>...</td>\n",
       "      <td>116.315002</td>\n",
       "      <td>19833.169922</td>\n",
       "      <td>0</td>\n",
       "      <td>271094.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>271910000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>1.34340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9438</td>\n",
       "      <td>1.052698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.944801</td>\n",
       "      <td>116.794998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>116.809998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close_CAD=X  Adj Close_CL=F  Adj Close_CNY=X  \\\n",
       "Date                                                           \n",
       "2016-12-30          1.34781       53.720001           6.9453   \n",
       "2017-01-02          1.34340             NaN           6.9438   \n",
       "\n",
       "            Adj Close_EURUSD=X  Adj Close_GC=F  Adj Close_INR=X  \\\n",
       "Date                                                              \n",
       "2016-12-30            1.057530          1150.0        67.932999   \n",
       "2017-01-02            1.052698             NaN        67.944801   \n",
       "\n",
       "            Adj Close_JPY=X  Adj Close_^DJI  Close_CAD=X  Close_CL=F  ...  \\\n",
       "Date                                                                  ...   \n",
       "2016-12-30       116.266998    19762.599609      1.34781   53.720001  ...   \n",
       "2017-01-02       116.794998             NaN      1.34340         NaN  ...   \n",
       "\n",
       "            Open_JPY=X     Open_^DJI  Volume_CAD=X  Volume_CL=F  Volume_CNY=X  \\\n",
       "Date                                                                            \n",
       "2016-12-30  116.315002  19833.169922             0     271094.0             0   \n",
       "2017-01-02  116.809998           NaN             0          NaN             0   \n",
       "\n",
       "            Volume_EURUSD=X  Volume_GC=F  Volume_INR=X  Volume_JPY=X  \\\n",
       "Date                                                                   \n",
       "2016-12-30                0         16.0             0             0   \n",
       "2017-01-02                0          NaN             0             0   \n",
       "\n",
       "            Volume_^DJI  \n",
       "Date                     \n",
       "2016-12-30  271910000.0  \n",
       "2017-01-02          NaN  \n",
       "\n",
       "[2 rows x 48 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "baa30030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first(price_data, first_data):\n",
    "    for i in range(price_data.shape[1]):\n",
    "        if np.isnan(first_data.iloc[1][i]):\n",
    "            price_data.iloc[0][i] = first_data.iloc[0][i]\n",
    "        else: \n",
    "            price_data.iloc[0][i] = first_data.iloc[1][i]\n",
    "    \n",
    "    return price_data\n",
    "\n",
    "price_data = pd.merge(price_data, score, how =\"left\",left_on='Date', right_on = \"Date\")[['Adj Close_CAD=X', 'Adj Close_CL=F', 'Adj Close_CNY=X',\n",
    "       'Adj Close_EURUSD=X', 'Adj Close_GC=F', 'Adj Close_INR=X',\n",
    "       'Adj Close_JPY=X', 'Adj Close_^DJI', 'Close_CAD=X', 'Close_CL=F',\n",
    "       'Close_CNY=X', 'Close_EURUSD=X', 'Close_GC=F', 'Close_INR=X',\n",
    "       'Close_JPY=X', 'Close_^DJI', 'High_CAD=X', 'High_CL=F', 'High_CNY=X',\n",
    "       'High_EURUSD=X', 'High_GC=F', 'High_INR=X', 'High_JPY=X', 'High_^DJI',\n",
    "       'Low_CAD=X', 'Low_CL=F', 'Low_CNY=X', 'Low_EURUSD=X', 'Low_GC=F',\n",
    "       'Low_INR=X', 'Low_JPY=X', 'Low_^DJI', 'Open_CAD=X', 'Open_CL=F',\n",
    "       'Open_CNY=X', 'Open_EURUSD=X', 'Open_GC=F', 'Open_INR=X', 'Open_JPY=X',\n",
    "       'Open_^DJI', \"class_3\"]]\n",
    "price_data = price_data.dropna(axis=0)\n",
    "y= price_data[\"Close_^DJI\"].copy()\n",
    "price_data = price_data.reset_index(drop=True)\n",
    "for i in range(len(price_data)-1):\n",
    "    price_data.iloc[len(price_data)-i-1] = price_data.iloc[len(price_data)-i-2]\n",
    "    \n",
    "price_data = first(price_data, first_data)\n",
    "\n",
    "\n",
    "price_data = pd.DataFrame(MinMaxScaler().fit_transform(price_data))\n",
    "price_data.columns = ['Adj_canada', 'Adj_oil', 'Adj_china',\n",
    "       'Adj_europe', 'Adj_gold', 'Adj_india',\n",
    "       'Adj_japan', 'Adj_dj', 'Close_canada', 'Close_oil',\n",
    "       'Close_china', 'Close_europe', 'Close_gold', 'Close_india',\n",
    "       'Close_japan', 'Close_dj', 'High_canada', 'High_oil', 'High_china',\n",
    "       'High_europe', 'High_gold', 'High_india', 'High_japan', 'High_dj',\n",
    "       'Low_canada', 'Low_oil', 'Low_china', 'Low_europe', 'Low_gold',\n",
    "       'Low_india', 'Low_japan', 'Low_dj', 'Open_canada', 'Open_oil',\n",
    "       'Open_china', 'Open_europe', 'Open_gold', 'Open_india', 'Open_japan',\n",
    "       'Open_dj', \"class_3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "e0ae40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "203ae23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(len(price_data)*0.7)\n",
    "\n",
    "train_x = price_data.iloc[0:train_index]\n",
    "train_y = y.iloc[0:train_index]\n",
    "test_x = price_data.iloc[train_index:len(price_data)]\n",
    "test_y = y.iloc[train_index:len(price_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "f75d751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.to_numpy().reshape(len(train_x),1,train_x.shape[1])\n",
    "train_y = train_y.to_numpy().reshape(len(train_y),1)\n",
    "test_x = test_x.to_numpy().reshape(len(test_x),1,test_x.shape[1])\n",
    "test_y = test_y.to_numpy().reshape(len(test_y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "d79bdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c50dd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(shape=(len(train_x)-window_size+1,window_size,train_x.shape[2]))\n",
    "for i in range(len(train_x)-window_size+1):\n",
    "    x[i]=np.vstack((train_x[i:i+window_size]))\n",
    "\n",
    "y = train_y[window_size-1:len(train_y)]\n",
    "\n",
    "x_t = np.zeros(shape=(len(test_x)-window_size+1,window_size,test_x.shape[2]))\n",
    "for i in range(len(test_x)-window_size+1):\n",
    "    x_t[i]=np.vstack((test_x[i:i+window_size]))\n",
    "    \n",
    "y_t = test_y[window_size-1:len(test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "dac8ea7d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 2s 321ms/step - loss: 0.1783 - mse: 0.0318 - val_loss: 0.1385 - val_mse: 0.0192\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1687 - mse: 0.0285 - val_loss: 0.1135 - val_mse: 0.0129\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.1668 - mse: 0.0279 - val_loss: 0.1300 - val_mse: 0.0169\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1496 - mse: 0.0224 - val_loss: 0.1363 - val_mse: 0.0186\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1396 - mse: 0.0195 - val_loss: 0.1156 - val_mse: 0.0134\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1297 - mse: 0.0169 - val_loss: 0.1146 - val_mse: 0.0131\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1202 - mse: 0.0145 - val_loss: 0.1297 - val_mse: 0.0168\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1119 - mse: 0.0125 - val_loss: 0.1518 - val_mse: 0.0230\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1087 - mse: 0.0118 - val_loss: 0.1452 - val_mse: 0.0211\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.1048 - mse: 0.0110 - val_loss: 0.1313 - val_mse: 0.0172\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0952 - mse: 0.0091 - val_loss: 0.1384 - val_mse: 0.0192\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0993 - mse: 0.0099 - val_loss: 0.1497 - val_mse: 0.0224\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0917 - mse: 0.0084 - val_loss: 0.1348 - val_mse: 0.0182\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0817 - mse: 0.0067 - val_loss: 0.1186 - val_mse: 0.0141\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0799 - mse: 0.0064 - val_loss: 0.1189 - val_mse: 0.0141\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0775 - mse: 0.0060 - val_loss: 0.1285 - val_mse: 0.0165\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0745 - mse: 0.0056 - val_loss: 0.1208 - val_mse: 0.0146\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0664 - mse: 0.0044 - val_loss: 0.1072 - val_mse: 0.0115\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0724 - mse: 0.0052 - val_loss: 0.1126 - val_mse: 0.0127\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0663 - mse: 0.0044 - val_loss: 0.1108 - val_mse: 0.0123\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0670 - mse: 0.0045 - val_loss: 0.0875 - val_mse: 0.0076\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0657 - mse: 0.0043 - val_loss: 0.0858 - val_mse: 0.0074\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0598 - mse: 0.0036 - val_loss: 0.0969 - val_mse: 0.0094\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0611 - mse: 0.0037 - val_loss: 0.0775 - val_mse: 0.0060\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0562 - mse: 0.0032 - val_loss: 0.0701 - val_mse: 0.0049\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0584 - mse: 0.0034 - val_loss: 0.0772 - val_mse: 0.0060\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0590 - mse: 0.0035 - val_loss: 0.0675 - val_mse: 0.0046\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0538 - mse: 0.0029 - val_loss: 0.0622 - val_mse: 0.0039\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0530 - mse: 0.0028 - val_loss: 0.0656 - val_mse: 0.0043\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0514 - mse: 0.0026 - val_loss: 0.0607 - val_mse: 0.0037\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0510 - mse: 0.0026 - val_loss: 0.0590 - val_mse: 0.0035\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0476 - mse: 0.0023 - val_loss: 0.0609 - val_mse: 0.0037\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0500 - mse: 0.0025 - val_loss: 0.0588 - val_mse: 0.0035\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0477 - mse: 0.0023 - val_loss: 0.0601 - val_mse: 0.0036\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0477 - mse: 0.0023 - val_loss: 0.0581 - val_mse: 0.0034\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0476 - mse: 0.0023 - val_loss: 0.0578 - val_mse: 0.0033\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0479 - mse: 0.0023 - val_loss: 0.0601 - val_mse: 0.0036\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0463 - mse: 0.0022 - val_loss: 0.0568 - val_mse: 0.0032\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0456 - mse: 0.0021 - val_loss: 0.0598 - val_mse: 0.0036\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0455 - mse: 0.0021 - val_loss: 0.0612 - val_mse: 0.0037\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0428 - mse: 0.0018 - val_loss: 0.0577 - val_mse: 0.0033\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0429 - mse: 0.0018 - val_loss: 0.0613 - val_mse: 0.0038\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0446 - mse: 0.0020 - val_loss: 0.0584 - val_mse: 0.0034\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0428 - mse: 0.0018 - val_loss: 0.0583 - val_mse: 0.0034\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0425 - mse: 0.0018 - val_loss: 0.0635 - val_mse: 0.0040\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0421 - mse: 0.0018 - val_loss: 0.0573 - val_mse: 0.0033\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0420 - mse: 0.0018 - val_loss: 0.0609 - val_mse: 0.0037\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0415 - mse: 0.0017 - val_loss: 0.0617 - val_mse: 0.0038\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0392 - mse: 0.0015 - val_loss: 0.0584 - val_mse: 0.0034\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0421 - mse: 0.0018 - val_loss: 0.0588 - val_mse: 0.0035\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0395 - mse: 0.0016 - val_loss: 0.0638 - val_mse: 0.0041\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0407 - mse: 0.0017 - val_loss: 0.0582 - val_mse: 0.0034\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0388 - mse: 0.0015 - val_loss: 0.0617 - val_mse: 0.0038\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0397 - mse: 0.0016 - val_loss: 0.0616 - val_mse: 0.0038\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0396 - mse: 0.0016 - val_loss: 0.0563 - val_mse: 0.0032\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0398 - mse: 0.0016 - val_loss: 0.0596 - val_mse: 0.0035\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0375 - mse: 0.0014 - val_loss: 0.0598 - val_mse: 0.0036\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0385 - mse: 0.0015 - val_loss: 0.0574 - val_mse: 0.0033\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0390 - mse: 0.0015 - val_loss: 0.0587 - val_mse: 0.0034\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0376 - mse: 0.0014 - val_loss: 0.0566 - val_mse: 0.0032\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0354 - mse: 0.0013 - val_loss: 0.0549 - val_mse: 0.0030\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0394 - mse: 0.0016 - val_loss: 0.0615 - val_mse: 0.0038\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0403 - mse: 0.0016 - val_loss: 0.0548 - val_mse: 0.0030\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0381 - mse: 0.0014 - val_loss: 0.0591 - val_mse: 0.0035\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0368 - mse: 0.0014 - val_loss: 0.0591 - val_mse: 0.0035\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0370 - mse: 0.0014 - val_loss: 0.0559 - val_mse: 0.0031\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0358 - mse: 0.0013 - val_loss: 0.0626 - val_mse: 0.0039\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0358 - mse: 0.0013 - val_loss: 0.0603 - val_mse: 0.0036\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0350 - mse: 0.0012 - val_loss: 0.0609 - val_mse: 0.0037\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0350 - mse: 0.0012 - val_loss: 0.0638 - val_mse: 0.0041\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0356 - mse: 0.0013 - val_loss: 0.0614 - val_mse: 0.0038\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0357 - mse: 0.0013 - val_loss: 0.0601 - val_mse: 0.0036\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0356 - mse: 0.0013 - val_loss: 0.0669 - val_mse: 0.0045\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0375 - mse: 0.0014 - val_loss: 0.0609 - val_mse: 0.0037\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0358 - mse: 0.0013 - val_loss: 0.0574 - val_mse: 0.0033\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0358 - mse: 0.0013 - val_loss: 0.0694 - val_mse: 0.0048\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0376 - mse: 0.0014 - val_loss: 0.0591 - val_mse: 0.0035\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0355 - mse: 0.0013 - val_loss: 0.0570 - val_mse: 0.0032\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0362 - mse: 0.0013 - val_loss: 0.0633 - val_mse: 0.0040\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0333 - mse: 0.0011 - val_loss: 0.0582 - val_mse: 0.0034\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0327 - mse: 0.0011 - val_loss: 0.0590 - val_mse: 0.0035\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0351 - mse: 0.0012 - val_loss: 0.0664 - val_mse: 0.0044\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0354 - mse: 0.0013 - val_loss: 0.0571 - val_mse: 0.0033\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0336 - mse: 0.0011 - val_loss: 0.0568 - val_mse: 0.0032\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0353 - mse: 0.0012 - val_loss: 0.0620 - val_mse: 0.0038\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0336 - mse: 0.0011 - val_loss: 0.0562 - val_mse: 0.0032\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0344 - mse: 0.0012 - val_loss: 0.0637 - val_mse: 0.0041\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0354 - mse: 0.0013 - val_loss: 0.0626 - val_mse: 0.0039\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0344 - mse: 0.0012 - val_loss: 0.0571 - val_mse: 0.0033\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0337 - mse: 0.0011 - val_loss: 0.0649 - val_mse: 0.0042\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0352 - mse: 0.0012 - val_loss: 0.0571 - val_mse: 0.0033\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0332 - mse: 0.0011 - val_loss: 0.0578 - val_mse: 0.0033\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0326 - mse: 0.0011 - val_loss: 0.0639 - val_mse: 0.0041\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0350 - mse: 0.0012 - val_loss: 0.0573 - val_mse: 0.0033\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0336 - mse: 0.0011 - val_loss: 0.0581 - val_mse: 0.0034\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0330 - mse: 0.0011 - val_loss: 0.0653 - val_mse: 0.0043\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0341 - mse: 0.0012 - val_loss: 0.0590 - val_mse: 0.0035\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0314 - mse: 9.8788e-04 - val_loss: 0.0591 - val_mse: 0.0035\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0328 - mse: 0.0011 - val_loss: 0.0625 - val_mse: 0.0039\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0316 - mse: 0.0010 - val_loss: 0.0629 - val_mse: 0.0040\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0334 - mse: 0.0011 - val_loss: 0.0623 - val_mse: 0.0039\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0321 - mse: 0.0010 - val_loss: 0.0621 - val_mse: 0.0039\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0337 - mse: 0.0011 - val_loss: 0.0596 - val_mse: 0.0036\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0331 - mse: 0.0011 - val_loss: 0.0602 - val_mse: 0.0036\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0330 - mse: 0.0011 - val_loss: 0.0575 - val_mse: 0.0033\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0318 - mse: 0.0010 - val_loss: 0.0607 - val_mse: 0.0037\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0331 - mse: 0.0011 - val_loss: 0.0620 - val_mse: 0.0038\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0316 - mse: 0.0010 - val_loss: 0.0608 - val_mse: 0.0037\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0320 - mse: 0.0010 - val_loss: 0.0627 - val_mse: 0.0039\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0326 - mse: 0.0011 - val_loss: 0.0625 - val_mse: 0.0039\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0300 - mse: 9.0009e-04 - val_loss: 0.0611 - val_mse: 0.0037\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0317 - mse: 0.0010 - val_loss: 0.0630 - val_mse: 0.0040\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0316 - mse: 0.0010 - val_loss: 0.0627 - val_mse: 0.0039\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0331 - mse: 0.0011 - val_loss: 0.0574 - val_mse: 0.0033\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0311 - mse: 9.6801e-04 - val_loss: 0.0629 - val_mse: 0.0040\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0306 - mse: 9.3627e-04 - val_loss: 0.0629 - val_mse: 0.0040\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0318 - mse: 0.0010 - val_loss: 0.0579 - val_mse: 0.0034\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0321 - mse: 0.0010 - val_loss: 0.0621 - val_mse: 0.0039\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0300 - mse: 9.0563e-04 - val_loss: 0.0596 - val_mse: 0.0036\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0306 - mse: 9.3471e-04 - val_loss: 0.0584 - val_mse: 0.0034\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0308 - mse: 9.4929e-04 - val_loss: 0.0617 - val_mse: 0.0038\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0308 - mse: 9.5180e-04 - val_loss: 0.0591 - val_mse: 0.0035\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0310 - mse: 9.6559e-04 - val_loss: 0.0594 - val_mse: 0.0035\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0300 - mse: 9.0248e-04 - val_loss: 0.0619 - val_mse: 0.0038\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0304 - mse: 9.2570e-04 - val_loss: 0.0563 - val_mse: 0.0032\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0311 - mse: 9.6802e-04 - val_loss: 0.0596 - val_mse: 0.0036\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0306 - mse: 9.4108e-04 - val_loss: 0.0637 - val_mse: 0.0041\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0310 - mse: 9.6454e-04 - val_loss: 0.0555 - val_mse: 0.0031\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0303 - mse: 9.2084e-04 - val_loss: 0.0662 - val_mse: 0.0044\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0304 - mse: 9.2374e-04 - val_loss: 0.0588 - val_mse: 0.0035\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0302 - mse: 9.1175e-04 - val_loss: 0.0600 - val_mse: 0.0036\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0297 - mse: 8.8170e-04 - val_loss: 0.0652 - val_mse: 0.0043\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0307 - mse: 9.4577e-04 - val_loss: 0.0580 - val_mse: 0.0034\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0310 - mse: 9.6375e-04 - val_loss: 0.0605 - val_mse: 0.0037\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0304 - mse: 9.2248e-04 - val_loss: 0.0580 - val_mse: 0.0034\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0290 - mse: 8.4249e-04 - val_loss: 0.0585 - val_mse: 0.0034\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0296 - mse: 8.7796e-04 - val_loss: 0.0587 - val_mse: 0.0034\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0304 - mse: 9.2247e-04 - val_loss: 0.0589 - val_mse: 0.0035\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0298 - mse: 8.9494e-04 - val_loss: 0.0585 - val_mse: 0.0034\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0302 - mse: 9.1309e-04 - val_loss: 0.0630 - val_mse: 0.0040\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0307 - mse: 9.4182e-04 - val_loss: 0.0619 - val_mse: 0.0038\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0303 - mse: 9.2139e-04 - val_loss: 0.0596 - val_mse: 0.0036\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0280 - mse: 7.8317e-04 - val_loss: 0.0658 - val_mse: 0.0043\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0290 - mse: 8.4172e-04 - val_loss: 0.0620 - val_mse: 0.0038\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0291 - mse: 8.4506e-04 - val_loss: 0.0614 - val_mse: 0.0038\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0297 - mse: 8.8362e-04 - val_loss: 0.0646 - val_mse: 0.0042\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0284 - mse: 8.1068e-04 - val_loss: 0.0613 - val_mse: 0.0038\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0302 - mse: 9.1863e-04 - val_loss: 0.0629 - val_mse: 0.0040\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0262 - mse: 6.8820e-04 - val_loss: 0.0650 - val_mse: 0.0042\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0276 - mse: 7.6104e-04 - val_loss: 0.0639 - val_mse: 0.0041\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0297 - mse: 8.8012e-04 - val_loss: 0.0655 - val_mse: 0.0043\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0281 - mse: 7.8879e-04 - val_loss: 0.0707 - val_mse: 0.0050\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0278 - mse: 7.7257e-04 - val_loss: 0.0629 - val_mse: 0.0040\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0286 - mse: 8.1621e-04 - val_loss: 0.0676 - val_mse: 0.0046\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0282 - mse: 7.9559e-04 - val_loss: 0.0657 - val_mse: 0.0043\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0281 - mse: 7.9146e-04 - val_loss: 0.0602 - val_mse: 0.0036\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0294 - mse: 8.6282e-04 - val_loss: 0.0634 - val_mse: 0.0040\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0304 - mse: 9.2356e-04 - val_loss: 0.0633 - val_mse: 0.0040\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0289 - mse: 8.3497e-04 - val_loss: 0.0608 - val_mse: 0.0037\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0281 - mse: 7.9258e-04 - val_loss: 0.0673 - val_mse: 0.0045\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0284 - mse: 8.0842e-04 - val_loss: 0.0634 - val_mse: 0.0040\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0289 - mse: 8.3731e-04 - val_loss: 0.0598 - val_mse: 0.0036\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0287 - mse: 8.2629e-04 - val_loss: 0.0678 - val_mse: 0.0046\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0282 - mse: 7.9312e-04 - val_loss: 0.0595 - val_mse: 0.0035\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0275 - mse: 7.6026e-04 - val_loss: 0.0628 - val_mse: 0.0039\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0284 - mse: 8.0933e-04 - val_loss: 0.0649 - val_mse: 0.0042\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0292 - mse: 8.5799e-04 - val_loss: 0.0564 - val_mse: 0.0032\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0303 - mse: 9.2038e-04 - val_loss: 0.0627 - val_mse: 0.0039\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0278 - mse: 7.7189e-04 - val_loss: 0.0594 - val_mse: 0.0035\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0272 - mse: 7.4095e-04 - val_loss: 0.0578 - val_mse: 0.0033\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0266 - mse: 7.0854e-04 - val_loss: 0.0678 - val_mse: 0.0046\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0291 - mse: 8.4833e-04 - val_loss: 0.0545 - val_mse: 0.0030\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0292 - mse: 8.5432e-04 - val_loss: 0.0568 - val_mse: 0.0032\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0269 - mse: 7.2181e-04 - val_loss: 0.0636 - val_mse: 0.0040\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0282 - mse: 7.9748e-04 - val_loss: 0.0536 - val_mse: 0.0029\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0301 - mse: 9.0656e-04 - val_loss: 0.0618 - val_mse: 0.0038\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0283 - mse: 8.0135e-04 - val_loss: 0.0596 - val_mse: 0.0036\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0275 - mse: 7.5716e-04 - val_loss: 0.0546 - val_mse: 0.0030\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0287 - mse: 8.2323e-04 - val_loss: 0.0630 - val_mse: 0.0040\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0274 - mse: 7.5218e-04 - val_loss: 0.0579 - val_mse: 0.0033\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0272 - mse: 7.4253e-04 - val_loss: 0.0591 - val_mse: 0.0035\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0265 - mse: 7.0409e-04 - val_loss: 0.0613 - val_mse: 0.0038\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0275 - mse: 7.5516e-04 - val_loss: 0.0567 - val_mse: 0.0032\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0265 - mse: 7.0189e-04 - val_loss: 0.0612 - val_mse: 0.0037\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0274 - mse: 7.4893e-04 - val_loss: 0.0638 - val_mse: 0.0041\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0285 - mse: 8.1598e-04 - val_loss: 0.0593 - val_mse: 0.0035\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0264 - mse: 6.9645e-04 - val_loss: 0.0662 - val_mse: 0.0044\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0293 - mse: 8.5699e-04 - val_loss: 0.0579 - val_mse: 0.0034\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0273 - mse: 7.4681e-04 - val_loss: 0.0583 - val_mse: 0.0034\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0281 - mse: 7.9176e-04 - val_loss: 0.0636 - val_mse: 0.0041\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0290 - mse: 8.4015e-04 - val_loss: 0.0600 - val_mse: 0.0036\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0261 - mse: 6.8068e-04 - val_loss: 0.0593 - val_mse: 0.0035\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0255 - mse: 6.4867e-04 - val_loss: 0.0600 - val_mse: 0.0036\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0265 - mse: 7.0299e-04 - val_loss: 0.0586 - val_mse: 0.0034\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0272 - mse: 7.4251e-04 - val_loss: 0.0620 - val_mse: 0.0038\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0270 - mse: 7.2918e-04 - val_loss: 0.0596 - val_mse: 0.0036\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0259 - mse: 6.7246e-04 - val_loss: 0.0602 - val_mse: 0.0036\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0257 - mse: 6.5902e-04 - val_loss: 0.0638 - val_mse: 0.0041\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0258 - mse: 6.6900e-04 - val_loss: 0.0578 - val_mse: 0.0033\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0257 - mse: 6.6036e-04 - val_loss: 0.0639 - val_mse: 0.0041\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape = (x.shape[1],x.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation=\"tanh\"))\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "model.compile(optimizer='adam', loss = root_mean_squared_error, metrics=['mse'])\n",
    "history = model.fit(x, y, epochs=200, batch_size=512, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "23ca05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1713 - mse: 0.0308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17127077281475067, 0.030820565298199654]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_t, y_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
