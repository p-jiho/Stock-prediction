{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cdc9262-1ea0-4713-beae-b732898a1852",
   "metadata": {},
   "source": [
    "1. 왜 딥러닝을 먼저 하는지?\n",
    "2. 왜 통계학에서 자주 사용하던 모형은 왜 배제되었나? 통계 모형의 장단점은 무엇인가?\n",
    "3. 선진 연구자는 어떤 통계 모형과 비교?\n",
    "4. 재현성 문제\n",
    "\n",
    "5. 다른 연구자와 다른점? 본문도 같이 사용, 데이터 양이 많다\n",
    "   제목만 사용한 결과와 비교, 10년전 것과만 비교?\n",
    "   \n",
    "6. 크롤링 방법들의 장단점 비교\n",
    "7. raw 데이터를 수집하는 문제\n",
    "8. 하루에 뉴스가 두 개 이상인데 어떤 기준으로 하나를 택하는가? **************\n",
    "\n",
    "folder 이름은 숫자 상수 취급"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ff0e4-bac5-46a0-bf39-6817b2f1443c",
   "metadata": {},
   "source": [
    "- Predicting stock market behavior using data mining technique and news sentiment analysis\n",
    "    - Ayman E.Khedr, S.E.Salama, Nagwa Yaseen, 2017년 논문\n",
    "    - sentiment analysis, 감성사전이 아닌 Naivs Bayes, K-NN 사용\n",
    "    - TF-Idf 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c87094b-02d9-41c3-a833-3aaeb8642e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import smart_open\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72aa1cc-aeb1-4279-b32a-136730bc460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/home/whfhrs3260/csv_data/'\n",
    "original_news_dir = dir + \"original_data.pkl\"\n",
    "tit_txt_dir = dir + \"tit_txt_combination_4years.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ad0272-28e7-4c96-bf35-4933e716f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_news = pd.read_pickle(original_news_dir)\n",
    "tit_txt_combination = pd.read_pickle(tit_txt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908aa4d1-b55b-42b5-9582-1df761e4576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_news.date = original_news.date.apply(lambda x: pd.to_datetime(x, errors=\"ignore\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e287d617-d1fd-4908-81dd-0e0d3582d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시간을 날짜 단위로 짜름\n",
    "def date_until_day(date_timestamp):  ## 2021-01-01 00:23:00 => 2021-01-01\n",
    "    date_timestamp = datetime.datetime.strftime(date_timestamp, \"%Y-%m-%d\")  \n",
    "    return date_timestamp\n",
    "\n",
    "\n",
    "news_day_standard = copy.deepcopy(original_news)     ## original data를 value만 copy\n",
    "\n",
    "news_day_standard.date = news_day_standard.date.apply(lambda x: date_until_day(x))  ## 시간을 시 단위로 짜름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4080ef3b-dd6d-4629-8f7b-d441de316e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NaN -> None으로 변경(데이터 처리를 편리하게 하기 위해)\n",
    "tit_txt_combination = tit_txt_combination.where(pd.notnull(tit_txt_combination), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c246f13b-ff13-449b-b537-350031c13142",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터에 date 변수 붙여주기\n",
    "tit_txt_date_day = pd.concat([news_day_standard.date,tit_txt_combination],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c04dab-5319-44b5-a4a1-d9296e908604",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시간 순으로 정렬\n",
    "tit_txt_date_day = tit_txt_date_day.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554d1924-a49a-4c9b-a5fd-2a7a533c112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 새로 정렬된 데이터의 인덱스를 순서대로 재설정  # index 100 51 21 40 ... => 1 2 3 4 5 ...\n",
    "tit_txt_date_day = tit_txt_date_day.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00c53d5e-4ccd-4034-a761-2e21b78873be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 list로 변환\n",
    "tit_txt_date_day = tit_txt_date_day.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e225f6c0-c24f-4b4b-8d12-f22c795e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 프레임을 list로 변환하는 과정에서 포함된 None을 삭제\n",
    "def del_list_None(lst):\n",
    "    lst = list(filter(None, lst))\n",
    "    return lst\n",
    "\n",
    "tit_txt_date_day = list(map(del_list_None, tit_txt_date_day ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4da34023-7c6f-482a-9a68-5383a1b84591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 시간인 경우 데이터를 합침 ex) 1월 1일 10시~10시 59분 사이의 기사는 전부 합침\n",
    "\n",
    "## 알고리즘 설명\n",
    "## basic_date 즉, 기준이 되는 date가 1월 1일 10시라고 가정\n",
    "## 현재 loop의 date가 1월 1일 10시라면 원래 있던 1월 1일 10시 데이터와 새로운 데이터를 결합\n",
    "## 현재 loop의 date가 1월 1일 10시가 아닌 1월 1일 11시로 새로운 date가 출현했다면 basic_date 즉, 기준 데이터를 1월 1일 11시로 변경하고 새로운 리스트를 생성\n",
    "## 이를 반복해 같은 시간인 뉴스끼리 결합되고 다른 시간인 경우는 다른 list로 분리\n",
    "## 결국, 1시간 당 1개의 리스트만 생성\n",
    "        \n",
    "# 같은 날짜인 경우 데이터를 합침 ex) 1월 1일 00시 00분 ~ 23시 59분 사이의 기사는 전부 합침\n",
    "length = -1\n",
    "tit_txt_combination_date_day = []\n",
    "\n",
    "basic_date = 0             ## 기준이 되는 date\n",
    "for i in range(len(tit_txt_date_day)):  ## 전체를 한번씩 돈다\n",
    "    new_date = tit_txt_date_day[i][0]    # new date는 현재 loop의 date\n",
    "    if basic_date == new_date:   # 현재 loop의 date가 기준 date와 같으면 실행\n",
    "        tit_txt_combination_date_day[length] = tit_txt_combination_date_day[length]+tit_txt_date_day[i][1:(len(tit_txt_date_day[i]))] ## 앞의 데이터에 새로운 데이터를 결합\n",
    "    else:                        # 현재 loop의 date가 기준 date와 다르면 실행 즉, 새로운 시간이 나타나면 실행\n",
    "        length += 1             # 길이가 한개 늘어남\n",
    "        tit_txt_combination_date_day.append(tit_txt_date_day[i][1:(len(tit_txt_date_day[i]))]) # 새로운 list 데이터 추가\n",
    "        basic_date = tit_txt_date_day[i][0]              # 기준 date를 새로운 date로 변경\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "992adcc5-d6ca-4c8a-a587-94968245a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(tit_txt_date_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "477121ef-36a4-4496-a9ee-494f8b7c9cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:21.758020\n"
     ]
    }
   ],
   "source": [
    "## -------------------------------------- 1. Bag of Words 생성 ---------------\n",
    "## BOW를 만드는 TfidfVectorizer 함수를 사용하기 위해서는 토큰화가 되지 않은 문장 데이터가 필요\n",
    "## => 토큰화가 되어있는 tit, txt 데이터를 한 문장으로 만들어 list로 구성\n",
    "## Bag of Words : 한 문서에 있는 단어들의 집합, 없는 단어면 0, 있는 단어면 갯수만큼 n => ex) 0 0 0 1 0 2 3 0 2\n",
    "## tf-idf : 단어의 빈도와 역 문서 빈도를 활용하여 중요한 단어에 가중치를 두는 방식\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "## 함수에 적용할 데이터 구조 수정\n",
    "def word_to_sentence(lst):  ## 토큰화가 되어있는 tit, tit 데이터를 한 문장으로 만듦\n",
    "    lst = \" \".join(lst)     ## ex) the, and, me, bye => the and me bye\n",
    "    return lst\n",
    "\n",
    "## 각 문장마다 수행, 결과 ex) [\"the and me bye\", ...,\"i do not me\"]\n",
    "tit_txt_sentence_day = list(map(word_to_sentence, tit_txt_combination_date_day))\n",
    "\n",
    "\n",
    "## BOW를 만듦 + tf-idf 적용\n",
    "bow_tfidf_tit_txt_day = TfidfVectorizer().fit(tit_txt_sentence_day)\n",
    "\n",
    "\n",
    "\n",
    "## BOW에서 각 요소의 이름 추출\n",
    "bow_tfidf_vocab_tit_txt_day = bow_tfidf_tit_txt_day.get_feature_names_out()\n",
    "\n",
    "\n",
    "# DataFrame 형식으로 BOW + tf-idf 데이터 구성\n",
    "bow_tfidf_tit_txt_df_day = pd.DataFrame(bow_tfidf_tit_txt_day.transform(tit_txt_sentence_day).toarray(), columns = bow_tfidf_vocab_tit_txt_day) \n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a6eb369-c28f-4880-9130-8aa157771e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------- 2. Word2Vec 생성 ---------------\n",
    "# Word2Vec : 단어를 벡터로 나타냄\n",
    "w2v_tit_txt_day = Word2Vec(sentences=tit_txt_combination_date_day, vector_size=100, window=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "527a2f9a-33a8-4ba4-90a7-eb74facfd777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:36.566237\n"
     ]
    }
   ],
   "source": [
    "## -------------------------------------- 3. FastText 생성 ---------------\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "fasttext_tit_txt_day = FastText(tit_txt_combination_date_day, vector_size=100, window=5,  workers=4, sg=1)\n",
    "\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c73000c6-2278-4970-8126-a01666995e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:10.251445\n"
     ]
    }
   ],
   "source": [
    "## -------------------------------------- 4. BERT 생성 ---------------\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "bert_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "bert_tit_txt_day= bert_model.encode(tit_txt_sentence_day)\n",
    "\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d09b76f-ddfc-49b7-a69b-ed1e65aadbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 231508/231508 [00:00<00:00, \n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_text = tokenizer.tokenize(original_news.title_text_bert.iloc[0])\n",
    "indexed_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "id = 0\n",
    "segment_ids = [0]*len(tokenized_text)\n",
    "for i in range(len(tokenized_text)):\n",
    "    if tokenized_text[i]!=\"[SEP]\":\n",
    "        segment_ids[i] = id\n",
    "    else:\n",
    "        segment_ids[i] = id\n",
    "        id+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9f237-7e1e-4044-90ba-5bd95d488abe",
   "metadata": {},
   "source": [
    "- sentiment 분석 고려\n",
    "- bert에서 사전학습된 가중치가 아닌 내가 학습시킬 수 있는 방법 고안\n",
    "- 공휴일, 주말 처리방법 고안"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
