{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a1953c-7764-448a-9c0e-9ab43511762f",
   "metadata": {},
   "source": [
    "#### 1. 크롤링을 사용한 이유는?\n",
    "\n",
    "A. news를 수집할 수 있는 패키지가 있지만 패키지안에 있는 news 데이터에 국한되지않고, 대표 신문사 중 하나인 NBC에서 데이터를 수집함으로써 주관적인 요소가 포함되지 않고, 다양한 분야의 많은 데이터를 수집할 수 있기 때문에 크롤링을 사용했다.\n",
    "\n",
    "#### 2. javascript를 사용한 이유는?\n",
    "\n",
    "A. 파이썬으로도 쉽게 크롤링이 가능하지만, 매일 data를 자동으로 쉽게 크롤링 할 수 있다는 장점에서 javascript로 크롤링을 진행하였다.\n",
    "\n",
    "#### 3. 왜 기간은 5년으로 지정했는가?\n",
    "\n",
    "A. 10년인 경우도 있고 6개월인 경우도 있다. 기간이 짧은 경우는 계절, 시간의 흐름을 충분히 담기엔 한계가 있다고 생각한다. 또한, 코로나 이전 상황과 이후 상황도 있으므로 기간은 코로나 이후로만 잡는 것은 안좋을 것으로 생각한다. 그러므로 5년 이상은 되어야 꽤 긴 기간동안 다양한 계절과 다양한 일들에 대한 정보를 포함하고 있을 수 있다. 또한 기간이 너무 길면 현재로는 의미없는 정보들도 포함이 되어있어 불필요할 것이라 생각하여 5년이 적정하다고 생각하였다.\n",
    "기존의 논문들은 기간이 짧거나 하루 뉴스 양이 상당히 작은 부분이 문제라고 생각하였으므로 기간을 길게 하고 뉴스의 양을 많이, 카테고리를 다양하게 수집한 것이 차별점이다.\n",
    "- Combining Time Series and Sentiment analysis for Stock Market Forecasting, 2021 : 뉴스의 양이 적은 논문\n",
    "\n",
    "#### 4. NBC 사이트로 정한 이유는?\n",
    "\n",
    "A. 긴 기간동안의 데이터를 다양한 카테고리의 다양한 뉴스를 많이 추출할 수 있기 때문이다. 그리고 NBC 사이트는 미국에서 꽤 유명한 뉴스 방송사이므로 많은 사람들이 뉴스를 보는 방송사이다. 이로인해 신뢰도도 확보가 되었으므로 NBC 사이트를 사용한다.\n",
    "\n",
    "#### 5. 날짜가 없는 데이터는 삭제한 이유는?\n",
    "\n",
    "A. 뉴스가 주가에 영향을 주는 부분을 확인하기 위해서 날짜는 중요한 변수이다. 날짜가 없다면 데이터로 사용을 할 수 없다.\n",
    "\n",
    "#### 6. 날짜가 없는 데이터들은 왜 생기는 것인가?\n",
    "\n",
    "A. NBC는 지난 뉴스들을 한 부분에 모아 기록해둔다.\n",
    "그 당시에는 뉴스가 생성이 되어서 기사의 제목은 현재까지 기록으로 남아있지만 현재는 기사가 사라진 경우 또는 동영상 기사인 경우이다.\n",
    "추가적으로 동영상 기사인 경우, 데이터로 사용할 뉴스 본문이 없으므로 사용할 수 없다.\n",
    "\n",
    "#### 7. 기사의 최초 작성일만 사용한 이유는?\n",
    "\n",
    "A. 최초로 기사가 떴을 때 가장 영향을 많이 받았을 것이다. 또한 기사의 전체 내용이나 흐름이 바뀌지는 않았을 것이다. 그러므로 처음 떴을 때를 기준으로 사용하는 것이 좋다고 판단한다.\n",
    "\n",
    "#### 8. 빈도수를 기반해 삭제한 이유는?\n",
    "\n",
    "A. 수많은 뉴스 중 빈도수가 5개 이하인 단어의 경우는 이 단어가 데이터를 분석하는데 큰 의미가 없다고 판별한다. 불필요한 데이터를 줄이는 것이 맞다.\n",
    "\n",
    "#### 9. title 데이터와 text 데이터를 합친 이유는?\n",
    "\n",
    "A. 딥러닝의 input 데이터를 만들기 위해 필요한 과정이다. 또한, title과 text가 크게 다른 맥락이라고 볼 수 없다. 그러므로 같은 맥락과 같은 의미를 가진 두 가지를 합쳐 한 벡터로 생성해 분석하는 것은 합당한 일이라고 판단한다. \n",
    "\n",
    "#### 10. 왜 title데이터와 text 데이터를 사용하는 것인가?\n",
    "\n",
    "A. 기존의 논문들에는 title 데이터만 사용하는 경우가 많았다. 본문을 사용하여 기존 논문들과의 차별화를 두었다.\n",
    "- Combining Time Series and Sentiment analysis for Stock Market Forecasting, 2021   : title만 사용한 논문\n",
    "- Stock prediction using combination of BERT sentiment Analysis and macro economy index : headline만 사용한 점을 개선점으로 언급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9cf712-0c9b-4a40-a990-decb913d9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 20:28:37.179281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 20:28:37.348317: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-24 20:28:37.355652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-24 20:28:37.355673: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-24 20:28:37.397617: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-24 20:28:38.213872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-24 20:28:38.213951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-24 20:28:38.213961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "    \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f50737-0b7a-4e3c-b8f8-0241bd12b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9f5b7f-2a38-46a6-9b06-46f1aad0459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = list(map(str, list(range(2017,2023))))\n",
    "month = list(map(str, list(range(1,13))))\n",
    "\n",
    "for i in range(0,9):\n",
    "    month[i] = \"0\"+month[i]\n",
    "\n",
    "year_month = [0]*(12*5+4)\n",
    "\n",
    "k=0\n",
    "for i in range(len(year)):\n",
    "    for j in range(len(month)):\n",
    "        if k < len(year_month):\n",
    "            year_month[k] = year[i]+\"_\"+month[j]\n",
    "            k = k+1\n",
    "        else:\n",
    "            k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ae2e42-efa4-4968-9fca-314f74955971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:38.068907\n"
     ]
    }
   ],
   "source": [
    "## 소문자 변환 - tit, txt에 적용(text, title을 토큰화한 리스트)\n",
    "def upper_to_lower_line(line):  ## 한 리스트의 한 문장씩 불러와서 upper_to_lower_word 적용\n",
    "    line = list(map(upper_to_lower_word,line))\n",
    "    return line\n",
    "\n",
    "def upper_to_lower_word(word):  ## string 형식, 한 문장의 한 단어씩 불러와서 대문자 변환   The -> the, THE -> THE\n",
    "    word = re.sub(\"^[A-Z]{1}[a-z]*$\",word.lower(),word)  ## 정규표현식에 맞는 단어만 lower 적용\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 표제어 추출 - tit, txt에 적용\n",
    "def lemmatization_line(line):   ## 한 리스트의 한 문장씩 불러와서 lemmatization_line 적용\n",
    "    line = list(map(lemmatization_word, line))\n",
    "    return line\n",
    "\n",
    "def lemmatization_word(data):  ## string 형식, 한 문장의 한 단어씩 불러와서 표제어 추출\n",
    "    n=WordNetLemmatizer()\n",
    "    data = n.lemmatize(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 불용어 제거 - tit, txt에 적용\n",
    "def del_stopword(line):              ## 한 리스트의 한 문장씩 불러와서 dir_stopword_produce 적용\n",
    "    dir_stop_words = stopwords.words('english')  ## 불용어 사전\n",
    "    \n",
    "    line_stopwords_intersection = list(set(line)& set(dir_stop_words))   ## 각 문장과 불용어 사전에 동시에 있는 단어 추출\n",
    "    \n",
    "    # 각 문장마다 불용어 사전과 교집합인 사전 생성\n",
    "    \n",
    "    # 각 문장마다 교집합 사전에 해당하지 않는 값만 추출\n",
    "    line = difference(line, line_stopwords_intersection)  \n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def difference(line, line_stopwords_intersection):      ## 각 문장, 각 문장과 불용어 사전의 교집합 입력\n",
    "    line = [i for i in line if i not in line_stopwords_intersection]  ## 불용어 사전에 해당하지 않는 단어만 추출\n",
    "    return line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dir = \"/home/whfhrs3260/data/\"\n",
    "\n",
    "def loading(data):\n",
    "\n",
    "    ## ----------------------데이터 파일 불러오기--------------------------------------------------\n",
    "    f = open(dir+str(data)+\".txt\",\"r\", encoding = \"UTF-8\")\n",
    "    datas = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------데이터 저장--------------------------------------------------\n",
    "    n =len(datas)\n",
    "    m=0\n",
    "    original_data = [0]*n\n",
    "    for line in datas:\n",
    "        original_data[m] = line.strip()   ## 한줄씩 읽기\n",
    "        m+=1\n",
    "        \n",
    "    for k in range(n):\n",
    "        original_data[k] = original_data[k].split(\" --- \")  ## 구분자를 이용해 구분\n",
    "    \n",
    "    original_data = pd.DataFrame(original_data)\n",
    "    original_data.columns = [\"category\", \"date\", \"title\", \"text\", \"author\"]  # specify the column name\n",
    "    original_data.index = list(range(len(original_data)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------데이터 구조 변경----------------------------------------------------\n",
    "    data_title_indx = np.array(np.where(original_data.iloc[:,3].isnull())[0])\n",
    "    data_title = original_data.iloc[data_title_indx,:]\n",
    "    data_text_indx = np.array(np.where(original_data.iloc[:,4].isnull())[0])\n",
    "    data_text = original_data.iloc[np.setdiff1d(data_text_indx, data_title_indx),:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # case 1. title만 있는 경우 변경\n",
    "    data_1_indx = np.intersect1d(data_title_indx,original_data[original_data[\"title\"] == \" ---\"].index)\n",
    "    data_1 = original_data.iloc[data_1_indx,:] ## case 1에 맞는 데이터\n",
    "    data_1.title = data_1.date\n",
    "    data_1.loc[:,[\"category\", \"date\"]] = None\n",
    "    original_data.iloc[data_1_indx,:] = data_1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # case 2. title과 text만 있는 경우 변경\n",
    "    data_2_indx = np.setdiff1d(data_title_indx, data_1_indx)\n",
    "    data_2 = original_data.iloc[data_2_indx,:]\n",
    "    data_2.text = data_2.title\n",
    "    data_2.title = data_2.date\n",
    "    data_2.loc[:,[\"category\", \"date\"]] = None\n",
    "    original_data.iloc[data_2_indx,:] = data_2\n",
    "    \n",
    "    \n",
    "    \n",
    "    # case 3. title, text, author만 있는 경우\n",
    "    data_3_indx = np.intersect1d(data_text_indx,original_data[original_data[\"category\"] == \"--- \"].index)\n",
    "    data_3 =original_data.iloc[data_3_indx,:]\n",
    "    data_3.author = data_3.text\n",
    "    data_3.text = data_3.title\n",
    "    data_3.title = data_3.date\n",
    "    data_3.loc[:,[\"category\", \"date\"]] = None\n",
    "    original_data.iloc[data_3_indx,:] = data_3\n",
    "\n",
    "    \n",
    "    \n",
    "    # 기사 최초 작성일만 추출\n",
    "    original_data.date = original_data.date.str.split(\"/\").str[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------불필요한 데이터 제거----------------------------------------------------\n",
    "    date_null_ind = np.array(np.where(original_data.iloc[:,[1]].isnull())[0])\n",
    "    date_black_ind = np.array(np.where(original_data.date==\"\")[0])\n",
    "    \n",
    "    del_date_ind = np.concatenate((date_null_ind, date_black_ind)) # date 열이 None인 행의 인덱스 추출\n",
    "    original_data = original_data.drop(del_date_ind)  # 인덱스에 해당하는 행 삭제 즉, date 열이 None인 행 삭제\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------데이터 시간 형식 변경----------------------------------------------------\n",
    "    ## 원래 date : Jan. 1, 2021, 12:23 AM UTC \n",
    "    ## 가공된 date : 2021-01-01 00:23:00\n",
    "    original_data.date = original_data.date.apply(lambda x:x.strip(\" UTC\\xa0\")) ## data의 date 열에서 UTC\\xa0이나 UTC에 해당하는 부분 삭제\n",
    "    original_data.date = original_data.date.apply(lambda x:x.strip(\" UTC\"))\n",
    "\n",
    "    original_data.date = original_data.date.apply(lambda x: pd.to_datetime(x, errors=\"ignore\"))  ## datetime 형식으로 변경\n",
    "    \n",
    "    # 기준 이전의 날짜는 자르기\n",
    "    original_data = original_data[original_data.date>=\"2012-01-01\"]\n",
    "    original_data = original_data[original_data.date<\"2022-05-01\"]\n",
    "    \n",
    "    \n",
    "    ## ---------------------text와 title 결합--------------------\n",
    "    original_data[\"title_text\"] = original_data[\"title\"]+\" \" +original_data[\"text\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------구두점 제거----------------------------------------------------\n",
    "    fullstop = re.compile(r'[,—\"“”‘’\\'-?:!;\\\\]')\n",
    "    original_data.title_text = original_data.title_text.apply(lambda x: fullstop.sub(\" \",x))  ## title에서 제거\n",
    "    \n",
    "           \n",
    "    \n",
    "    ## ----------------------토큰화----------------------------------------------------\n",
    "    tit_txt_combination = list(map(word_tokenize, original_data.title_text))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------대문자 소문자 변환----------------------------------------------------\n",
    "    tit_txt_combination = list(map(upper_to_lower_line, tit_txt_combination)) ## 첫번째 문자만 대문자인 경우 소문자로 변환\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------표제어 추출----------------------------------------------------\n",
    "    tit_txt_combination = list(map(lemmatization_line, tit_txt_combination))\n",
    "   \n",
    "    \n",
    "    \n",
    "    ## ----------------------불용어 제거----------------------------------------------------\n",
    "    tit_txt_combination = list(map(del_stopword, tit_txt_combination))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ----------------------데이터 프레임으로 형식 변환----------------------------------------------------\n",
    "    tit_txt_combination = pd.DataFrame(tit_txt_combination)    # 데이터를 정리하기 위해서 변환이 필요\n",
    "    \n",
    "    return [original_data, tit_txt_combination]\n",
    "\n",
    "\n",
    "    \n",
    "def list_chunk(lst, num_cores):   ## num_cores에 맞게 df를 분리하는 함수  1,2,3,4 -> 코어가 2개인 경우 [1,2],[3,4]로 분류\n",
    "    return [lst[i:i+num_cores] for i in range(0, len(lst), num_cores)]              ##   각 코어어 할당해주기 위해 필요\n",
    "\n",
    "\n",
    "def type_conversion(df):   ## 한 코어가 실행한 n개의 데이터 프레임을 1개로 합치는 함수\n",
    "    \n",
    "    original_data = pd.DataFrame()\n",
    "    tit_txt_combination = pd.DataFrame()\n",
    "    \n",
    "    for j in df:\n",
    "        original_data_, tit_txt_combination_ = j\n",
    "        original_data = pd.concat([original_data,original_data_])\n",
    "        tit_txt_combination = pd.concat([tit_txt_combination,tit_txt_combination_])\n",
    "    return [original_data, tit_txt_combination]\n",
    "\n",
    "\n",
    "def parallel(df, func, num_cores):\n",
    "    \n",
    "    df_split = list_chunk(df,num_cores)  ## num_cores에 맞게 데이터 분리\n",
    "\n",
    "    pool = Pool(num_cores) ## process에 분배하여 함수 실행의 병렬처리를 도와줌\n",
    "    \n",
    "    def pool_map(df_split):\n",
    "        df = pool.map(loading, df_split)          ## 위에서 나눈 df_split을 각 process에 할당하여 실행\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    df = list(map(pool_map, df_split))\n",
    "    \n",
    "    df = list(map(type_conversion,df))  ## 각각의 코어가 맡은 데이터프레임을 합침 즉, 1개의 코어가 2개의 데이터프레임을 실행했다면? 1개의 데이터프레임으로 합침\n",
    "\n",
    "    ## 각 코어들이 만든 데이터 프레임도 합침\n",
    "    original_data, tit_txt_combination = type_conversion(df)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return original_data,tit_txt_combination\n",
    "\n",
    "def main():\n",
    "    df = year_month\n",
    "    num_cores = 28\n",
    "    original_data, tit_txt_combination = parallel(df, loading, num_cores)\n",
    "    return original_data, tit_txt_combination\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = datetime.datetime.now()\n",
    "    original_data, tit_txt_combination = main()\n",
    "    print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295f6cd4-f146-4603-abdb-fca176070ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:01.063026\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "tit_txt_combination = tit_txt_combination.where(pd.notnull(tit_txt_combination), None)\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba553d8a-d698-41f6-8644-95e03baaf647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:57.329801\n"
     ]
    }
   ],
   "source": [
    "## 빈도수 기반 사전 생성\n",
    "def del_list_None(lst):\n",
    "    lst = list(filter(None, lst))\n",
    "    return lst\n",
    "\n",
    "start = datetime.datetime.now()  ## 시작 시간\n",
    "\n",
    "tit_txt_combination = tit_txt_combination.values.tolist()  ## 데이터프레임 -> list\n",
    "tit_txt_combination = list(map(del_list_None, tit_txt_combination))   ## 변환하면서 생긴 None 삭제\n",
    "\n",
    "frequent_tit_txt = pd.DataFrame(pd.Series(list(itertools.chain(*tit_txt_combination))).value_counts())   ## tit에 있는 단어들의 빈도수 추출\n",
    "tit_txt_less_frequent_word = list(frequent_tit_txt[4 >= frequent_tit_txt[0]].index)     ## 빈도가 4 이하인 단어를 추출\n",
    "\n",
    "print(datetime.datetime.now()-start)  # 끝나는 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d7b8c7-36a2-4353-8532-4880f6f83feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:04.617430\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def del_less_frequent_tit_txt(line):  ## 리스트의 한 line  [\"a\",\"b\",\"c\"]\n",
    "    line_frequent_intersection = list(set(line)& set(tit_txt_less_frequent_word))   ## 불용어 사전과 동일하게 각 line과 불용어의 교집합 사전을 만들어 해당되는 값 제거\n",
    "    data = difference(line, line_frequent_intersection)\n",
    "    return data\n",
    "\n",
    "def work_func(tit_txt_combination):\n",
    "    tit_txt_combination = list(map(del_list_None, tit_txt_combination))  \n",
    "    tit_txt_combination = list(map(del_less_frequent_tit_txt, tit_txt_combination))\n",
    "    return tit_txt_combination\n",
    "\n",
    "def main():\n",
    "    num_cores = 28\n",
    "    df = tit_txt_combination\n",
    "    df_split = np.array_split(df,num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pool.map(work_func, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = datetime.datetime.now()\n",
    "    tit_txt_combination = main()\n",
    "    print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985bd762-a705-41e0-bb5e-417a05c32c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tit_txt_combination)-1):\n",
    "    i += 1\n",
    "    tit_txt_combination[0].extend(tit_txt_combination[i])\n",
    "tit_txt_combination = tit_txt_combination[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "867a6af9-df8e-4dcb-9eab-11b878c793d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:30.082053\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "tit_txt_combination = pd.DataFrame(tit_txt_combination)\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "137ece39-5357-45bb-9001-4293306125ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/home/whfhrs3260/csv_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4599e85-6812-436d-9b30-fe33e529ee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.360573\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "original_data.to_pickle(dir+\"original_data.pkl\")\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98edb9ec-abdc-43fe-9dbb-00fbe390902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:36.690935\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "tit_txt_combination.to_pickle(dir+\"tit_txt_combination_4years.pkl\")\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "808a3683-3300-4e68-9096-1091c4d8efb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>title_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. news</td>\n",
       "      <td>2017-01-01 00:20:00</td>\n",
       "      <td>Texas Judge Halts Federal Transgender Protections</td>\n",
       "      <td>AUSTIN, Texas — A federal judge in Texas has o...</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>Texas Judge Halts Federal Transgender Protecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>World</td>\n",
       "      <td>2017-01-01 01:08:00</td>\n",
       "      <td>At Least 39 Killed in Shooting at Turkey Night...</td>\n",
       "      <td></td>\n",
       "      <td>NBC News</td>\n",
       "      <td>At Least    Killed in Shooting at Turkey Night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World</td>\n",
       "      <td>2017-01-01 02:31:00</td>\n",
       "      <td>President Obama Briefed on Turkey Attack</td>\n",
       "      <td></td>\n",
       "      <td>NBC News</td>\n",
       "      <td>President Obama Briefed on Turkey Attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TV</td>\n",
       "      <td>2017-01-01 03:56:00</td>\n",
       "      <td>Actor William Christopher, \"M*A*S*H\" Chaplain,...</td>\n",
       "      <td>William Christopher, who played the unassuming...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Actor William Christopher   M A S H  Chaplain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>World</td>\n",
       "      <td>2016-12-31 14:07:00</td>\n",
       "      <td>Hello, 2017! Cities Around the World Ring in S...</td>\n",
       "      <td>Revelers rang in 2017 across the globe with da...</td>\n",
       "      <td>Alastair JamiesonErik OrtizAndy Eckardt</td>\n",
       "      <td>Hello        Cities Around the World Ring in S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>Pop Culture News</td>\n",
       "      <td>2022-04-30 16:17:00</td>\n",
       "      <td>Veteran TV actress Jossara Jinaro dies of canc...</td>\n",
       "      <td>Veteran actress Jossara Jinaro, known for her ...</td>\n",
       "      <td>Nicole Acevedo</td>\n",
       "      <td>Veteran TV actress Jossara Jinaro dies of canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>Russia-Ukraine Conflict</td>\n",
       "      <td>2022-04-30 17:32:00</td>\n",
       "      <td>Angelina Jolie visits children, doctors in Ukr...</td>\n",
       "      <td>Actor Angelina Jolie arrived in Lviv, Ukraine ...</td>\n",
       "      <td>Diana DasrathMinyvonne Burke</td>\n",
       "      <td>Angelina Jolie visits children  doctors in Ukr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>U.S. news</td>\n",
       "      <td>2022-04-30 19:34:00</td>\n",
       "      <td>Teacher under fire for cotton, cuffs in class ...</td>\n",
       "      <td>Rochester school officials are investigating a...</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>Teacher under fire for cotton  cuffs in class ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>Culture Matters</td>\n",
       "      <td>2022-04-30 19:22:00</td>\n",
       "      <td>Bill Murray speaks on accusations of inappropr...</td>\n",
       "      <td>Actor Bill Murray opened up about what led to ...</td>\n",
       "      <td>Minyvonne Burke</td>\n",
       "      <td>Bill Murray speaks on accusations of inappropr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>Latino</td>\n",
       "      <td>2022-04-30 11:07:00</td>\n",
       "      <td>Latino legal scholar remembered for advancing ...</td>\n",
       "      <td>A Latino law professor is being remembered for...</td>\n",
       "      <td>Edwin Flores</td>\n",
       "      <td>Latino legal scholar remembered for advancing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93771 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     category                date  \\\n",
       "0                   U.S. news 2017-01-01 00:20:00   \n",
       "1                       World 2017-01-01 01:08:00   \n",
       "2                       World 2017-01-01 02:31:00   \n",
       "3                          TV 2017-01-01 03:56:00   \n",
       "4                       World 2016-12-31 14:07:00   \n",
       "...                       ...                 ...   \n",
       "1826         Pop Culture News 2022-04-30 16:17:00   \n",
       "1827  Russia-Ukraine Conflict 2022-04-30 17:32:00   \n",
       "1828                U.S. news 2022-04-30 19:34:00   \n",
       "1829          Culture Matters 2022-04-30 19:22:00   \n",
       "1830                   Latino 2022-04-30 11:07:00   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Texas Judge Halts Federal Transgender Protections   \n",
       "1     At Least 39 Killed in Shooting at Turkey Night...   \n",
       "2              President Obama Briefed on Turkey Attack   \n",
       "3     Actor William Christopher, \"M*A*S*H\" Chaplain,...   \n",
       "4     Hello, 2017! Cities Around the World Ring in S...   \n",
       "...                                                 ...   \n",
       "1826  Veteran TV actress Jossara Jinaro dies of canc...   \n",
       "1827  Angelina Jolie visits children, doctors in Ukr...   \n",
       "1828  Teacher under fire for cotton, cuffs in class ...   \n",
       "1829  Bill Murray speaks on accusations of inappropr...   \n",
       "1830  Latino legal scholar remembered for advancing ...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     AUSTIN, Texas — A federal judge in Texas has o...   \n",
       "1                                                         \n",
       "2                                                         \n",
       "3     William Christopher, who played the unassuming...   \n",
       "4     Revelers rang in 2017 across the globe with da...   \n",
       "...                                                 ...   \n",
       "1826  Veteran actress Jossara Jinaro, known for her ...   \n",
       "1827  Actor Angelina Jolie arrived in Lviv, Ukraine ...   \n",
       "1828  Rochester school officials are investigating a...   \n",
       "1829  Actor Bill Murray opened up about what led to ...   \n",
       "1830  A Latino law professor is being remembered for...   \n",
       "\n",
       "                                       author  \\\n",
       "0                        The Associated Press   \n",
       "1                                    NBC News   \n",
       "2                                    NBC News   \n",
       "3                                     Reuters   \n",
       "4     Alastair JamiesonErik OrtizAndy Eckardt   \n",
       "...                                       ...   \n",
       "1826                           Nicole Acevedo   \n",
       "1827             Diana DasrathMinyvonne Burke   \n",
       "1828                         Associated Press   \n",
       "1829                          Minyvonne Burke   \n",
       "1830                             Edwin Flores   \n",
       "\n",
       "                                             title_text  \n",
       "0     Texas Judge Halts Federal Transgender Protecti...  \n",
       "1     At Least    Killed in Shooting at Turkey Night...  \n",
       "2             President Obama Briefed on Turkey Attack   \n",
       "3     Actor William Christopher   M A S H  Chaplain ...  \n",
       "4     Hello        Cities Around the World Ring in S...  \n",
       "...                                                 ...  \n",
       "1826  Veteran TV actress Jossara Jinaro dies of canc...  \n",
       "1827  Angelina Jolie visits children  doctors in Ukr...  \n",
       "1828  Teacher under fire for cotton  cuffs in class ...  \n",
       "1829  Bill Murray speaks on accusations of inappropr...  \n",
       "1830  Latino legal scholar remembered for advancing ...  \n",
       "\n",
       "[93771 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a86d3cf3-e001-4139-99f8-8e7c2233ab20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16264</th>\n",
       "      <th>16265</th>\n",
       "      <th>16266</th>\n",
       "      <th>16267</th>\n",
       "      <th>16268</th>\n",
       "      <th>16269</th>\n",
       "      <th>16270</th>\n",
       "      <th>16271</th>\n",
       "      <th>16272</th>\n",
       "      <th>16273</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>texas</td>\n",
       "      <td>judge</td>\n",
       "      <td>halt</td>\n",
       "      <td>federal</td>\n",
       "      <td>transgender</td>\n",
       "      <td>protection</td>\n",
       "      <td>AUSTIN</td>\n",
       "      <td>texas</td>\n",
       "      <td>federal</td>\n",
       "      <td>judge</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>least</td>\n",
       "      <td>killed</td>\n",
       "      <td>shooting</td>\n",
       "      <td>turkey</td>\n",
       "      <td>nightclub</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>president</td>\n",
       "      <td>obama</td>\n",
       "      <td>briefed</td>\n",
       "      <td>turkey</td>\n",
       "      <td>attack</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>actor</td>\n",
       "      <td>william</td>\n",
       "      <td>christopher</td>\n",
       "      <td>h</td>\n",
       "      <td>chaplain</td>\n",
       "      <td>dead</td>\n",
       "      <td>william</td>\n",
       "      <td>christopher</td>\n",
       "      <td>played</td>\n",
       "      <td>unassuming</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello</td>\n",
       "      <td>city</td>\n",
       "      <td>around</td>\n",
       "      <td>world</td>\n",
       "      <td>ring</td>\n",
       "      <td>start</td>\n",
       "      <td>new</td>\n",
       "      <td>year</td>\n",
       "      <td>reveler</td>\n",
       "      <td>rang</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93766</th>\n",
       "      <td>veteran</td>\n",
       "      <td>TV</td>\n",
       "      <td>actress</td>\n",
       "      <td>jinaro</td>\n",
       "      <td>dy</td>\n",
       "      <td>cancer</td>\n",
       "      <td>age</td>\n",
       "      <td>family</td>\n",
       "      <td>said</td>\n",
       "      <td>veteran</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93767</th>\n",
       "      <td>angelina</td>\n",
       "      <td>jolie</td>\n",
       "      <td>visit</td>\n",
       "      <td>child</td>\n",
       "      <td>doctor</td>\n",
       "      <td>ukraine</td>\n",
       "      <td>actor</td>\n",
       "      <td>angelina</td>\n",
       "      <td>jolie</td>\n",
       "      <td>arrived</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93768</th>\n",
       "      <td>teacher</td>\n",
       "      <td>fire</td>\n",
       "      <td>cotton</td>\n",
       "      <td>cuff</td>\n",
       "      <td>class</td>\n",
       "      <td>slavery</td>\n",
       "      <td>rochester</td>\n",
       "      <td>school</td>\n",
       "      <td>official</td>\n",
       "      <td>investigating</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93769</th>\n",
       "      <td>bill</td>\n",
       "      <td>murray</td>\n",
       "      <td>speaks</td>\n",
       "      <td>accusation</td>\n",
       "      <td>inappropriate</td>\n",
       "      <td>behavior</td>\n",
       "      <td>led</td>\n",
       "      <td>suspension</td>\n",
       "      <td>mortal</td>\n",
       "      <td>actor</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93770</th>\n",
       "      <td>latino</td>\n",
       "      <td>legal</td>\n",
       "      <td>scholar</td>\n",
       "      <td>remembered</td>\n",
       "      <td>advancing</td>\n",
       "      <td>equity</td>\n",
       "      <td>education</td>\n",
       "      <td>law</td>\n",
       "      <td>latino</td>\n",
       "      <td>law</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93771 rows × 16274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1            2           3              4           5      \\\n",
       "0          texas    judge         halt     federal    transgender  protection   \n",
       "1          least   killed     shooting      turkey      nightclub        None   \n",
       "2      president    obama      briefed      turkey         attack        None   \n",
       "3          actor  william  christopher           h       chaplain        dead   \n",
       "4          hello     city       around       world           ring       start   \n",
       "...          ...      ...          ...         ...            ...         ...   \n",
       "93766    veteran       TV      actress      jinaro             dy      cancer   \n",
       "93767   angelina    jolie        visit       child         doctor     ukraine   \n",
       "93768    teacher     fire       cotton        cuff          class     slavery   \n",
       "93769       bill   murray       speaks  accusation  inappropriate    behavior   \n",
       "93770     latino    legal      scholar  remembered      advancing      equity   \n",
       "\n",
       "           6            7         8              9      ... 16264 16265 16266  \\\n",
       "0         AUSTIN        texas   federal          judge  ...  None  None  None   \n",
       "1           None         None      None           None  ...  None  None  None   \n",
       "2           None         None      None           None  ...  None  None  None   \n",
       "3        william  christopher    played     unassuming  ...  None  None  None   \n",
       "4            new         year   reveler           rang  ...  None  None  None   \n",
       "...          ...          ...       ...            ...  ...   ...   ...   ...   \n",
       "93766        age       family      said        veteran  ...  None  None  None   \n",
       "93767      actor     angelina     jolie        arrived  ...  None  None  None   \n",
       "93768  rochester       school  official  investigating  ...  None  None  None   \n",
       "93769        led   suspension    mortal          actor  ...  None  None  None   \n",
       "93770  education          law    latino            law  ...  None  None  None   \n",
       "\n",
       "      16267 16268 16269 16270 16271 16272 16273  \n",
       "0      None  None  None  None  None  None  None  \n",
       "1      None  None  None  None  None  None  None  \n",
       "2      None  None  None  None  None  None  None  \n",
       "3      None  None  None  None  None  None  None  \n",
       "4      None  None  None  None  None  None  None  \n",
       "...     ...   ...   ...   ...   ...   ...   ...  \n",
       "93766  None  None  None  None  None  None  None  \n",
       "93767  None  None  None  None  None  None  None  \n",
       "93768  None  None  None  None  None  None  None  \n",
       "93769  None  None  None  None  None  None  None  \n",
       "93770  None  None  None  None  None  None  None  \n",
       "\n",
       "[93771 rows x 16274 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tit_txt_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964549d8-954d-4c36-a729-d5e6f91c0f07",
   "metadata": {},
   "source": [
    "1. 타이틀, 본문을 왜 결합 사용: 딥러닝 모형\n",
    "2. 타이틀에 있는 단어는 거의 본문에서 다시 사용하지 않는가? 다른 단어지만 의미는 같은 단어?\n",
    "3. 다른 전처리 과정에 통합해야 할 듯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
