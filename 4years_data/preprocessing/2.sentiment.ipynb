{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b229fe33-cded-4514-bda3-c684a88733a2",
   "metadata": {},
   "source": [
    "1. sentiment 분석에서 왜 NLTK의 VADER을 사용하였나?\n",
    "\n",
    "Stock prediction using combination of BERT sentiment Analysis and macro economy index, 2020\n",
    "\n",
    "Stock Market Trend Forecasting Based on Multiple Textual Features: A Deep Learning Method, 2021\n",
    "\n",
    "위 논문에서 VADER을 사용하였다.\n",
    "\n",
    "2. sentiment 분성에서 왜 기준은 compound를 0.2로 하였나?\n",
    "\n",
    "Stock prediction using combination of BERT sentiment Analysis and macro economy index, 2020\n",
    "\n",
    "위 논문에서 기준을 0.2로 잡았다. compound는 복합적으로 긍정에 가까운지 부정에 가까운지를 -1~1까지의 값으로 나타내어준다.\n",
    "\n",
    "3. 왜 class가 3개인 것과 5개 인것으로 나누었나?\n",
    "\n",
    "보통 -1, 0, 1로 하지만 강한 긍정, 강한 부정과 약한 긍정, 약한 부정이 있지 않을까 싶어 class가 5개인 경우도 분석해볼 예정이다.\n",
    "\n",
    "4. 기준을 둘것인가? 기준을 두지 않을 것인가?\n",
    "\n",
    "Stock prediction using combination of BERT sentiment Analysis and macro economy index, 2020 이 논문에서는 기준을 두어 class로 분류해 분석을 하였다.\n",
    "\n",
    "Stock Market Trend Forecasting Based on Multiple Textual Features: A Deep Learning Method, 2021 이 논문은 기준을 두지 않은 score로 분석을 하였다.\n",
    "\n",
    "이부분은 고민을 좀 더 해볼 예정\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd61435-c1c0-429e-b7ef-038faa9377e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whfhrs3260/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-24 21:00:09.982478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 21:00:10.104431: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-24 21:00:10.130258: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-24 21:00:10.673155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-24 21:00:10.673208: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-24 21:00:10.673213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import smart_open\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import copy\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e877a9-8e0e-432f-abcd-ea67ad88674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/home/whfhrs3260/csv_data/'\n",
    "original_news_dir = dir + \"original_data.pkl\"\n",
    "tit_txt_dir = dir + \"tit_txt_combination_4years.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b845c0f1-254e-43b8-9fdd-26f8af1e0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_news = pd.read_pickle(original_news_dir)\n",
    "tit_txt_combination = pd.read_pickle(tit_txt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf312b5-d43b-4161-be24-6998d07662f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## date type으로 변경\n",
    "original_news.date = original_news.date.apply(lambda x: pd.to_datetime(x, errors=\"ignore\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00a8597-87f2-4134-ad14-b6b68bd800a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시간을 날짜 단위로 짜름\n",
    "def date_until_day(date_timestamp):  ## 2021-01-01 00:23:00 => 2021-01-01\n",
    "    date_timestamp = datetime.datetime.strftime(date_timestamp, \"%Y-%m-%d\")  \n",
    "    return date_timestamp\n",
    "\n",
    "\n",
    "news_day = copy.deepcopy(original_news)     ## original data를 value만 copy\n",
    "\n",
    "news_day.date = news_day.date.apply(lambda x: date_until_day(x))  ## 시간을 시 단위로 짜름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6629295d-e511-45bc-b4d5-af45a841c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_list_None(lst):\n",
    "    lst = list(filter(None, lst))\n",
    "    return lst\n",
    "\n",
    "\n",
    "## NaN -> None으로 변경(데이터 처리를 편리하게 하기 위해)\n",
    "tit_txt_combination = tit_txt_combination.where(pd.notnull(tit_txt_combination), None)\n",
    "\n",
    "tit_txt_combination.reset_index(inplace=True, drop=True)\n",
    "news_day.reset_index(inplace=True, drop=True)\n",
    "\n",
    "## 데이터에 date 변수 붙여주기\n",
    "tit_txt_day = pd.concat([news_day.date,tit_txt_combination],axis=1) \n",
    "\n",
    "\n",
    "## 시간 순으로 정렬\n",
    "tit_txt_day = tit_txt_day.sort_values(by='date')\n",
    "\n",
    "## 새로 정렬된 데이터의 인덱스를 순서대로 재설정  # index 100 51 21 40 ... => 1 2 3 4 5 ...\n",
    "tit_txt_day = tit_txt_day.reset_index(drop=True)\n",
    "\n",
    "# 데이터프레임을 list로 변환\n",
    "tit_txt_day = tit_txt_day.values.tolist() \n",
    "\n",
    "## 데이터 프레임을 list로 변환하는 과정에서 포함된 None을 삭제\n",
    "tit_txt_day = list(map(del_list_None, tit_txt_day ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6370c66-7308-4faf-b89c-420a832881e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 날짜인 경우 데이터를 합침 ex) 1월 1일 00시 00분 ~ 23시 59분 사이의 기사는 전부 합침\n",
    "length = -1\n",
    "tit_txt_combination_day = []\n",
    "\n",
    "basic_date = 0             ## 기준이 되는 date\n",
    "for i in range(len(tit_txt_day)):  ## 전체를 한번씩 돈다\n",
    "    new_date = tit_txt_day[i][0]    # new date는 현재 loop의 date\n",
    "    if basic_date == new_date:   # 현재 loop의 date가 기준 date와 같으면 실행\n",
    "        tit_txt_combination_day[length] = tit_txt_combination_day[length]+tit_txt_day[i][1:(len(tit_txt_day[i]))] ## 앞의 데이터에 새로운 데이터를 결합\n",
    "    else:                        # 현재 loop의 date가 기준 date와 다르면 실행 즉, 새로운 시간이 나타나면 실행\n",
    "        length += 1             # 길이가 한개 늘어남\n",
    "        tit_txt_combination_day.append(tit_txt_day[i][1:(len(tit_txt_day[i]))]) # 새로운 list 데이터 추가\n",
    "        basic_date = tit_txt_day[i][0]              # 기준 date를 새로운 date로 변경\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "561d4f55-3640-4c3a-8835-0e1db84882d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tit_txt_day = pd.DataFrame(tit_txt_day)\n",
    "tit_txt_token = tit_txt_day.loc[:,1:tit_txt_day.shape[1]].copy() ## token만 뽑아 오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0adc817a-f125-48ab-bd96-85e315766236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_sentence(lst):  ## 토큰화가 되어있는 tit, tit 데이터를 한 문장으로 만듦\n",
    "    lst = \" \".join(lst)     ## ex) the, and, me, bye => the and me bye\n",
    "    return lst\n",
    "\n",
    "## 각 문장마다 수행, 결과 ex) [\"the and me bye\", ...,\"i do not me\"]\n",
    "tit_txt_token = tit_txt_token.values.tolist()                  # dataframe -> list로 변경\n",
    "tit_txt_token = list(map(del_list_None, tit_txt_token ))       # None 삭제\n",
    "tit_txt_sentence = list(map(word_to_sentence, tit_txt_token))  ## token을 다시 연결해 문장으로 만듦\n",
    "tit_txt_sentence = pd.DataFrame(tit_txt_sentence)              # dataframe으로 변경\n",
    "tit_txt_sentence = pd.concat([tit_txt_day.loc[:,0],tit_txt_sentence],axis=1) # sentence에 date를 붙힘\n",
    "tit_txt_sentence.columns = [\"Date\",\"Sentence\"]  # 열이름 새로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf99becb-9557-4b2b-8dc5-fbb898a237d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258082/3588393056.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tit_txt_sentence.Score[i] = score[\"compound\"]                        # 긍정, 부정, 중립, 복합 중 복합 점수만 추출\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "tit_txt_sentence[\"Score\"]=0\n",
    "\n",
    "for i in range(len(tit_txt_sentence)):\n",
    "    score = analyzer.polarity_scores(tit_txt_sentence.Sentence[i])       # 감성점수 추출\n",
    "    tit_txt_sentence.Score[i] = score[\"compound\"]                        # 긍정, 부정, 중립, 복합 중 복합 점수만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c450a4c-b2c0-4433-8baf-0e0e454f941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# price data 불러오기\n",
    "price_data = yf.download(['^DJI'],start = '2016-12-31', end = \"2022-05-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c7ebce3-1746-4cb0-9092-b679dd3bd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_until_day(date_timestamp):  ## 2021-01-01 00:23:00 => 2021-01-01\n",
    "    date_timestamp = datetime.datetime.strftime(date_timestamp, \"%Y-%m-%d\")  \n",
    "    return date_timestamp\n",
    "\n",
    "## 개장일만 추출\n",
    "opening = price_data.index.copy()\n",
    "opening = pd.DataFrame(opening)\n",
    "opening[\"opening_date\"] = 1\n",
    "\n",
    "# 개장일 날짜 형태 변경\n",
    "opening.Date = opening.Date.apply(lambda x: date_until_day(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c07bd46d-2bbf-41bc-bfb5-db614041c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = pd.date_range(start='20161230', end='20220430')\n",
    "Date = pd.DataFrame({\"Date\" : Date.values})\n",
    "Date.Date = Date.Date.apply(lambda x: date_until_day(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db252d6b-c328-42b2-9160-a206e4a7b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_news_data_date = pd.merge(Date, opening, how =\"left\",left_on='Date', right_on = \"Date\") \n",
    "set_news_data_date = set_news_data_date.where(pd.notnull(set_news_data_date), 0)         # 개장일과 겹치지 않는 곳은 0\n",
    "set_news_data_date.Date = set_news_data_date.Date.apply(lambda x: pd.to_datetime(x, errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e08dc1a-6ded-4122-b678-f2fd6f87464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258082/2423599921.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n"
     ]
    }
   ],
   "source": [
    "standard=set_news_data_date.Date.iloc[len(set_news_data_date)-1] + datetime.timedelta(days=1)\n",
    "set_news_data_date[\"price_date\"]=0\n",
    "for i in range(len(set_news_data_date)-1,-1,-1):\n",
    "    if i==(len(set_news_data_date)-1):\n",
    "        standard = set_news_data_date.Date[i]\n",
    "        set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n",
    "    elif (set_news_data_date.opening_date[i]==1)&(set_news_data_date.opening_date[i+1]==1):\n",
    "        standard = set_news_data_date.Date[i]\n",
    "        set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n",
    "    elif (i!=0):\n",
    "        if((set_news_data_date.opening_date[i]==1)&(set_news_data_date.opening_date[i+1]==0)&(set_news_data_date.opening_date[i-1]==0)):\n",
    "            set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n",
    "            standard = set_news_data_date.Date[i]\n",
    "        else:\n",
    "            set_news_data_date.price_date[i]=standard\n",
    "    else:\n",
    "        set_news_data_date.price_date[i]=standard\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a15678f5-f1c9-45f9-bd97-238c9bf90d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstandard=set_news_data_date.Date.iloc[len(set_news_data_date)-1] + datetime.timedelta(days=1)\\nset_news_data_date[\"price_date\"]=0\\nfor i in range(len(set_news_data_date)-1,-1,-1):\\n    if i==(len(set_news_data_date)-1):\\n        standard = set_news_data_date.Date[i]\\n        set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\\n    elif (set_news_data_date.opening_date[i]==1)&(set_news_data_date.opening_date[i+1]==1):\\n        standard = set_news_data_date.Date[i]\\n        set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\\n    else:\\n        set_news_data_date.price_date[i]=standard\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "standard=set_news_data_date.Date.iloc[len(set_news_data_date)-1] + datetime.timedelta(days=1)\n",
    "set_news_data_date[\"price_date\"]=0\n",
    "for i in range(len(set_news_data_date)-1,-1,-1):\n",
    "    if i==(len(set_news_data_date)-1):\n",
    "        standard = set_news_data_date.Date[i]\n",
    "        set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n",
    "    elif (set_news_data_date.opening_date[i]==1)&(set_news_data_date.opening_date[i+1]==1):\n",
    "        standard = set_news_data_date.Date[i]\n",
    "        set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n",
    "    else:\n",
    "        set_news_data_date.price_date[i]=standard\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "763d4ca4-ee9d-441f-919a-00f67f6ed8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_news_data_date.Date = set_news_data_date.Date.apply(lambda x: date_until_day(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fa53279-d4d1-4851-80d6-4fc7613d124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tit_txt_sentence = tit_txt_sentence[[\"Date\",\"Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59d86569-3d08-4bc0-9032-54007d1fe6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tit_txt_score = pd.merge(set_news_data_date,tit_txt_sentence, how =\"left\",on = \"Date\")\n",
    "tit_txt_score = tit_txt_score[['Date','Score', \"price_date\"]]\n",
    "tit_txt_score = tit_txt_score.groupby('price_date').mean({\"Score\"})  \n",
    "tit_txt_score = tit_txt_score[tit_txt_score.index<=\"2022-04-29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05a871f9-c3a9-410d-8541-6e162f196d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = price_data[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61e880ed-deaa-448f-ae8f-b5a82b4ee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data_score = pd.concat([price_data,tit_txt_score],axis = 1)\n",
    "price_data_score = price_data_score[-price_data_score.Score.isnull()]\n",
    "price_data_score = price_data_score[[\"Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8d1efd7-0281-4f4d-bb1c-666d914f282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data_score.to_csv(dir+\"price_data_score_4years.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44525a-9d42-47ba-ac96-e142f733afd7",
   "metadata": {},
   "source": [
    "1. sentiment score + price로만\n",
    "2. sentiment score + price의 등락 + price로만\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
