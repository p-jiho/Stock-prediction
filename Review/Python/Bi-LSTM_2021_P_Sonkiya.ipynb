{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be45eb-cf71-4d7c-9577-a5a88555e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stock price prediction using BERT and GAN, 2017\n",
    "# LSTM -> Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033e96cd-4699-4ec8-b85d-dce7d1040d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 01:48:31.842977: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 01:48:31.943030: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-14 01:48:31.946082: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-14 01:48:31.946098: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-14 01:48:32.396088: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-14 01:48:32.396133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-14 01:48:32.396139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "import random\n",
    "import train_test as tt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d65c91-2bbb-4956-ae80-6789fc707570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_test, y_pred):\n",
    "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100 \n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d17fe5e-8761-478c-a58d-756a826d0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66eea42-0b4d-42cd-8039-d4669a4672b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 논문 재현(변수 다르고, APPLE 사용) #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8fce199-6a4e-4181-ac7b-a9cb45b2a4e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whfhrs3260/Python/10year_data/review/train_test.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whfhrs3260/Python/10year_data/review/train_test.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data.before_close[i+1] = price_data.Close[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 3s 32ms/step - loss: 0.0041 - val_loss: 0.0012\n",
      "Epoch 2/150\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.1951e-04 - val_loss: 4.5164e-05\n",
      "Epoch 3/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.8686e-05 - val_loss: 5.5531e-05\n",
      "Epoch 4/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1625e-05 - val_loss: 2.5378e-05\n",
      "Epoch 5/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9758e-05 - val_loss: 3.4613e-05\n",
      "Epoch 6/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0268e-05 - val_loss: 3.3834e-05\n",
      "Epoch 7/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9691e-05 - val_loss: 3.1085e-05\n",
      "Epoch 8/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0060e-05 - val_loss: 3.7102e-05\n",
      "Epoch 9/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0753e-05 - val_loss: 2.8183e-05\n",
      "Epoch 10/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1269e-05 - val_loss: 2.6828e-05\n",
      "Epoch 11/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2961e-05 - val_loss: 2.6505e-05\n",
      "Epoch 12/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0226e-05 - val_loss: 2.5579e-05\n",
      "Epoch 13/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3033e-05 - val_loss: 3.6119e-05\n",
      "Epoch 14/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0117e-05 - val_loss: 2.9961e-05\n",
      "Epoch 15/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9335e-05 - val_loss: 3.3366e-05\n",
      "Epoch 16/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0057e-05 - val_loss: 2.5004e-05\n",
      "Epoch 17/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9751e-05 - val_loss: 3.3334e-05\n",
      "Epoch 18/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.0399e-05 - val_loss: 2.3751e-05\n",
      "Epoch 19/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0474e-05 - val_loss: 4.0907e-05\n",
      "Epoch 20/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.5427e-05 - val_loss: 4.4268e-05\n",
      "Epoch 21/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.1174e-05 - val_loss: 2.3370e-05\n",
      "Epoch 22/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9333e-05 - val_loss: 2.4200e-05\n",
      "Epoch 23/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.3439e-05 - val_loss: 2.3484e-05\n",
      "Epoch 24/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1187e-05 - val_loss: 3.2967e-05\n",
      "Epoch 25/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.0699e-05 - val_loss: 3.5708e-05\n",
      "Epoch 26/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.5105e-05 - val_loss: 4.1598e-05\n",
      "Epoch 27/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3349e-05 - val_loss: 2.5457e-05\n",
      "Epoch 28/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9615e-05 - val_loss: 2.3671e-05\n",
      "Epoch 29/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9553e-05 - val_loss: 2.7519e-05\n",
      "Epoch 30/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.8034e-05 - val_loss: 2.3366e-05\n",
      "Epoch 31/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0970e-05 - val_loss: 4.8884e-05\n",
      "Epoch 32/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2466e-05 - val_loss: 3.0059e-05\n",
      "Epoch 33/150\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.9406e-05 - val_loss: 2.9921e-05\n",
      "Epoch 34/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0385e-05 - val_loss: 2.3444e-05\n",
      "Epoch 35/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1728e-05 - val_loss: 3.0697e-05\n",
      "Epoch 36/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9546e-05 - val_loss: 3.2110e-05\n",
      "Epoch 37/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0999e-05 - val_loss: 2.4025e-05\n",
      "Epoch 38/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1964e-05 - val_loss: 3.0297e-05\n",
      "Epoch 39/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0901e-05 - val_loss: 2.3039e-05\n",
      "Epoch 40/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.9617e-05 - val_loss: 2.3152e-05\n",
      "Epoch 41/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2125e-05 - val_loss: 2.3964e-05\n",
      "Epoch 42/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0312e-05 - val_loss: 2.4205e-05\n",
      "Epoch 43/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1464e-05 - val_loss: 3.0353e-05\n",
      "Epoch 44/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0048e-05 - val_loss: 3.4645e-05\n",
      "Epoch 45/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2339e-05 - val_loss: 4.2909e-05\n",
      "Epoch 46/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.3970e-05 - val_loss: 3.3368e-05\n",
      "Epoch 47/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.1843e-05 - val_loss: 2.3813e-05\n",
      "Epoch 48/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2177e-05 - val_loss: 2.2501e-05\n",
      "Epoch 49/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.0178e-05 - val_loss: 3.3443e-05\n",
      "Epoch 50/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0067e-05 - val_loss: 2.6611e-05\n",
      "Epoch 51/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.7954e-05 - val_loss: 2.5346e-05\n",
      "Epoch 52/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0902e-05 - val_loss: 2.4471e-05\n",
      "Epoch 53/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0183e-05 - val_loss: 2.2927e-05\n",
      "Epoch 54/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1258e-05 - val_loss: 2.5520e-05\n",
      "Epoch 55/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3502e-05 - val_loss: 2.5262e-05\n",
      "Epoch 56/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1394e-05 - val_loss: 5.0701e-05\n",
      "Epoch 57/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.5489e-05 - val_loss: 2.7597e-05\n",
      "Epoch 58/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9792e-05 - val_loss: 4.7434e-05\n",
      "Epoch 59/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0235e-05 - val_loss: 6.1104e-05\n",
      "Epoch 60/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.5583e-05 - val_loss: 2.3356e-05\n",
      "Epoch 61/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0102e-05 - val_loss: 2.5511e-05\n",
      "Epoch 62/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2689e-05 - val_loss: 3.0696e-05\n",
      "Epoch 63/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.9181e-05 - val_loss: 2.6609e-05\n",
      "Epoch 64/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3925e-05 - val_loss: 2.2047e-05\n",
      "Epoch 65/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9368e-05 - val_loss: 2.6374e-05\n",
      "Epoch 66/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0248e-05 - val_loss: 3.7476e-05\n",
      "Epoch 67/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8663e-05 - val_loss: 2.4096e-05\n",
      "Epoch 68/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.2850e-05 - val_loss: 4.9566e-05\n",
      "Epoch 69/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.2959e-05 - val_loss: 3.1386e-05\n",
      "Epoch 70/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0572e-05 - val_loss: 2.1199e-05\n",
      "Epoch 71/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3398e-05 - val_loss: 2.5943e-05\n",
      "Epoch 72/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9932e-05 - val_loss: 2.6940e-05\n",
      "Epoch 73/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0486e-05 - val_loss: 2.0674e-05\n",
      "Epoch 74/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.1789e-05 - val_loss: 2.6266e-05\n",
      "Epoch 75/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.1948e-05 - val_loss: 2.2188e-05\n",
      "Epoch 76/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8924e-05 - val_loss: 2.6183e-05\n",
      "Epoch 77/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.0646e-05 - val_loss: 5.6703e-05\n",
      "Epoch 78/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9104e-05 - val_loss: 6.6526e-05\n",
      "Epoch 79/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.6175e-05 - val_loss: 2.1293e-05\n",
      "Epoch 80/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3937e-05 - val_loss: 2.5616e-05\n",
      "Epoch 81/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3993e-05 - val_loss: 2.0841e-05\n",
      "Epoch 82/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0679e-05 - val_loss: 4.3169e-05\n",
      "Epoch 83/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8382e-05 - val_loss: 3.6805e-05\n",
      "Epoch 84/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2263e-05 - val_loss: 3.7823e-05\n",
      "Epoch 85/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.0581e-05 - val_loss: 2.4050e-05\n",
      "Epoch 86/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.0903e-05 - val_loss: 2.2118e-05\n",
      "Epoch 87/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1332e-05 - val_loss: 2.8126e-05\n",
      "Epoch 88/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8142e-05 - val_loss: 2.3657e-05\n",
      "Epoch 89/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1049e-05 - val_loss: 2.1254e-05\n",
      "Epoch 90/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.8995e-05 - val_loss: 2.3135e-05\n",
      "Epoch 91/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9114e-05 - val_loss: 2.1771e-05\n",
      "Epoch 92/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8108e-05 - val_loss: 2.3349e-05\n",
      "Epoch 93/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9624e-05 - val_loss: 2.0595e-05\n",
      "Epoch 94/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2597e-05 - val_loss: 5.7871e-05\n",
      "Epoch 95/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1267e-05 - val_loss: 2.1730e-05\n",
      "Epoch 96/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8849e-05 - val_loss: 3.4441e-05\n",
      "Epoch 97/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.5011e-05 - val_loss: 8.0536e-05\n",
      "Epoch 98/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.3491e-05 - val_loss: 7.8825e-05\n",
      "Epoch 99/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.1045e-05 - val_loss: 2.1283e-05\n",
      "Epoch 100/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.6776e-05 - val_loss: 2.0175e-05\n",
      "Epoch 101/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.5802e-05 - val_loss: 2.3478e-05\n",
      "Epoch 102/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9945e-05 - val_loss: 2.4389e-05\n",
      "Epoch 103/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7396e-05 - val_loss: 2.0466e-05\n",
      "Epoch 104/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9658e-05 - val_loss: 1.9336e-05\n",
      "Epoch 105/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7697e-05 - val_loss: 2.9678e-05\n",
      "Epoch 106/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0555e-05 - val_loss: 1.9782e-05\n",
      "Epoch 107/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1062e-05 - val_loss: 2.1233e-05\n",
      "Epoch 108/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7301e-05 - val_loss: 2.3791e-05\n",
      "Epoch 109/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3642e-05 - val_loss: 7.1948e-05\n",
      "Epoch 110/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.7069e-05 - val_loss: 2.0122e-05\n",
      "Epoch 111/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3012e-05 - val_loss: 3.6410e-05\n",
      "Epoch 112/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9505e-05 - val_loss: 1.9161e-05\n",
      "Epoch 113/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8736e-05 - val_loss: 1.9291e-05\n",
      "Epoch 114/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.9076e-05 - val_loss: 3.1136e-05\n",
      "Epoch 115/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.6844e-05 - val_loss: 1.9829e-05\n",
      "Epoch 116/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9039e-05 - val_loss: 2.2229e-05\n",
      "Epoch 117/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7507e-05 - val_loss: 2.4273e-05\n",
      "Epoch 118/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0694e-05 - val_loss: 3.6356e-05\n",
      "Epoch 119/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3901e-05 - val_loss: 2.1972e-05\n",
      "Epoch 120/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0188e-05 - val_loss: 7.8122e-05\n",
      "Epoch 121/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.1247e-05 - val_loss: 2.0334e-05\n",
      "Epoch 122/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.8734e-05 - val_loss: 2.2319e-05\n",
      "Epoch 123/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7462e-05 - val_loss: 3.0579e-05\n",
      "Epoch 124/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2032e-05 - val_loss: 2.2569e-05\n",
      "Epoch 125/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7595e-05 - val_loss: 3.7209e-05\n",
      "Epoch 126/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9254e-05 - val_loss: 1.9529e-05\n",
      "Epoch 127/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0970e-05 - val_loss: 3.1384e-05\n",
      "Epoch 128/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.7494e-05 - val_loss: 4.0100e-05\n",
      "Epoch 129/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.5205e-05 - val_loss: 2.7643e-05\n",
      "Epoch 130/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7858e-05 - val_loss: 2.7631e-05\n",
      "Epoch 131/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7622e-05 - val_loss: 2.0433e-05\n",
      "Epoch 132/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6879e-05 - val_loss: 2.4762e-05\n",
      "Epoch 133/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.8474e-05 - val_loss: 2.1086e-05\n",
      "Epoch 134/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.0110e-05 - val_loss: 3.5118e-05\n",
      "Epoch 135/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9544e-05 - val_loss: 2.1833e-05\n",
      "Epoch 136/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.8376e-05 - val_loss: 1.7836e-05\n",
      "Epoch 137/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8421e-05 - val_loss: 3.3072e-05\n",
      "Epoch 138/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7827e-05 - val_loss: 1.9198e-05\n",
      "Epoch 139/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.9227e-05 - val_loss: 3.9498e-05\n",
      "Epoch 140/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.1783e-05 - val_loss: 2.8032e-05\n",
      "Epoch 141/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6771e-05 - val_loss: 2.0576e-05\n",
      "Epoch 142/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1463e-05 - val_loss: 3.9440e-05\n",
      "Epoch 143/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7552e-05 - val_loss: 1.7255e-05\n",
      "Epoch 144/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6805e-05 - val_loss: 1.8198e-05\n",
      "Epoch 145/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6089e-05 - val_loss: 1.9638e-05\n",
      "Epoch 146/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.8910e-05 - val_loss: 1.7235e-05\n",
      "Epoch 147/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.4535e-05 - val_loss: 2.5234e-05\n",
      "Epoch 148/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.7466e-05 - val_loss: 1.8308e-05\n",
      "Epoch 149/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.9542e-05 - val_loss: 2.0288e-05\n",
      "Epoch 150/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6999e-05 - val_loss: 4.0305e-05\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0024\n"
     ]
    }
   ],
   "source": [
    "dir = \"/home/whfhrs3260/csv_data/price_data_score_10years.csv\"\n",
    "stock = \"AAPL\"\n",
    "variable = [\"before_close\",\"Score\"]\n",
    "window_size = 3\n",
    "start_date = \"2010-07-01\"\n",
    "end_date = \"2020-07-31\"\n",
    "\n",
    "scaler_x, scaler_y,x, y, x_t, y_t = tt.train_test_result(dir, stock, variable, window_size, start_date, end_date)\n",
    "\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Bidirectional(LSTM(128, input_shape = (x.shape[1],x.shape[2]))))\n",
    "bi_lstm_model.add(Dense(64))\n",
    "bi_lstm_model.add(Dense(1,activation=\"tanh\")) \n",
    "adam = optimizers.Adam(learning_rate=0.0012)\n",
    "bi_lstm_model.compile(optimizer=adam, loss = \"mse\")\n",
    "history = bi_lstm_model.fit(x, y, epochs=150, batch_size=64, validation_split=0.2)\n",
    "    \n",
    "bi_lstm_pred = bi_lstm_model.evaluate(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9aad281-5d8d-4c81-b5a9-d68236d6130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float64, numpy=4.769939369398908>,\n",
       " 22.752321587741648,\n",
       " 3.1731552927117597,\n",
       " 4.936982723174324)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_pred = bi_lstm_model.predict(x_t)\n",
    "bi_lstm_pred = scaler_y.inverse_transform(bi_lstm_pred)\n",
    "y_t = scaler_y.inverse_transform(y_t)\n",
    "root_mean_squared_error(y_t, bi_lstm_pred), mean_squared_error(y_t, bi_lstm_pred), mean_absolute_error(y_t, bi_lstm_pred), MAPE(y_t, bi_lstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31c7bc2b-917c-4cdf-a4f6-0331a2008142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### 2 variable(news, close 사용) #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f94ae6-3994-47d5-9030-23eef5332c6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whfhrs3260/Python/10year_data/review/train_test.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whfhrs3260/Python/10year_data/review/train_test.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data.before_close[i+1] = price_data.Close[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 3s 33ms/step - loss: 0.0102 - val_loss: 0.0021\n",
      "Epoch 2/150\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 5.2329e-04 - val_loss: 0.0012\n",
      "Epoch 3/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4892e-04 - val_loss: 0.0018\n",
      "Epoch 4/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 8.3841e-05 - val_loss: 0.0013\n",
      "Epoch 5/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 6.5307e-05 - val_loss: 0.0011\n",
      "Epoch 6/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5.2056e-05 - val_loss: 8.9385e-04\n",
      "Epoch 7/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.6215e-05 - val_loss: 7.4312e-04\n",
      "Epoch 8/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.4231e-05 - val_loss: 7.1460e-04\n",
      "Epoch 9/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.4728e-05 - val_loss: 7.4582e-04\n",
      "Epoch 10/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.1386e-05 - val_loss: 5.8524e-04\n",
      "Epoch 11/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.2766e-05 - val_loss: 5.5106e-04\n",
      "Epoch 12/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.1784e-05 - val_loss: 5.0622e-04\n",
      "Epoch 13/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.1414e-05 - val_loss: 5.8440e-04\n",
      "Epoch 14/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0789e-05 - val_loss: 4.8130e-04\n",
      "Epoch 15/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.1320e-05 - val_loss: 4.4078e-04\n",
      "Epoch 16/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.3702e-05 - val_loss: 4.4757e-04\n",
      "Epoch 17/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.1534e-05 - val_loss: 4.7471e-04\n",
      "Epoch 18/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0891e-05 - val_loss: 5.2021e-04\n",
      "Epoch 19/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0448e-05 - val_loss: 4.4531e-04\n",
      "Epoch 20/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.2713e-05 - val_loss: 4.8984e-04\n",
      "Epoch 21/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.1854e-05 - val_loss: 4.1881e-04\n",
      "Epoch 22/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9143e-05 - val_loss: 4.8074e-04\n",
      "Epoch 23/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.9443e-05 - val_loss: 4.4525e-04\n",
      "Epoch 24/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8781e-05 - val_loss: 3.9472e-04\n",
      "Epoch 25/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8271e-05 - val_loss: 3.2931e-04\n",
      "Epoch 26/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.7845e-05 - val_loss: 4.2020e-04\n",
      "Epoch 27/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0029e-05 - val_loss: 4.3688e-04\n",
      "Epoch 28/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7878e-05 - val_loss: 5.0501e-04\n",
      "Epoch 29/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9453e-05 - val_loss: 3.2786e-04\n",
      "Epoch 30/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0019e-05 - val_loss: 4.0779e-04\n",
      "Epoch 31/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.1603e-05 - val_loss: 3.1469e-04\n",
      "Epoch 32/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7832e-05 - val_loss: 3.5224e-04\n",
      "Epoch 33/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8129e-05 - val_loss: 3.3477e-04\n",
      "Epoch 34/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7541e-05 - val_loss: 2.5535e-04\n",
      "Epoch 35/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9691e-05 - val_loss: 3.2240e-04\n",
      "Epoch 36/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.0701e-05 - val_loss: 4.2799e-04\n",
      "Epoch 37/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.0359e-05 - val_loss: 2.8740e-04\n",
      "Epoch 38/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8264e-05 - val_loss: 3.9577e-04\n",
      "Epoch 39/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.0412e-05 - val_loss: 3.7223e-04\n",
      "Epoch 40/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9541e-05 - val_loss: 2.2549e-04\n",
      "Epoch 41/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8703e-05 - val_loss: 2.2083e-04\n",
      "Epoch 42/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9446e-05 - val_loss: 3.8195e-04\n",
      "Epoch 43/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6417e-05 - val_loss: 2.5119e-04\n",
      "Epoch 44/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7811e-05 - val_loss: 2.7566e-04\n",
      "Epoch 45/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6785e-05 - val_loss: 2.9569e-04\n",
      "Epoch 46/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8157e-05 - val_loss: 2.5276e-04\n",
      "Epoch 47/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6843e-05 - val_loss: 2.2936e-04\n",
      "Epoch 48/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.1123e-05 - val_loss: 3.2658e-04\n",
      "Epoch 49/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.6215e-05 - val_loss: 2.6578e-04\n",
      "Epoch 50/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.9410e-05 - val_loss: 2.8312e-04\n",
      "Epoch 51/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.0195e-05 - val_loss: 2.5223e-04\n",
      "Epoch 52/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6300e-05 - val_loss: 2.1394e-04\n",
      "Epoch 53/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.7596e-05 - val_loss: 2.4191e-04\n",
      "Epoch 54/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6550e-05 - val_loss: 2.4090e-04\n",
      "Epoch 55/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.7827e-05 - val_loss: 1.7699e-04\n",
      "Epoch 56/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.9301e-05 - val_loss: 2.6256e-04\n",
      "Epoch 57/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.7064e-05 - val_loss: 1.7770e-04\n",
      "Epoch 58/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.2176e-05 - val_loss: 3.4353e-04\n",
      "Epoch 59/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.7276e-05 - val_loss: 2.0385e-04\n",
      "Epoch 60/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8830e-05 - val_loss: 2.7022e-04\n",
      "Epoch 61/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.3645e-05 - val_loss: 2.1540e-04\n",
      "Epoch 62/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0393e-05 - val_loss: 2.6530e-04\n",
      "Epoch 63/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8601e-05 - val_loss: 2.6548e-04\n",
      "Epoch 64/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.5352e-05 - val_loss: 2.5821e-04\n",
      "Epoch 65/150\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 4.0656e-05 - val_loss: 3.3217e-04\n",
      "Epoch 66/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.7915e-05 - val_loss: 2.0445e-04\n",
      "Epoch 67/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6485e-05 - val_loss: 2.6167e-04\n",
      "Epoch 68/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.8066e-05 - val_loss: 3.4713e-04\n",
      "Epoch 69/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.2058e-05 - val_loss: 2.1261e-04\n",
      "Epoch 70/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.1040e-05 - val_loss: 3.3099e-04\n",
      "Epoch 71/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.2164e-05 - val_loss: 1.9505e-04\n",
      "Epoch 72/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6389e-05 - val_loss: 2.1955e-04\n",
      "Epoch 73/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.5529e-05 - val_loss: 2.3040e-04\n",
      "Epoch 74/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.4881e-05 - val_loss: 2.2348e-04\n",
      "Epoch 75/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.4990e-05 - val_loss: 2.0009e-04\n",
      "Epoch 76/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8309e-05 - val_loss: 1.7401e-04\n",
      "Epoch 77/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8715e-05 - val_loss: 1.7361e-04\n",
      "Epoch 78/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.1719e-05 - val_loss: 2.2190e-04\n",
      "Epoch 79/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.5690e-05 - val_loss: 1.6608e-04\n",
      "Epoch 80/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.5257e-05 - val_loss: 1.6774e-04\n",
      "Epoch 81/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8194e-05 - val_loss: 2.2578e-04\n",
      "Epoch 82/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.2818e-05 - val_loss: 2.4939e-04\n",
      "Epoch 83/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7100e-05 - val_loss: 1.5887e-04\n",
      "Epoch 84/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6661e-05 - val_loss: 2.8271e-04\n",
      "Epoch 85/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.2470e-05 - val_loss: 1.7717e-04\n",
      "Epoch 86/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.1050e-05 - val_loss: 1.4460e-04\n",
      "Epoch 87/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.9086e-05 - val_loss: 1.9482e-04\n",
      "Epoch 88/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6644e-05 - val_loss: 2.1550e-04\n",
      "Epoch 89/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.4555e-05 - val_loss: 1.7801e-04\n",
      "Epoch 90/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.4251e-05 - val_loss: 1.4303e-04\n",
      "Epoch 91/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8444e-05 - val_loss: 2.4860e-04\n",
      "Epoch 92/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.4472e-05 - val_loss: 1.5023e-04\n",
      "Epoch 93/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.3656e-05 - val_loss: 1.7875e-04\n",
      "Epoch 94/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6023e-05 - val_loss: 2.2261e-04\n",
      "Epoch 95/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.5038e-05 - val_loss: 2.2440e-04\n",
      "Epoch 96/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5.1028e-05 - val_loss: 1.5770e-04\n",
      "Epoch 97/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5.1183e-05 - val_loss: 2.7074e-04\n",
      "Epoch 98/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.0438e-05 - val_loss: 1.5505e-04\n",
      "Epoch 99/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.3901e-05 - val_loss: 2.1340e-04\n",
      "Epoch 100/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.2237e-05 - val_loss: 1.6176e-04\n",
      "Epoch 101/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7457e-05 - val_loss: 2.7939e-04\n",
      "Epoch 102/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.9469e-05 - val_loss: 1.5093e-04\n",
      "Epoch 103/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.1647e-05 - val_loss: 2.6713e-04\n",
      "Epoch 104/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.4466e-05 - val_loss: 2.0506e-04\n",
      "Epoch 105/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8853e-05 - val_loss: 2.8880e-04\n",
      "Epoch 106/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9324e-05 - val_loss: 2.0161e-04\n",
      "Epoch 107/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.4136e-05 - val_loss: 1.6265e-04\n",
      "Epoch 108/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8556e-05 - val_loss: 2.4643e-04\n",
      "Epoch 109/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.6118e-05 - val_loss: 1.8815e-04\n",
      "Epoch 110/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.3818e-05 - val_loss: 1.9381e-04\n",
      "Epoch 111/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.2940e-05 - val_loss: 1.6247e-04\n",
      "Epoch 112/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8199e-05 - val_loss: 2.0332e-04\n",
      "Epoch 113/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.3458e-05 - val_loss: 2.2988e-04\n",
      "Epoch 114/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8608e-05 - val_loss: 2.0556e-04\n",
      "Epoch 115/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.3493e-05 - val_loss: 1.4964e-04\n",
      "Epoch 116/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7176e-05 - val_loss: 1.4007e-04\n",
      "Epoch 117/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.4459e-05 - val_loss: 2.0199e-04\n",
      "Epoch 118/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.2616e-05 - val_loss: 2.6672e-04\n",
      "Epoch 119/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6247e-05 - val_loss: 2.8542e-04\n",
      "Epoch 120/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6029e-05 - val_loss: 2.1266e-04\n",
      "Epoch 121/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.9071e-05 - val_loss: 2.1680e-04\n",
      "Epoch 122/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.1505e-05 - val_loss: 4.4729e-04\n",
      "Epoch 123/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0821e-05 - val_loss: 2.6959e-04\n",
      "Epoch 124/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.4826e-05 - val_loss: 1.6006e-04\n",
      "Epoch 125/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8988e-05 - val_loss: 1.6860e-04\n",
      "Epoch 126/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9994e-05 - val_loss: 1.9846e-04\n",
      "Epoch 127/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.2393e-05 - val_loss: 1.3699e-04\n",
      "Epoch 128/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.7082e-05 - val_loss: 2.2242e-04\n",
      "Epoch 129/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9803e-05 - val_loss: 2.8874e-04\n",
      "Epoch 130/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.5519e-05 - val_loss: 2.3755e-04\n",
      "Epoch 131/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.1672e-05 - val_loss: 2.4236e-04\n",
      "Epoch 132/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.4813e-05 - val_loss: 1.5022e-04\n",
      "Epoch 133/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.9000e-05 - val_loss: 2.3378e-04\n",
      "Epoch 134/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.9677e-05 - val_loss: 1.3752e-04\n",
      "Epoch 135/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.2693e-05 - val_loss: 2.1760e-04\n",
      "Epoch 136/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6775e-05 - val_loss: 2.1351e-04\n",
      "Epoch 137/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.0802e-05 - val_loss: 2.4174e-04\n",
      "Epoch 138/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.1817e-05 - val_loss: 2.4766e-04\n",
      "Epoch 139/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6916e-05 - val_loss: 1.5806e-04\n",
      "Epoch 140/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6915e-05 - val_loss: 2.1875e-04\n",
      "Epoch 141/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.1401e-05 - val_loss: 2.6778e-04\n",
      "Epoch 142/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.9421e-05 - val_loss: 1.6812e-04\n",
      "Epoch 143/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.5885e-05 - val_loss: 3.4340e-04\n",
      "Epoch 144/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.4784e-05 - val_loss: 1.3781e-04\n",
      "Epoch 145/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.8184e-05 - val_loss: 1.8781e-04\n",
      "Epoch 146/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.1851e-05 - val_loss: 1.4812e-04\n",
      "Epoch 147/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.6556e-05 - val_loss: 2.1272e-04\n",
      "Epoch 148/150\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.1233e-05 - val_loss: 2.5227e-04\n",
      "Epoch 149/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.2205e-05 - val_loss: 2.1247e-04\n",
      "Epoch 150/150\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.1294e-05 - val_loss: 1.4669e-04\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "dir = \"/home/whfhrs3260/csv_data/price_data_score_10years.csv\"\n",
    "stock = \"^DJI\"\n",
    "variable = [\"before_close\",\"Score\"]\n",
    "window_size = 3\n",
    "start_date = \"2012-01-01\"\n",
    "end_date = \"2022-04-30\"\n",
    "\n",
    "scaler_x, scaler_y,x, y, x_t, y_t = tt.train_test_result(dir, stock, variable, window_size, start_date, end_date)\n",
    "\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Bidirectional(LSTM(128, input_shape = (x.shape[1],x.shape[2]))))\n",
    "bi_lstm_model.add(Dense(64))\n",
    "bi_lstm_model.add(Dense(1,activation=\"tanh\")) \n",
    "adam = optimizers.Adam(learning_rate=0.0012)\n",
    "bi_lstm_model.compile(optimizer=adam, loss = \"mse\")\n",
    "history = bi_lstm_model.fit(x, y, epochs=150, batch_size=64, validation_split=0.2)\n",
    "    \n",
    "bi_lstm_pred = bi_lstm_model.evaluate(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a842fd41-c580-4d1e-9087-d363887f243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float64, numpy=1904.3791837918943>,\n",
       " 3626660.075659882,\n",
       " 1481.821582784383,\n",
       " 4.553761019666613)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_pred = bi_lstm_model.predict(x_t)\n",
    "bi_lstm_pred = scaler_y.inverse_transform(bi_lstm_pred)\n",
    "y_t = scaler_y.inverse_transform(y_t)\n",
    "root_mean_squared_error(y_t, bi_lstm_pred), mean_squared_error(y_t, bi_lstm_pred), mean_absolute_error(y_t, bi_lstm_pred), MAPE(y_t, bi_lstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b72bbfe-542c-4772-b35d-b55286651835",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_model = \"/home/whfhrs3260/model/\"\n",
    "#bi_lstm_model.save(dir_model + 'bi_lstm_128_64_window30_10years.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79c8c7-541f-47da-9445-21d898c33121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9f284-dc1a-4c53-8ad7-5f2b70a73f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00e6ef-f80a-49d8-93f3-c2c7358e0073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db66f6-cd48-4da1-b07f-e06bb852bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74d59446-6b63-4133-88d0-1293d3de5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "432f200e-31f1-42c8-8bd8-570f830970a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whfhrs3260/Python/10year_data/review/train_test.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set_news_data_date.price_date[i]=standard  + datetime.timedelta(days=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whfhrs3260/Python/10year_data/review/train_test.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  price_data.before_close[i+1] = price_data.Close[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "dir_model = \"/home/whfhrs3260/model/\"\n",
    "dir = \"/home/whfhrs3260/csv_data/price_data_score_10years.csv\"\n",
    "stock = \"^DJI\"\n",
    "variable = [\"before_close\",\"Score\"]\n",
    "window_size = 30\n",
    "start_date = \"2012-01-01\"\n",
    "end_date = \"2022-04-30\"\n",
    "\n",
    "scaler_x, scaler_y,x, y, x_t, y_t = tt.train_test_result(dir, stock, variable, window_size, start_date, end_date)\n",
    "\n",
    "bi_lstm_model = load_model(dir_model + 'bi_lstm_128_64_window30_10years.h5', custom_objects ={\"root_mean_squared_error\" : root_mean_squared_error})\n",
    "bi_lstm_pred = bi_lstm_model.predict(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be080a8d-8698-4863-8442-e93ea289814b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float64, numpy=742.1500555456893>,\n",
       " 550786.7049464696,\n",
       " 561.2534303220707,\n",
       " 1.8152156454865287)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_pred = scaler_y.inverse_transform(bi_lstm_pred)\n",
    "y_t = scaler_y.inverse_transform(y_t)\n",
    "root_mean_squared_error(y_t, bi_lstm_pred), mean_squared_error(y_t, bi_lstm_pred), mean_absolute_error(y_t, bi_lstm_pred), MAPE(y_t, bi_lstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdac1d4-518e-4ed9-9214-80c79a14e331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
